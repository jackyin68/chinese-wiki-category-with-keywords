
人工神经网络（Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。
和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。

== 背景 ==
对人类中枢神经系统的观察启发了人工神经网络这个概念。在人工神经网络中，简单的人工节点，称作神经元（neurons），连接在一起形成一个类似生物神经网络的网状结构。
人工神经网络目前没有一个统一的正式定义。不过，具有下列特点的统计模型可以被称作是“神经化”的：
* 具有一组可以被调节的权重（被学习算法调节的数值参数）
* 可以估计输入数据的非线性函数关系
这些可调节的权重可以被看做神经元之间的连接强度。
人工神经网络与生物神经网络的相似之处在于，它可以集体地、并行地计算函数的各个部分，而不需要描述每一个单元的特定任务。神经网络这个词一般指统计学、认知心理学和人工智能领域使用的模型，而控制中央神经系统的神经网络属于理论神经科学和计算神经科学。
在神经网络的现代软件实现中，被生物学启发的那种方法已经很大程度上被抛弃了，取而代之的是基于统计学和信号处理的更加实用的方法。在一些软件系统中，神经网络或者神经网络的一部分（例如人工神经元）是大型系统中的一个部分。这些系统结合了适应性的和非适应性的元素。虽然这种系统使用的这种更加普遍的方法更适宜解决现实中的问题，但是这和传统的连接主义人工智能已经没有什么关联了。不过它们还有一些共同点：非线性、分布式、并行化，局部性计算以及适应性。从历史的角度讲，神经网络模型的应用标志着二十世纪八十年代后期从高度符号化的人工智能（以用条件规则表达知识的专家系统为代表）向低符号化的机器学习（以用动力系统的参数表达知识为代表）的转变。

== 历史 ==
沃伦·麦卡洛克和沃尔特·皮茨（1943）基于数学和一种称为阈值逻辑的算法创造了一种神经网络的计算模型。这种模型使得神经网络的研究分裂为两种不同研究思路。一种主要关注大脑中的生物学过程，另一种主要关注神经网络在人工智能里的应用。

=== 赫布型学习 ===
二十世纪40年代后期，心理学家唐纳德·赫布根据神经可塑性的机制创造了一种对学习的假说，现在称作赫布型学习。赫布型学习被认为是一种典型的非监督式学习规则，它后来的变种是长期增强作用的早期模型。从1948年开始，研究人员将这种计算模型的思想应用到B型图灵机上。
法利和韦斯利·A·克拉克（1954）首次使用计算机，当时称作计算器，在MIT模拟了一个赫布网络。纳撒尼尔·罗切斯特（1956）等人模拟了一台 IBM 704计算机上的抽象神经网络的行为。 
创造了感知机。这是一种模式识别算法，用简单的加减法实现了两层的计算机学习网络。罗森布拉特也用数学符号描述了基本感知机里没有的回路，例如异或回路。这种回路一直无法被神经网络处理，直到保罗·韦伯斯(1975)创造了反向传播算法。
在马文·明斯基和西摩尔·派普特（1969）发表了一项关于机器学习的研究以后，神经网络的研究停滞不前。他们发现了神经网络的两个关键问题。第一是基本感知机无法处理异或回路。第二个重要的问题是电脑没有足够的能力来处理大型神经网络所需要的很长的计算时间。直到计算机具有更强的计算能力之前，神经网络的研究进展缓慢。

=== 反向传播算法与复兴 ===
后来出现的一个关键的进展是保罗·韦伯斯发明的反向传播算法（Werbos 1975）。这个算法有效地解决了异或的问题，还有更普遍的训练多层神经网络的问题。
在二十世纪80年代中期，分布式并行处理（当时称作联结主义）流行起来。戴维·鲁姆哈特和詹姆斯·麦克里兰德的教材对于联结主义在计算机模拟神经活动中的应用提供了全面的论述。
神经网络传统上被认为是大脑中的神经活动的简化模型，虽然这个模型和大脑的生理结构之间的关联存在争议。人们不清楚人工神经网络能多大程度地反映大脑的功能。
支持向量机和其他更简单的方法（例如线性分类器）在机器学习领域的流行度逐渐超过了神经网络，但是在2000年代后期出现的深度学习重新激发了人们对神经网络的兴趣。

=== 2006年之后的进展 ===
人们用CMOS创造了用于生物物理模拟和神经形态计算的计算装置。最新的研究显示了用于大型主成分分析和卷积神经网络的纳米装置具有良好的前景。如果成功的话，这会创造出一种新的神经计算装置，因为它依赖于学习而不是编程，并且它从根本上就是模拟的而不是数字化的，虽然它的第一个实例可能是数字化的CMOS装置。
在2009到2012年之间，Jürgen Schmidhuber在Swiss AI Lab IDSIA的研究小组研发的循环神经网络和深前馈神经网络赢得了8项关于模式识别和机器学习的国际比赛。例如，Alex Graves et al.的双向、多维的LSTM赢得了2009年ICDAR的3项关于连笔字识别的比赛，而且之前并不知道关于将要学习的3种语言的信息。
IDSIA的Dan Ciresan和同事根据这个方法编写的基于GPU的实现赢得了多项模式识别的比赛，包括IJCNN 2011交通标志识别比赛等等。他们的神经网络也是第一个在重要的基准测试中（例如IJCNN 2012交通标志识别和NYU的扬·勒丘恩（Yann LeCun）的MNIST手写数字问题）能达到或超过人类水平的人工模式识别器。
类似1980年Kunihiko Fukushima发明的neocognitron和视觉标准结构（由David H. Hubel和Torsten Wiesel在初级视皮层中发现的那些简单而又复杂的细胞启发）那样有深度的、高度非线性的神经结构可以被多伦多大学杰夫·辛顿实验室的非监督式学习方法所训练。

==分类==
典型的人工神经网络具有以下三个部分：
*结构（Architecture）结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。
*激励函数（Activation Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。
*学习规则（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激发。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激发。最后，输出神经元的激励值决定了识别出来的是哪个字母。

==神经元==
神经元示意图：
343x343像素
*a1~an为输入向量的各个分量
*w1~wn为神经元各个突触的权值
*b为偏置
*f为传递函数，通常为非线性函数。一般有traingd(),tansig(),hardlim()。以下默认为hardlim()
*t为神经元输出
数学表示
*为权向量，为的转置
*为输入向量
*为偏置
*为传递函数
可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。
单个神经元的作用：把一个n维向量空间用一个超平面分割成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。
该超平面的方程： 
*权向量
*偏置
*超平面上的向量

==神经元网络==

===单层神经元网络===
是最基本的神经元网络形式，由有限个神经元构成，所有神经元的输入向量都是同一个向量。由于每一个神经元都会产生一个标量结果，所以单层神经元的输出是一个向量，向量的维数等于神经元的数目。
示意图：
331x331像素

===多层神经元网络===

==人工神经网络的实用性==
人工神经网络是一个能够学习，能够总结归纳的系统，也就是说它能够通过已知数据的实验运用来学习和归纳总结。人工神经网络通过对局部情况的对照比较（而这些比较是基于不同情况下的自动学习和要实际解决问题的复杂性所决定的），它能够推理产生一个可以自动识别的系统。与之不同的基于符号系统下的学习方法，它们也具有推理功能，只是它们是建立在逻辑演算法的基础上，也就是说它们之所以能够推理，基础是需要有一个推理演算法则的集合。

==人工神经元网络模型==
通常来说，一个人工神经元网络是由一个多层神经元结构组成，每一层神经元拥有输入（它的输入是前一层神经元的输出）和输出，每一层（我们用符号记做）Layer(i)是由Ni(Ni代表在第i层上的N)个网络神经元组成，每个Ni上的网络神经元把对应在Ni-1上的神经元输出做为它的输入，我们把神经元和与之对应的神经元之间的连线用生物学的名称，叫做突触（Synapse），在数学模型中每个突触有一个加权数值，我们称做权重，那么要计算第i层上的某个神经元所得到的势能等于每一个权重乘以第i-1层上对应的神经元的输出，然后全体求和得到了第i层上的某个神经元所得到的势能，然后势能数值通过该神经元上的激活函数（activation function，常是∑函数（Sigmoid function）以控制输出大小，因为其可微分且连续，方便差量规则（Delta rule）处理。），求出该神经元的输出，注意的是该输出是一个非线性的数值，也就是说通过激励函数求的数值根据极限值来判断是否要激活该神经元，换句话说我们对一个神经元网络的输出是否线性不感兴趣。

==基本结构==
一种常见的多层结构的前馈网络（Multilayer Feedforward Network）由三部分组成，
*输入层（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。
*输出层（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。
*隐藏层（Hidden layer），简称「隐层」，是输入层和输出层之间众多神经元和链接组成的各个层面。隐层可以有一层或多层。隐层的节点（神经元）数目不定，但数目越多神经网络的非线性越显著，从而神经网络的强健性（robustness）（控制系统在一定结构、大小等的参数摄动下，维持某些性能的特性）更显著。习惯上会选输入节点1.2至1.5倍的节点。
这种网络一般称为感知器（对单隐藏层）或多层感知器（对多隐藏层），神经网络的类型已经演变出很多种，这种分层的结构也并不是对所有的神经网络都适用。

==学习过程==
通过训练样本的校正，对各个层的权重进行校正（learning）而建立模型的过程，称为自动学习过程（training algorithm）。具体的学习方法则因网络结构和模型不同而不同，常用反向传播算法（Backpropagation/倒传递/逆传播，以output利用一次微分来修正weight）来验证。

==种类==
人工神经网络分类为以下两种：
1.依学习策略（Algorithm）分类主要有：
*-{zh-hans:监督式学习网络;zh-hant:监督式学习网络}-（Supervised Learning Network）为主
*-{zh-hans:无监督式学习网络;zh-hant:无监督式学习网络}-（Unsupervised Learning Network）
*-{zh-hans:混合式学习网络;zh-hant:混合式学习网络}-（Hybrid Learning Network）
*-{zh-hans:联想式学习网络;zh-hant:联想式学习网络}-（Associate Learning Network）
*-{zh-hans:最适化学习网络;zh-hant:最适化学习网络}-（Optimization Application Network）
2.依网络架构（Connectionism）分类主要有：
*前馈神经网络（Feed Forward Network）
*循环神经网络（Recurrent Network）
*强化式架构（Reinforcement Network）

==理论性质==

===计算能力===
多层感知器（Multilayer Perceptron，缩写MLP）是一个通用的函数逼近器，由Cybenko定理证明。然而，证明不依赖特定的神经元数量或权重。Hava Siegelmann和Eduardo D. Sontag的工作证明了，一个具有有理数权重值的特定递归结构（与全精度实数权重值相对应）由有限个神经元和标准的线性关系构成的神经网络相当于一个通用图灵机。他们进一步表明，使用无理数权重值会产生一个超图灵机。

===容量===
人工神经网络模型有一个属性，称为“容量”，这大致相当于他们记住（而非正确分类）输入数据的能力。它与网络的参数、和结构有关。谷歌在研究中使用打乱标签的方法，来测试模型是否能否记住所有的输出。虽然很明显，这样模型在测试集上的表现几乎是随机猜测，但是模型能够记住所有训练集的输入数据，即记住他们被打乱后的标签。而记住有限的样本的信息（Expressivity），需要的模型的参数（权重）数量存在下限。

===收敛性===
模型并不总是收敛到唯一解，因为它取决于一些因素。首先，函数可能存在许多局部极小值，这取决于成本函数和模型。其次，在远离局部最小值时，优化方法可能无法保证收敛。第三，对大量的数据或参数，一些方法变得不切实际。在一般情况下，我们发现，理论保证的收敛不能成为实际应用的一个可靠的指南。

===综合统计===
在目标是创建一个普遍系统的应用程序中，过度训练的问题出现了。这出现在回旋或过度具体的系统中当网络的容量大大超过所需的自由参数。为了避免这个问题，有两个方向：第一个是使用交叉验证和类似的技术来检查过度训练的存在和选择最佳参数如最小化泛化误差。二是使用某种形式的正规化。这是一个在概率化（贝叶斯）框架里出现的概念，其中的正则化可以通过为简单模型选择一个较大的先验概率模型进行；而且在统计学习理论中，其目的是最大限度地减少了两个数量：“风险”和“结构风险”，相当于误差在训练集和由于过度拟合造成的预测误差。

== 相关 ==
* 机器学习
* 深度学习
* 逻辑回归
* 线性回归

== 参看 ==
* 感知机
* 多层感知器
* ER随机图

== 参见 ==
* 人工智能

== 参考文献 ==

==外部连结==
*  神经网络介绍
*  Performance comparison of neural network algorithms tested on UCI data sets
*  A close view to Artificial Neural Networks Algorithms
* Neural Networks
*  A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.
*  Neural Networks in Materials Science
*  A practical tutorial on Neural Networks
*  Applications of neural networks
*  XOR实例




机器学习是人工智能的一个分支。人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、Convex analysis、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。
机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。

== 定义 ==
机器学习有下面几种定义：
*机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
*机器学习是对能通过经验自动改进的计算机算法的研究。
*机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。
一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

== 分类 ==
机器学习可以分成下面几种类别：
* 监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。
监督学习和非监督学习的差别就是训练集目标是否人标注。他们都有训练集 且都有输入和输出
* 无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类。
* 半监督学习介于监督学习与无监督学习之间。
* 增强学习机器为了达成目标，随着环境的变动，而逐步调整其行为，并评估每一个行动之后所到的回馈是正向的或负向的。

== 算法 ==
具体的机器学习算法有：
* 构造间隔理论分布：聚类分析和模式识别
** 人工神经网络
** 决策树
** 感知器
** 支持向量机
** 集成学习AdaBoost
** 降维与度量学习
** 聚类
** 贝叶斯分类器
* 构造条件概率：回归分析和统计分类
**高斯过程回归
**线性判别分析
**最近邻居法
**径向基函数核
*通过再生模型构造概率密度函数：
**最大期望算法
**概率图模型：包括贝叶斯网和Markov随机场 
**Generative Topographic Mapping
*近似推断技术：
**马尔可夫链
**蒙特卡罗方法
**变分法
*最优化：大多数以上方法，直接或者间接使用最优化算法。

== 参考文献 ==

=== 引用 ===

=== 来源 ===
 书籍
* Bishop, C. M. (1995). 《模式识别神经网络》，牛津大学出版社. ISBN 0-19-853864-2.
* Bishop, C. M. (2006). 《模式识别与机器学习》，Springer. ISBN 978-0-387-31073-2.
* Richard O. Duda, Peter E. Hart, David G. Stork (2001). 《模式分类》（第2版）, New York: Wiley. ISBN 0-471-05669-3.
* MacKay, D. J. C. (2003).  《信息理论、推理和学习算法》，剑桥大学出版社. ISBN 0-521-64298-1
* Mitchel.l, T. (1997). 《机器学习》, McGraw Hill. ISBN 0-07-042807-7
* Sholom Weiss, Casimir Kulikowski (1991). Computer Systems That Learn, Morgan Kaufmann. ISBN 1-55860-065-5.

== 外部链接 ==
* UCI description
* 机器学习软件Weka
* Pablo Castro主页
* 机器学习网邮件列表
* 机器学习和自然语言处理-弗莱堡大学
* 机器学习和数据挖掘，生物信息学小组，慕尼黑工业大学
* 机器学习和生物计算-Bristol大学
* 机器学习和应用统计学@微软研究
* 机器学习研究月刊
* 机器学习期刊
* 机器学习-Kmining，数据挖掘和KDD科学参考fix-attempted=yes 
*Book " 智能系统社区" by Walter Fritz
* 开放目录项目
* 机器学习论文-CiteSeer
* Orange，使用Python脚本语言的机器学习组件和可视化编程接口

== 参见 ==
* 人工智能
* 计算智能
* 数据挖掘
* 模式识别
* 机器学习方面重要出版物（计算机科学）
* 机器学习方面重要出版物（统计学）
* 自主控制机器人
* 归纳逻辑编程
* 决策树
* 神经网络
* 强化学习
* 贝叶斯学习
* 最近邻居法
* 计算学习理论
* 深度学习
* 机器学习控制




自然语言处理（Natural Language Processing，缩写作 NLP）是人工智慧和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。
自然语言认知和理解是让电脑把输入的语言变成有意思的符号和关系，然后根据目的再处理。自然语言生成系统则是把计算机数据转化为自然语言。

== 历史 ==
自然语言处理大体是从1950年代开始，虽然更早期也有作为。1950年，图灵发表论文「Computing Machinery and Intelligence」，提出现在所谓的「图灵测试」作为判断智能的条件。
1954年的Georgetown-IBM experiment 涉及全部 automatic translation 超过60句俄文成为英文。研究人员声称三到五年之内即可解决机器翻译的问题。不过实际进展远低于预期，1966年的 ALPAC report 发现十年研究未达预期目标，机器翻译的研究经费遭到大幅削减。一直到1980年代末期，统计机器翻译系统发展出来，机器翻译的研究才得以更上一层楼。
1960年代发展特别成功的NLP系统包括SHRDLU——一个词汇设限、运作于受限如「积木世界」的一种自然语言系统，以及1964-1966年约瑟夫·维森鲍姆模拟「个人中心治疗」而设计的ELIZA——几乎未运用人类思想和感情的讯息，有时候却能呈现令人讶异地类似人之间的互动。「病人」提出的问题超出ELIZA 极小的知识范围之时，可能会得到空泛的回答。例如问题是「我的头痛」，回答是「为什么说你头痛？」
1970年代，程式设计师开始设计「概念本体论」（conceptual ontologies）的程式，将现实世界的资讯，架构成电脑能够理解的资料。实例有MARGIE、SAM、PAM、TaleSpin、QUALM、Politics以及Plot Unit。许多聊天机器人在这一时期写成，包括 PARRY  、 Racter 以及 Jabberwacky  。
一直到1980年代，多数自然语言处理系统是以一套复杂、人工订定的规则为基础。不过从1980年代末期开始，语言处理引进了机器学习的演算法，NLP产生革新。成因有两个：运算能力稳定增加（参见摩尔定律）；以及乔姆斯基 语言学理论渐渐丧失主导（例如转换-生成文法）。该理论的架构不倾向于语料库——机器学习处理语言所用方法的基础。有些最早期使用的机器学习演算法，例如决策树，是硬性的、「如果-则」规则组成的系统，类似当时既有的人工订定的规则。不过part-of-speech tagging 将隐马尔可夫模型引入NLP，并且研究日益聚焦于软性的、以机率做决定的统计模型，基础是将输入资料里每一个特性赋予代表其份量的数值。许多语音识别现今依赖的 cache language model 即是一种统计模型的例子。这种模型通常足以处理非预期的输入数据，尤其是输入有错误（真实世界的数据总免不了），并且在整合到包含多个子任务的较大系统时，结果比较可靠。
许多早期的成功属于机器翻译领域，尤其归功IBM的研究，渐次发展出更复杂的统计模型。这些系统得以利用加拿大和欧盟现有的语料库，因为其法律规定政府的会议必须翻译成所有的官方语言。不过，其他大部分系统必须特别打造自己的语料库，一直到现在这都是限制其成功的一个主要因素，于是大量的研究致力于从有限的数据更有效地学习。
近来的研究更加聚焦于非监督式学习和semi-supervised learning的演算法。这种演算法，能够从没有人工注解理想答案的资料里学习。大体而言，这种学习比监督学习困难，并且在同量的数据下，通常产生的结果较不准确。不过没有注解的数据量极巨（包含了全球资讯网），弥补了较不准确的缺点。
近年来，深度学习技巧纷纷出炉 在自然语言处理方面获得最尖端的成果，例如语言模型、语法分析等等。

== 任务和限制 ==
理论上，NLP是一种很吸引人的人机交互方式。早期的语言处理系统如SHRDLU，当它们处于一个有限的“积木世界”，运用有限的词汇表会话时，工作得相当好。这使得研究员们对此系统相当乐观，然而，当把这个系统拓展到充满了现实世界的含糊与不确定性的环境中时，他们很快丧失了信心。
由于理解（understanding）自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，自然语言认知，同时也被视为一个人工智慧完备（AI-complete）的问题。同时，在自然语言处理中，"理解"的定义也变成一个主要的问题。

== 实际问题 ==
一些NLP面临的问题实例：
* 句子“我们把香蕉给猴子，因为(-{牠}-们)饿了”和“我们把香蕉给猴子，因为(-{它}-们)熟透了”有同样的结构。但是代词“它们”在第一句中指的是“猴子”，在第二句中指的是“香蕉”。如果不了解猴子和香蕉的属性，无法区分。(简体中文和英文的-{它/it}-没有区分，但在中文(正体中文)里-{「牠」和「它」}-是有区别的，只是代词在中文里常常被省略，因此需区别属性并且标示出来)
不少的中文相关笑话即是利用类似结构的中文造句而成，此类笑话通常带有《中文博大精深》之类的词汇,叙述多以老外参加考试为背景。例子如下：『
某老外苦学汉语10年，到东方参加汉语考试。试题为「请解释下列句子」：
阿呆给长官送红包时，两个人的对话颇有意思。
长官：「你这是什么意思？」
阿呆：「没什么意思，意思意思。」
长官：「你这就不够意思了。」
阿呆：「小意思，小意思。」
长官：「你这人真有意思。」
阿呆：「其实也没有别的意思。」
长官：「那我就不好意思了。」
阿呆：「是我不好意思。」
老外泪流满面，交白卷回国了。
』

==主要范畴==
*文本朗读（Text to speech）
*语音合成（Speech synthesis）
*语音识别（Speech recognition）
*断词/分词（Text segmentation/Word tokenization）
*中文自动分词（Chinese word segmentation）
*语法分析/剖析（Syntactic analysis/Parsing）
*汉语自动句法分析
*词汇标示框架(Lexical Markup Framework)
*n元语法 (n-gram）
*词嵌入 (Word2vec) 
*词性标注（Part-of-speech tagging）
*文档分类 (Document classification)
*自然语言生成（Natural language generation）
*文本分类（Text categorization）
*信息检索（Information retrieval）
*信息抽取（Information extraction）
*文字校对（Text-proofing）
*问答系统（Question answering）
给一句人类语言的问句，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是甚么?)
*聊天机器人 (ChatBot)
*对话系统 (Dialogue system)
*机器翻译（Machine translation）
将某种人类语言自动翻译至另一种语言
*自动摘要（Automatic summarization）
产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要
*文字蕴涵（Textual entailment）
*命名实体识别（Named entity recognition, NER）
*主题模型（Topic Model）
*文本情感分析（Sentiment analysis）
*语意分析（Semantic analysis）
*潜在语义学（Latent Semantic Analysis）
*词袋模型（Bag-of-words model）
*标签云 (Tag Cloud)
*自然语言理解 (Natural Language Understanding)

==自然语言处理研究的难点==

===单词的边界界定===
在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。

===词义的消歧===
许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。

===句法的模糊性===
 自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。

===有瑕疵的或不规范的输入===
例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字元识别（OCR）的错误。

===语言行为与计划===
句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是动手把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。

==当前自然语言处理研究的发展趋势==
第一，传统的基于句法-语义规则的理性主义方法过于复杂，随着语料库建设和语料库语言学的崛起，大规模真实文本的机器学习处理成为自然语言处理的主要选择。
第二，统计数学方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。
第三，浅层处理与深层处理并重，统计与规则方法并重，形成混合式的系统。
第四，自然语言处理中越来越重视词汇的作用，出现了强烈的“词汇主义”的倾向。词汇知识库的建造成为了普遍关注的问题。

== 统计自然语言处理==
统计自然语言处理运用了推测学、机率、统计的方法来解决上述，尤其是针对容易高度模糊的长串句子，当套用实际文法进行分析产生出成千上万笔可能性时所引发之难题。处理这些高度模糊句子所采用消歧的方法通常运用到语料库以及马可夫模型（Markov models）。统计自然语言处理的技术主要由同样自人工智慧下与学习行为相关的子领域：机器学习及资料采掘所演进而成。

=== 相关实例 ===
*  GATE: a Java Library for Text Engineering
*  LTP:语言技术平台（简体中文）
* MARF
*  Python编程语言的自然语言处理工具包教程
*  fastNLP

== 延伸阅读 ==
*  doi=10.1073/pnas.92.22.9977
* Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. ISBN 978-0-596-51649-9.
* Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6.
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5.  Official html and pdf versions available without charge.
* Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. ISBN 978-0-262-13360-9.
* David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-Verlag. ISBN 978-0-387-19557-5.

== 外部连结 ==
*  人类语言技术当前发展情况概览
*  哥伦比亚大学自然语言处理研究组
*  卡内基梅隆大学语言技术研究院
*  斯坦福大学自然语言处理研究小组
*  中文自然语言处理开放平台
*  ACL（美国电脑语言学协会）提供的相关杂志以及研讨会的论文

== 参见 ==
* universal translator
* 电脑语言学
* 受限自然语言
* 信息抽取
* 资讯检索
* 自然语言理解
* 潜在语义索引
* 潜在语义学
* Stochastic grammar
* 机器记者
* Automated essay scoring
* Biomedical text mining
* Compound term processing
* 计算语言学
* Computer-assisted reviewing
* 深度学习
* Deep linguistic processing
* Foreign language reading aid
* Foreign language writing aid
* Language technology
* 隐含狄利克雷分布（LDA）
* Native-language identification
* Natural language programming
* Natural language user interface
* 扩展查询
* Reification (linguistics)
* Semantic folding
* 语音处理
* Spoken dialogue system
* 校对
* Text simplification
* Thought vector
* Truecasing
* 问答系统
* Word2vec




模式识别（Pattern recognition），就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。随着计算机技术的发展，人类有可能研究复杂的信息处理过程。信息处理过程的一个重要形式是生命体对环境及客体的识别。对人类来说，特别重要的是对光学信息（通过视觉器官来获得）和声学信息（通过听觉器官来获得）的识别。这是模式识别的两个重要方面。市场上可见到的代表性产品有光学字符识别、语音识别系统。
计算机识别的显著特点是速度快、准确性高、效率高，在将来完全可以取代人工录入。
识别过程与人类的学习过程相似。以光学字元识别之“汉字识别”为例：首先将汉字图像进行处理，抽取主要表达特征并将特征与汉字的代码存在计算机中。就像老师教我们「这个字叫什么、如何写」记在大脑中。这一过程叫做“训练”。识别过程就是将输入的汉字图像经处理后与计算机中的所有字进行比较，找出最相近的字就是识别结果。这一过程叫做“匹配”。

== 应用领域 ==
* 计算机视觉
** 医学影像分析
** 光学文字识别
* 自然语言处理
** 语音识别
** 手写识别
* 生物特征识别
** 人脸识别
** 指纹识别
** 虹膜识别
* 文件分类
* 互联网搜索引擎
* 信用评分
*测绘学
**摄影测量与遥感学

== 参看 ==
* 人工智能
* 数据挖掘
* 模糊逻辑
* 信息捕获
* 机器学习
* 统计学
*概率分类（Probabilistic classification）

== 参考文献 ==
* Richard O.Duda,Peter E.Hart,David G.Stork(2001) 模式识别 (第2版), Wiley出版社，纽约, ISBN 978-0-471-05669-0.
* Dietrich Paulus，Joachim Hornegger (1998) 应用模式识别 (第2版), Vieweg. ISBN 978-3-528-15558-2
* J. Schuermann: 模式识别: 人工神经元网络及其应用, Wiley&Sons, 1996, ISBN 978-0-471-13534-0
* Sholom Weiss，Casimir Kulikowski (1991) 学习型计算机系统, Morgan Kaufmann. ISBN 978-1-55860-065-2

==延伸阅读==
*isbn=0-12-269851-7
*isbn=1-59749-272-8
*isbn=3-528-15558-2
*isbn=0-471-13534-8
*location=Amsterdam
*isbn=1-55860-065-5
* issue=1
* An introductory tutorial to classifiers (introducing the basic terms, with numeric example)

==外部连结==
*  The International Association for Pattern Recognition
*  List of Pattern Recognition web sites
*  Journal of Pattern Recognition Research
*  Pattern Recognition Info
*  Pattern Recognition (Journal of the Pattern Recognition Society)
*  International Journal of Pattern Recognition and Artificial Intelligence
*  International Journal of Applied Pattern Recognition
*  Open Pattern Recognition Project, intended to be an open source platform for sharing algorithms of pattern recognition
*  OpenNN: Open Neural Networks Library
*  国家教育研究院-图形辨识 




蚁群算法（Ant Colony Optimization, ACO），又称蚂蚁算法，是一种用来在图中寻找优化路径的机率型算法。它由Marco Dorigo于1992年在他的博士论文“ Ant system: optimization by a colony of cooperating agents”中提出，其灵感来源于蚂蚁在寻找食物过程中发现路径的行为。蚁群算法是一种模拟进化算法，初步的研究表明该算法具有许多优良的性质。针对PID控制器参数优化设计问题，将蚁群算法设计的结果与遗传算法设计的结果进行了比较，数值仿真结果表明，蚁群算法具有一种新的模拟进化优化方法的有效性和应用价值。

== 常见的扩展 ==
下面是一些最常用的变异蚁群算法：

=== 精英蚂蚁系统 ===
全局最优解决方案在每个迭代以及其他所有的蚂蚁的沉积信息素。

=== 最大最小蚂蚁系统（MMAS）===
添加的最大和最小的信息素量 τmax，τmin ，只有全局最佳或迭代最好的巡逻沉积的信息素。所有的边缘都被初始化为τmax并且当接近停滞时重新初始化为τmax。

=== 蚁群系统 ===
蚁群系统已被提出。

=== 基于排序的蚂蚁系统（ASrank） ===
所有解决方案都根据其长度排名。然后为每个解决方案衡量信息素的沉积量，最短路径相比较长路径的解沉积了更多的信息素。

=== 连续正交蚁群（COAC） ===
COAC的信息素沉积机制能使蚂蚁协作而有效地寻解。利用正交设计方法，在可行域的蚂蚁可以使用增大的全局搜索能力和精度，快速、高效地探索他们选择的区域。 
正交设计方法和自适应半径调整方法也可推广到其他优化算法中，在解决实际问题施展更大的威力。

== 收敛 ==
一些版本的算法可以被证明是收敛的（即它能够在有限时间找到全局最优解）。第一个蚁群算法收敛的证据制作于2000年，建立了基于图像的蚁群算法，继而是ACS和MMAS的算法。像大多数元启发式方法一样，估计理论收敛速度是很难的。在2004年，Zlochin和他的同事们表明，COA-type算法在分布算法的叉熵和估计方面能被随机梯度下降法吸收。他们建议这些元启发式方法作为一个“研究性模式”。 

== 应用 ==
蚁群优化算法已应用于许多组合优化问题，包括蛋白质折叠或路由车辆的二次分配问题，很多派生的方法已经应用于实变量动力学问题，随机问题，多目标并行的实现。它也被用旅行推销员问题的拟最优解。在图表动态变化的情况下解决相似问题时，他们相比模拟退火算法和遗传算法方法有优势；蚁群算法可以连续运行并适应实时变化。这在网络路由和城市交通系统中是有利的。
第一蚁群优化算法被称为“蚂蚁系统”，它旨在解决推销员问题，其目标是要找到一系列城市的最短遍历路线。总体算法相对简单，它基于一组蚂蚁，每只完成一次城市间的遍历。在每个阶段，蚂蚁根据一些规则选择从一个城市移动到另一个：它必须访问每个城市一次;一个越远的城市被选中的机会越少（能见度更低）;在两个城市边际的一边形成的信息素越浓烈，这边被选择的概率越大;如果路程短的话，已经完成旅程的蚂蚁会在所有走过的路径上沉积更多信息素，每次迭代后，信息素轨迹挥发。

=== 调度问题 ===
车间作业调度问题（JSP） 
开放式车间调度问题（OSP）
排列流水车间问题（PFSP） 
单机总延迟时间问题（SMTTP）
单机总加权延迟问题（SMTWTP） 
资源受限项目调度问题（RCPSP）
车间组调度问题（GSP） 
附带依赖安装时间顺序的单机总延迟问题（SMTTPDST）
附带顺序相依设置/转换时间的多阶段流水车间调度问题（MFSP）

=== 车辆路径问题 ===
限量车辆路径问题（CVRP）
多站车辆路径问题（MDVRP）
周期车辆路径问题（PVRP）
分批配送车辆路径问题（SDVRP） 
随机车辆路径问题（SVRP）
装货配送的车辆路径问题（VRPPD）
带有时间窗的车辆路径问题（VRPTW）
依赖时间的时间窗车辆路径问题（TDVRPTW）
带时间窗和复合服务员工的车辆路径问题（VRPTWMS）

=== 分配问题 ===
二次分配问题（QAP）
广义分配问题（GAP）
频率分配问题（FAP）
冗余分配问题（RAP）

=== 设置问题 ===
覆盖设置问题（SCP）
分区设置问题（SPP）
约束重量的树图划分问题（WCGTPP）
加权弧L-基数树问题（AWlCTP）
多背包问题（MKP）
最大独立集问题（MIS）

=== 其他 ===
面向关系的网络路由
无连接网络路由
数据挖掘
项目调度中的贴现现金流
分布式信息检索
网格工作流调度问题
图像处理
系统识别
蛋白质折叠
电子电路设计

== 相关的方法 ==
遗传算法（GA）支持一系列的解决方案。解的合并或突变增加了解集，其中质量低劣的解被丢弃，寻找高级解决方案的过程模仿了这一演变。
模拟退火（SA）是一个全局优化相关​​的通过产生当前解的相邻解来遍历搜索空间的技术。高级的相邻解总是可接受的。低级的相邻解可能会根据基于质量和温度参数德差异的概率被接受。温度参数随着算法的进程被修改以改变搜索的性质。
反作用搜索优化的重点在于将机器学习与优化的结合，加入内部反馈回路以根据问题、根据实例、根据当前解的附近情况的特点自动调整算法的自由参数。
禁忌搜索（TS）类似于模拟退火，他们都是通过测试独立解的突变来遍历解空间的。而模拟退火算法对于一个独立解只生成一个突变，禁忌搜索会产生许多变异解并且移动到产生的解中的符合度最低的一个。为了防止循环并且促进在解空间中的更大进展，由部分或完整的解组建维系了一个禁忌列表。移动到元素包含于禁忌列表的解是禁止，禁忌列表随着解遍历解空间的过程而不断更新。
人工免疫系统（AIS）算法仿照了脊椎动物的免疫系统。
粒子群优化（PSO），群智能方法。
引力搜索算法（GSA），群智能方法。
蚁群聚类方法（ACCM中），这个方法利用了聚类方法扩展了蚁群优化。
随机传播搜索（SDS），基于代理的概率全局搜索和优化技术，最适合于将目标函数分解成多个独立的分布函数的优化问题。

== 历史 ==
蚁群优化算法的年表：
1959年，Pierre-Paul Grassé发明了Stigmergy理论来解释白蚁建设巢的行为。
1983年，Deneubourg和他的同事们研究了蚂蚁的集体行为。
1988年，Moyson Manderick写了一篇蚂蚁自组织的文章。
1989年，Goss, Aron, Deneubourg和Pasteels关于阿根廷蚂蚁的集体行为的研究，给蚁群优化算法的思想提供了灵感。 
1989年，Ebling和他的同事落实了觅食行为的模型。
1991年，M. Dorigo在他的博士论文中提出了蚂蚁系统（文章于1992年发表）。一份从论文中提取的技术报告五年后出版，由V. Maniezzo和A.Colorni合著。
1996年，蚂蚁系统的文章出版。
1996年，Hoos与Stützle发明了最大最小蚂蚁系统。
1997年，Dorigo和Gambardella发布了蚁群系统。 
1997年，Schoonderwoerd和他的同事们开发了对电信网络的首次应用。
1998年，Dorigo发起了第一次蚁群算法的专题会议。
1998年，Stützle提出初步的并行实现。
1999年，Bonabeau, Dorigo和Theraulaz的出版了一本书，主要关于人工蚂蚁。
2000年，未来计算机系统杂志上发表了蚂蚁算法特刊。
2000年，调度的最早期的应用程序，调度了序列和约束的满意度。
2000年，Gutjahr提供了一个蚁群算法收敛的第一个证据。
2001年，COA算法首次被使用（Eurobios和AntOptima）。
2001年，IREDA和他的同事们发表了第一个多对象算法。
2002年，调度设计的首次应用，贝叶斯网络。
2002年，比安奇和她的同事提出了随机问题的最早算法。
2004年，Zlochin和Dorigo表明，有些算法等价于随机梯度下降法，Cross-entropy method和估计分布算法。
2005年，首次在蛋白质折叠问题上的应用。




联结主义是认知科学领域的一种方法，期望能够以人工神经网络 （ANN）来解释心灵现象。 
具有隐藏层的连结（ANN）模型

==基本原则==
联结主义的中心原则是使用，简单且经常一致的单元互联网络，来描述心理现象。不同模型的联结及单元形式可以有所不同。例如，网络的单元及联结可以分别表示神经元及突触,如同人脑那样。  

=== 扩散激活 ===
在大多数联结主义模型中，网络会随着时间而变化。联结主义模型的一个密切且普遍的特征是激活 。任何时候，网络中的单元都会有个激活，而该激活是表示该单元在某方面的数值。例如，如果模型中的单位是神经元，则激活可以表示神经元产生动作电位峰值的概率 。激活通常会传递到与其联结的所有其他单元。扩散激活一直是神经网络模型的特征，而该特征在认知心理学家使用的联结主义模型中也很常见。 

===神经网络===
迄今为止，神经网络是最常用的联结主义模型。尽管神经网络模型种类繁多，但它们几乎始终遵循关于思维的两个基本原则：
# 任何心理状态都可以描述为网络中神经单元上数字激活值的N维向量 。
# 记忆是通过修改神经单元之间的联结强度来创建的。联结强度或“权重”通常表示为N×N 矩阵 。 
大多数神经网络模型中的变化来自：
* 单元的解释（interpretation of units）：单元可以解释为神经元或神经元组。
* 激活的定义（definition of activation）：激活可以通过多种方式进行定义。例如，在玻尔兹曼机中，激活被解释为产生动作电位尖峰的概率，并通过逻辑函数确定输入到单元的总和。
* 学习演算法（learning algorithm）：不同的网络以不同的方式修改其联结。通常，联结权重会随时间的改变，用于该变化的数学定义都称为“学习演算法”。 
联结主义者一致认为，与前馈神经网络 （无循环的有向网络，称为DAG ）相比，循环神经网络 （其中的网络联结可以形成有向循环的有向网络）是更好的大脑模型。许多递回联结主义模型也纳入了动力学系统理论 。许多研究者，例如联结主义者Paul Smolensky ，皆认为联结主义模型将朝着完全地连续、高维度、非线性的动态系统方式发展。

===生物现实主义===
一般而言，联结主义者的工作不需要具有生物学上的现实意义，因此缺乏神经科学的合理性。 

===学习===
神经网络中的权重根据某些学习规则或演算法（如赫布学习)进行调整。因此，联结主义者为神经网络创建了许多复杂的学习过程。学习总是涉及修改联结权重。通常，给定的数据集由神经单元的某个子集的激活向量组成时，这些公式会涉及到数学公式，以确定权重的变化。设计基于联结主义的教学方法是近年来研究的热点。  
借由这种方式正规化学习，联结主义者能够使用许多工具。在联结主义的学习方法中，有一种很常见的策略是在以权重矩阵所定义的空间中，于该空间的误差表面与梯度下降合并。在联结主义者模型中，所有梯度下降学习都涉及通过误差表面对应于权重的偏导数来更改每个权重。反向传播 （BP）最早流行于1980年代，它可能是当今最普遍的联结主义梯度下降算法。

==历史==
联结主义的思想可以追溯到一个世纪以前，在20世纪中后期之前，这些思想仍仅止于猜测。

===-{zh-hans:分布式并行处理; zh-tw:分散式平行处理}-（PDP）===
当今流行的联结主义方法，最初称为分散式平行处理 （英语：Parallel distributed processing，PDP），是一种人工神经网络方法，强调了神经处理的并行性以及神经表征的分散性，为研究人员提供了一个通用的数学框架。主要包括八个方面： 
* 一组处理单元，由一组整数表示。
* 单元的激活，由时间相关函数的向量表示。
* 单元的输出函数，由激活函数的向量表示。
* 单元之间的连通性模式，由表示联结强度的实数矩阵表示。
* 通过联结传播激活的传播规则，以单位输出上的函数表示。
* 激活规则，用于组合送到单元的输入，以确定单元有新的激活，由当前激活和传播函数表示。
* 根据经验修改联结的学习规则，以基于任意数量的变量的权重变化表示。
* 为系统提供经验的环境，以单元之中某些子集的激活向量集表示。 
许多造成PDP发展的研究都是在1970年代完成，但是直到1980年代出版了《 -{zh-hans:分布式并行处理; zh-tw:分散式平行处理}-：认知微观结构的探索》第一卷（基础）及第二卷（心理和生物学模型），作者为詹姆斯·L·麦克莱兰德 ，戴维·E·鲁梅尔哈特和PDP研究小组，PDP才开始流行。 如今，这些书被认为是有著巨大影响的联结主义著作，虽然书中未使用过“联结主义”一词，但仍普遍视PDP等同于联结主义。 

===早期工作===
PDP的直接根源是研究人员的感知器理论，例如1950年代和1960年代的弗兰克·罗森布拉特 (Frank Rosenblatt)。但是，由马文·明斯基和西摩尔·派普特在1969年出版的《 感知器 》一书，使感知器模型变得非常不得人心。它详述了单层（无隐藏层）感知器计算功能的局限性，甚至无法执行如异或问题 （exclusive disjunction，如判断这是苹果还是桔子，但不是二者皆是）这样的简单功能。 借由证明多层次非线性神经网络更加强大，并可用于大量的函数阵列，PDP克服了这项难关。  
许多早期的研究人员提倡联结主义风格的模型，例如1940年代和1950年代的沃伦·麦卡洛克、沃尔特·皮茨 （ MP神经元 ）、唐纳德·赫布和卡尔·拉什利。麦卡洛克和皮茨展示了神经系统如何实现一阶逻辑 ：其经典论文《神经活动中内在思想的逻辑演算》（1943）深深影响了这方面的发展，而他们则是受到了尼古拉斯·拉舍夫斯基（ Nicolas Rashevsky） 在1930年代的重要成果之影响。赫布对神经功能的推测做出了巨大贡献，并提出了一种叫做赫布学习的学习原则 ，一直使用至今。拉什利（Lashley）主张采用分散式表示法，这是因为他在多年的病变实验中，并未发现任何类似于局部印迹的东西。

===PDP以外的联结主义 ===
虽然PDP是联结主义的主要形式，但还有其他理论工作也应归类为联结主义。
许多联结主义的原理可以追溯到心理学的早期工作，例如威廉•詹姆斯。 基于人脑知识的心理学理论在19世纪后期很流行。早在1869年，神经学家约翰·休格林·杰克逊(John Hughlings Jackson)就主张多层次分散式系统。在此基础上，赫伯特·斯宾塞的《心理学原理 》第3版（1872年）和西格蒙德·弗洛伊德的《科学心理学计划》（1895年）提出了联结主义或原型联结主义理论，而这些往往只是推测性的理论。到了20世纪初，爱德华·桑戴克进行了尝试建立联结型网络的实验。 
哈耶克在1920年发表的一篇论文中，独立构思了赫本突触学习模型，将该模型发展成由赫本突触网路所构成的「全球脑理论」，构成更大的地图系统和记忆网路date=March 2015。弗兰克·罗森布拉特（Frank Rosenblatt）在其感知器论文中引用了哈耶克的突破性成果。
联结主义模型的另一种形式，是由语言学家悉尼·兰姆（Sydney Lamb）在1960年代开发的关系网络框架。关系网络仅由语言学家使用，从未与PDP方法统一，因此，现今很少研究人员使用。 
另外还有混合联结主义模型，主要是将符号表征与神经网络模型混合在一起。一些如Ron Sun的研究人员提倡混合方法。 

==联结主义与计算主义之争==
随着联结主义在1980年代后期变得越来越流行，一些研究人员（包括杰瑞·福多，史迪芬·平克等）对此表示反对。他们认为，随着当时的发展，联结主义威胁到经典的计算主义方法，并抹去认知科学和心理学领域正在取得的进步。计算主义是认知主义的一种特殊形式，认为心理活动是计算性的，也就是说，大脑通过对图灵机之类的符号，执行纯粹的形式运算来进行操作。一些研究人员认为，联结主义的趋势代表着联想主义的回归，以及对思想语言概念的放弃，而他们对此感到不以为然。相比之下，联结主义的趋势使得联结主义对其它研究人员而言，变得更具吸引力。 
联结主义和计算主义不必然矛盾，但是在1980年代末和1990年代初的争论，造成了两种方法之间的对立。在整个争论中，一些研究人员认为，尽管尚未就此问题完全达成共识，但是联结主义和计算主义可完全兼容。两种方法的差异包括：
* 计算主义者假定符号模型在结构上类似于大脑的底层结构，而联结主义者则进行“低层次”建模，以确保其模型类似于神经结构。
* 一般来说，计算主义者专注于关注外在符号的结构（心智模型）和内部操作的句法规则，而联结论者则关注从环境刺激中学习，并以神经元之间的联结形式存储此信息。
* 计算主义者认为，内在的心理活动是由显式符号的操作组成，而联结主义者认为，对于心理活动，显式符号的操作会给出拙劣模型。
* 计算主义者通常会假设存在领域特定的符号子系统，辅助特定认知领域的学习（例如语言、意图、数字），而联结主义者则提出一个或一小部分的通用学习机制。
尽管存在这些差异，一些理论家提出，联结主义架构只是有机大脑碰巧实现符号操作系统的一种方式。这在逻辑上是可能的，因为众所周知的事实，联结主义模型可以实现计算主义模型中的符号操作系统，date=February 2011如果这个模型要解释人类执行「符号操作任务」的能力，则确实必须做到这一点。但争论的焦点在于这种符号操作是否构成了一般认知的基础，所以这并不是对计算主义的潜在辩护。然而，举例来说，计算性描述可能有助于对逻辑认知的高级描述。
争论主要集中在逻辑论点上，即联结主义网络是否能产生「这种在推理中观察到的句法结构」。尽管联结主义的处理过程在大脑中不太可能实现，date=February 2011，但后来仍实现了这样的句法结构，date=February 2011因此争论仍在持续。截至2016年,神经生理学进展和一般神经网络的理解的进步，导致了很多此类的早期问题得以成功地建模,因此，关于基本认知的争论在很大程度上取决于赞成联结主义的神经科学家。 date=March 2012。然而，这些近期的发展尚未在心理学或心灵哲学等其他领域中达成共识。
近年来date=February 2016，动态系统流行于心灵哲学领域，为这一争论提供了新的视角; 一些作者现在认为date=February 2016，联结主义和计算主义之间的任何分裂，更确切地说是计算主义和动态系统之间的分裂。
2014年， DeepMind的 Alex Graves等人发表了一系列论文，描述了一种新型的深度神经网络结构，称为神经图灵机 该结构能够读取磁带上的符号，并将符号存储在记忆体中。关系网络是DeepMind发行的另一个深度网络模块，能够创建类似客体的表征形式，并操纵它们来回答复杂的问题。关系网络和神经图灵机进一步证明了联结主义和计算主义不必矛盾。 

==参见==
* Associationism
* 人工智能
* 行为主义
* 自组织映射
* Pandemonium_architecture
* 机器学习
* 特征整合论
* 消除唯物主义
* 控制论（Cybernetics）

==参考文献==
*大卫·鲁梅尔哈特（Rumelhart, D.E.）、詹姆斯·麦克莱（J.L. McClelland）和PDP研究小组(1986)：平行分布式处理: Explorations in the Microstructure of Cognition.第1册: Foundations，Cambridge, MA: MIT Press
*詹姆斯·麦克莱（McClelland, J.L.）、大卫·鲁梅尔哈特（D.E. Rumelhart）和PDP研究小组(1986)：平行分布式处理: Explorations in the Microstructure of Cognition.第2册:心理和生物模型，Cambridge，MA: MIT Press 
* Pinker, Steven and Mehler, Jacques (1988). 联结和Symbols, Cambridge MA: MIT Press.
* Jeffrey L. Elman, Elizabeth A. Bates, Mark H.Johnson, Annette Karmiloff-Smith, Domenico Parisi, Kim Plunkett (1996). Rethinking Innateness: A 联结主义 perspective on development，Cambridge MA: MIT Press.
* Marcus, Gary F. (2001). The Algebraic Mind: Integrating 联结主义和认知科学(学习、发展和Conceptual Change), Cambridge, MA: MIT Press

==外部链接==
*  联结主义简史
*  Dictionary of Philosophy of Mind entry on connectionism
*  Stanford Encyclopedia of Philosophy entry on 联结主义
*  A demonstration of Interactive Activation and Competition Networks
*  关于联结主义的心灵哲学词典
* author-link=James Garson
*  互动激活和竞争网络的演示
* title=Connectionism
*  对联结主义的批判
category:主义




这是一个学科的列表。学科是在大学教学(教育)与研究的知识分科。学科是被发表研究和学术杂志、学会和系所所定义及承认的。
领域通常有子领域或分科，而其之间的分界是随便且模糊的。
在中世纪的欧洲，大学里只有四个学系：神学、医学、法学和艺术，而最后一个的地位稍微低于另外三个的地位。在中世纪至十九世纪晚期的大学世俗化过程中，传统的课程开始增辅进了非古典的语言及文学、物理、化学、生物和工程等学科，现今的学科起源便源自于此。到了二十世纪初期，教育学、社会学及心理学也开始出现在大学的课程里了。
以下简表展示出各大类科目，以及各大类科目中的主要科目。

== 30px形式科学 ==

=== 数学 ===
另见：AMS  数学科目分类（外部连结）
* 代数
** 群论
*** 群表示论
** 环论
*** 交换代数
** 体论
** 线性代数（向量空间）
** 多重线性代数
** 李代数
** 结合代数
** 泛代数
** 同调代数
** 微分代数
** 格 (数学)（序理论）
** 表示论
** K-理论
** 范畴论
* 数学分析
** 实变函数论
*** 微积分学
** 复分析
** 泛函分析
** 非标准分析
** 傅立叶分析
** 常微分方程
** 偏微分方程
* 机率论
** 测度
** 遍历理论
** 随机过程
* 几何学与拓扑学
** 平面几何学
** 立体几何学
** 解析几何学
** 点集拓扑学
** 代数拓扑
** 微分拓扑
** 代数几何
** 微分几何
** 射影几何
** 仿射几何学
** 非欧几里得几何
* 数论
** 解析数论
** 代数数论
*  逻辑与数学基础
** 集合论
** 证明论
** 模型论
** 可计算性理论
** 模态逻辑
** 直觉主义逻辑
* 理论数学
** 趣味数学
** 数学哲学
* 应用数学
** 统计学
*** 数理统计学
*** 计量经济学
*** 精算
*** 人口学
** 数值分析
** 作业研究
*** 最优化
*** 线性规划
*** 动态规划
*** 任务分配问题
*** 系统分析
*** 随机过程
** 动力系统
*** 混沌理论
*** 碎形
** 数学物理
*** 量子力学
*** 量子场论
*** 量子重力
**** 弦理论
*** 统计力学
** 计算理论
*** 计算复杂性理论
** 资讯理论
** 密码学
** 组合数学
*** 编码理论
** 图论
** 赛局理论

=== 统计学 ===
* 计算统计学
** 数据挖掘
** 回归分析
** 模拟
*** 自助法
* 试验设计
** 变异数分析
** 反应曲面法
* 调查取样
** 抽样
* 统计模型
** 生物统计学
*** 流行病学
** 多变量分析
*** 时间序列
** 可靠度理论
** 品质控制
* 统计理论
** 决策论
** 数理统计学
*** 机率
** 社会统计调查

=== 系统科学 ===
* 复杂系统
* 模控学
* 控制理论
** 控制工程
** 控制系统
** 动力系统
* 作业研究
* 系统动力学
* 系统工程
** 系统分析
* 系统科学
** 发展系统理论
** 动态系统理论

=== 电脑科学 ===
与ACM  电脑科学分类系统
* 计算理论
** 自动机理论（形式语言）
** 可计算性理论
** 计算复杂性理论
** 并行性理论
* 演算法
** 随机化演算法
** 分散式演算法
** 平行演算法
* 计算机系统结构
* 作业系统
* 电脑网络
** 信息理论
** 网际网路，全球资讯网
** 无线网路（行动运算）
** 普适计算
** 云端运算
* 电脑安全与可靠性
** 密码学
* 分布式计算
** 网格计算
* 并行计算
* 量子计算机
* 软件工程
** 形式化方法（形式验证）
* 计算机图形学
** 图像处理
** 科学可视化
** 计算几何
* 程序语言
** 编程范型
*** 物件导向程序设计
*** 函数程式语言
** 形式语义学
** 类型论
** 编译器
* 商业信息学
** 信息科技
** 管理信息系统
** 医学信息学
* 人机交互
* 信息学
** 数据管理
** 数据挖掘
** 数据库
*** 关系数据库
** 信息检索
** 信息管理
** 知识管理
** 多媒体
* 人工智能
** 认知科学
*** 自动推理
*** 机器学习
**** 人工神经网路
**** 支持向量机
*** 自然语言处理（计算语言学）
*** 计算机视觉
** 专家系统
* 在数学、自然科学、工程学与医学上的运算
** 数值分析
** 计算数学
** 计算科学
** 计算物理学
** 计算化学
** 计算神经科学
** 电脑辅助工程
*** 有限元素分析
*** 计算流体力学
* 在社会科学、艺术、人文学科与职业上的运算
** 计算社会学
** 金融工程学
* 电脑与社会
** 计算机硬件历史
** 计算机科学历史

=== 逻辑 ===
* 数理逻辑
** 集合论
** 证明论
** 模型论
** 可计算性理论
** 模态逻辑
** 直觉主义逻辑
* 哲学逻辑
** 模态逻辑
*** 义务逻辑
*** 信念逻辑
** 逻辑推理
* 计算机逻辑
** 形式语义学
** 形式化方法（形式验证）
** 类型论
** 逻辑编程
** 多值逻辑
*** 模糊逻辑

== 30px自然科学 ==

=== 太空科学 ===
* 天体生物学/宇宙生物学
* 宇宙学
** 行星科学
*** 行星地质学
** 恒星天文学
*** 太阳天文学
** 月质学
* 天文学
** 观测天文学
*** 无线电天文学
*** 微波天文学
*** 红外天文学
*** 可见光天文学
*** 紫外线天文学
*** X射线天文学
*** 伽马射线天文学
* 天文物理学
** 重力论
*** 黑洞
** 星际物质
** 直接数值模拟应用在
*** 电浆体天体物理学
*** 星系的形成和演化
*** 高能天文物理学
*** 流体动力学
*** 磁流体力学
*** 恒星形成
** 物理宇宙学
*** 量子力学
** 恒星天文物理学
*** 日震
*** 恒星演化
*** 恒星核合成

=== 生命科学 ===
* 生物化学
* 生物资讯学
* 生物物理学
* 湖沼学
* 生命工学
** 克隆学
* 生物分类法
* 真菌学
* 寄生虫学
* 病理学
* 生理学
** 人体生理学
*** 运动生理学
* 系统分类学（分类学）

==== 生物学 ====
* 演化论
* 解剖学
** 比较解剖学
** 人体解剖学
* 植物学
** 民族植物学 
** 藻类学
* 生物地理学
* 细胞生物学
* 时间生物学（生物钟学）
* 低温生物学
* 发育生物学
** 胚胎学
* 生态学
** 人类生态学
** 景观生态学
* 遗传学
** 分子遗传学
** 群体遗传学
* 内分泌学
* 演化生物学
* 人体生物学
* 海洋生物学
* 微生物学
* 分子生物学
* 营养学
* 神经科学
** 行为神经科学
* 古生物学
* 病毒学
** 分子病毒学
** 流行病毒学
* 天体生物学
* 动物学
** 动物通讯
** 昆虫学
** 动物行为学
** 爬虫两栖类学
** 鱼类学
** 鸟卵学
** 鸟类学
** 恐龙学
** 灵长类动物学
** 动物解剖学
** 神秘动物学

=== 化学 ===
* 大气化学
* 地球化学
* 宇宙化学
* 分析化学
** 仪器分析
* 生物化学
* 药物化学
* 化学资讯学
* 计算化学
* 材料科学
* 土壤化学
* 石油化学
* 环境化学
* 化学工业
* 量子化学
* 无机化学
** 元素化学
** 无机合成化学
* 有机化学
** 有机金属化学
** 有机合成化学
** 天然有机化学
* 物理化学
* 核化学
* 食品化学
* 辐射化学
* 理论化学
** 化学哲学
** 量子化学

=== 物理学 ===
* 声学
* 应用物理学
* 天体物理学
* 原子，分子与光学物理学
* 生物物理学
* 计算物理学
* 凝聚态物理学
* 低温物理学
* 电磁学
* 粒子物理学
* 流体动力学
* 地球物理学
* 材料科学
* 数学物理
* 医学物理
* 力学
* 分子物理学
* 牛顿力学
* 原子核物理学
* 光学
* 电浆
* 量子力学
* 固体力学
* 固体物理学
* 统计力学
* 理论物理学
* 热力学
* 车辆动力学

=== 地球科学 ===
* 环境科学
* 环境化学
* 宝石学
* 大地测量学
* 地质学
* 地球化学
* 地貌学
* 地球物理学
* 冰河学
* 水文地质学
* 水文学
* 气象学
* 矿物学
* 海洋学
* 土壤学（Pedology）
* 古生物学
* 行星科学（另外，也是空间科学的一部分）
* 土壤科学
* 地质构造
* 火山学

== 30px社会科学 ==

=== 历史学 ===
* 古代史
** 史前史
* 文化史
* 外交史
* 文学史
* 经济史
* 政治史
* 方志学
* 系谱学/谱牒学
* 民族历史学
* 教育史
* 传播史
* 考古学
** 地质历史学
* 科技史（科学技术史） 
* 军事史/战争史
* 现代史
* 艺术史
* 金石学
* 哲学史
* 法制史
* 世界史
* 历史地理学
* 生物学史
* 自然史/博物学
* 历史语言学

==== 以地区划分的历史 ====
* 非洲史
** 古埃及历史
* 美国历史
* 阿根廷历史
* 中国历史
* 日本历史
* 欧洲历史
** 古罗马历史
* 古希腊历史
* 美索不达米亚历史
* 印度历史
* 印度尼西亚历史

=== 地理学 ===
* 人文地理学
** 文化地理学
*** 女性主义地理学
** 经济地理学
*** 发展地理学
** 历史地理学
** 音乐地理学
** 时间地理学
** 政治地理学或地缘政治学
*** 军事地理学
*** 战略地理学
** 人口地理学
** 食品地理学
** 社会地理学
*** 行为地理学
*** 儿童地理学
*** 健康地理学
*** 旅游地理学
* 自然地理学
** 生物地理学
** 气候学
*** 古气候学
** 海岸地理学
** 地貌学
** 大地测量学
** 水文学与水文地理学
*** 冰川学
*** 湖沼学
*** 海洋学
**** 海洋生物学
** 景观生态学
** 古地理学
* 地质学
** 地质历史学
** 地球空洞说
* 地图学
* 都市地理学
* 环境地理学
* 宗教地理学
* 地名学
* 地理知识学
* 理论地理学
* 地志学
* 行星学

=== 经济学 ===
* 农业经济学
* 行为经济学
* 消费者经济学
* 发展经济学
* 计量经济学
* 经济地理学
* 经济史
* 经济社会学
* 能源经济学
* 环境经济学
* 演化经济学
* 实验经济学
* 女性主义经济学
* 赛局理论
* 绿色经济
* 经济成长
* 人类发展理论
* 产业组织理论
* 讯息经济学
* 制度经济学
* 国际经济学
* 劳动经济学
* 法律经济学
* 总体经济学
* 数理经济学
* 个体经济学
* 货币经济学
* 政治经济学
* 公共财政
* 公共经济学
* 房地产经济学
* 自然资源经济学
* 社会选择理论
* 社会主义经济学
* 社会经济学
* 福利经济学

=== 政治科学 ===
* 公民教育
* 比较政治学
* 地缘政治学
* 国际关系
* 国际组织
* 政策研究
* 政治行为理论
* 政治文化
* 政治经济学
* 政治史
* 政治哲学
* 选举学
* 公共行政
** 非营利组织行政
** 非政府组织行政
* 公共政策
* 社会选择理论

=== 心理学 ===
* 变态心理学
* 应用心理学
* 生物心理学
* 临床心理学
* 认知心理学
* 社区心理学
* 比较心理学
* 消费者行为
* 咨商心理学
* 文化心理学
* 差别心理学
* 发展心理学
* 教育心理学
* 演化心理学
** 演化发展心理学
** 演化教育心理学
* 实验心理学
* 法医心理学
* 健康心理学
* 医疗心理学
* 神经心理学
* 工业与组织心理学
* 超心理学
* 人格心理学
* 政治心理学
* 正向心理学
* 心理统计学
* 宗教心理学
* 心理物理学
* 计量心理学
* 社会心理学
* 运动心理学

=== 社会学 ===
* 应用社会学
** 政治社会学
** 公共社会学
** 社会工程
** 休闲研究
* 集合行为
** 社会运动
* 社群资讯学
** 社会性网络分析
* 比较社会学
* 冲突理论
* 文化研究
* 犯罪学
* 人口学／人口
* 环境社会学
* 女性主义社会学
* 未来学
* 人类生态学
* 互动论
** 现象学
** 民族学方法论
** 符号相互作用论
** 社会建构主义
* 医疗社会学
* 组织行为学
* 科学论
* STS
* 性学
* 社会资本
* 社会控制
** 纯社会学
* 社会经济学
* 社会哲学
* 社会心理学
* 社会政策
* 社会调查
** 计算社会学
** 经济社会学／社会经济学
*** 经济发展
***  社会发展
** 文化社会学
** 越轨社会学（偏差社会学）
** 教育社会学
** 家族社会学
** 知识社会学
** 法律社会学
** 宗教社会学
** 运动社会学
** 工业社会学
* 社会理论
* 社会阶层
* 社会学理论
* 社会生物学
* 社会语言学
* 城市研究或城市社会学／农村社会学
* 影像社会学

=== 人类学 ===
* 体质人类学
** 法医人类学
** 双演化理论（基因－文化共同演化）
** 人类行为生态学
** 人类演化
** 医疗人类学
** 考古人类学
** 遗传学
*** 群体遗传学
** 灵长类动物学
* 人类语言学
** 共时语言学（又称静态语言学）
** 历史语言学（亦称越时语言学）
** 民族语言学
** 社会语言学
* 哲学人类学
* 文化人类学
** 政治人类学
** 心理人类学
** 都市人类学
** 宗教人类学
** 经济人类学
** 民族志
** 民族历史学
** 民族学
** 民族音乐学
** 民俗学
** 神话
*** 中国神话
*** 印度神话
*** 罗马神话
*** 埃及神话
*** 印度神话
*** 希腊神话

=== 考古学 ===
* 古典考古学
**石器分析
**考古植物学
* 民族考古学
* 埃及学
* 实验考古学
**海洋考古学
**地景考古学
**战场考古学
* 科技考古学
* 后过程主义考古学
* 古人类学
* 史前考古学
* 地理考古学
** 近东考古学
** 中世纪考古学

=== 语言学 ===
* 应用语言学
* 计算语言学或 自然语言处理
* 篇章分析
* 语源学
* 历史语言学
* 语言学史
* 构词学
* 文字学
* 语音学
* 音韵学
* 语用学
* 语义学
* 符号学
** 旗帜学∗
* 社会语言学
* 语法学
* 修辞学
* 国际语学

=== 文化与种族研究 ===
* 亚洲研究
* 亚裔美国人研究
* 非洲研究与非裔美人研究
* 奇卡诺研究（墨西哥裔研究）
* 童年研究
* 拉丁裔研究
* 美洲原住民研究

=== 性别与性欲特质研究 ===
* 女性心理学
* 性别研究／性别理论
* 异性恋主义
* 人类性行为
* 人类性欲特质
* 阳刚心理学
* 男性研究
* 酷儿研究／酷儿理论
* 性教育
* 性学
* 女性研究

== 30px人文学科和艺术 ==

=== 区域研究 ===
* 非洲研究
* 美洲研究
** 美国研究
*** 阿巴拉契亚研究
** 加拿大研究
** 拉丁美洲研究
* 亚洲研究
** 中东研究
*** 伊朗研究
**中亚研究
** 印度学
** 日本学
** 汉学/中国研究
*** 敦煌学
* 欧洲研究
** 凯尔特研究
** 德国研究
** 斯堪地纳维亚研究
** 斯拉夫研究
** 俄国研究
** 爱尔兰研究
** 东欧研究
**伊比利亚研究

=== 视觉艺术 ===
* 美术史
* 书法
* 艺术鉴赏
* 创意艺术
* 素描
* 美术
* 绘画
* 电影制作
* 摄影
* 综合媒材
* 版画
* 工作室艺术
* 雕塑
* 艺术保存与复原
* 文化创意
* 广告学
* 多媒体

=== 文学 ===
* 英语文学
** 美国文学
*** 非裔美国人文学
** 英国文学
** 印度文学
** 爱尔兰文学
** 中世纪文学
** 后现代主义文学
* 世界文学
** 西洋古典学
** 比较文学
** 中国文学
*** 红学
*** 金学
** 法语文学
** 盖尔语文学
** 德语文学与奥地利文学
** 印地语文学
** 希伯来语文学
** 日本文学
** 义大利文学
** 葡萄牙语文学与巴西文学
** 俄国文学
** 西班牙文学
** 意第绪语文学
* 西方文学理论
** 批判理论
** 文艺评论
** 诗学
** 修辞学
* 创意写作
** 创造性记实文学
** 小说写作
** 记实文学写作
** 诗歌写作
** 电影剧本创作
** 剧作家
* 比较文学

==== 英文研究 ====
* 英文语言学
* 英文社会语言学
* 英文篇章分析
* 写作学习
* 世界的英文
* 英语史
* 把英文做为第二语言教授
* 美国文学
** 非裔美国人文学
** 犹太美国人文学
** 美国南方文学
* 澳洲文学
* 英国文学（英格兰之外的文学可能以凯尔特语写作）
** 英语文学
** 北爱尔兰文学
** 苏格兰文学
** 威尔斯文学
* 加拿大文学（大部份的加拿大文学使用法语写作）
* 印度文学
* 爱尔兰文学
* 纽西兰文学
* 奈及利亚文学

=== 表演艺术 ===

==== 音乐 ====
* 伴奏
* 室内乐
* 交响乐
* 管乐
* 弦乐
* 宗教音乐
* 作曲
* 指挥
** 合唱指挥
** 管弦乐团指挥
* 音乐教育
* 音乐历史
* 乐理
* 音乐地理学
** 乐种
* 录音
* 音乐学
** 民族音乐学
** 音乐社会学
** 音响学
* 管弦乐研究
* 爵士乐研究
* 乐曲分析
* 表演与文学
** 风琴与键盘乐器
** 钢琴
** 弦乐器，竖琴和吉他
** 歌唱
** 民谣
** 木管乐器
** 铜管乐器
** 打击乐器

==== 舞蹈 ====
* 编舞
* 舞蹈记录法
* 民族舞蹈
* 舞蹈史
* 舞蹈分析
* 舞蹈研究

==== 戏剧 ====
* 戏剧史
* 表演
* 导演
* 演出法
* 剧作家
* 音乐剧
* 戏剧

==== 电影 ====
* 动画设计
* 电影艺术
** 编导艺术
** 表演艺术
** 摄影艺术
* 电影评论

=== 哲学 ===
* 形上学
** 本体论
** 目的论
** 心灵哲学
** 行动理论
* 知识论
* 伦理学
** 规范伦理学
** 元伦理学
** 价值理论
** 道德心理学
** 应用伦理学
*** 动物权利
*** 生物伦理学
*** 环境伦理
* 美学
* 社会哲学与政治哲学
** 女性主义哲学（女权论）
** 无政府主义
** 马克思主义
* 哲学传统与学派
** 非洲哲学
** 亚里士多德学派
** 分析哲学
** 欧陆哲学
** 东方哲学
** 女性主义哲学
* 哲学史
** 古代哲学
** 中国哲学
** 中世纪哲学
** 17世纪哲学
** 当代哲学
* 逻辑学（理则学）
** 哲学逻辑
** 数理逻辑
* 应用哲学
** 教育哲学
** 历史哲学
** 宗教哲学
*** 神学
** 语言哲学
** 数学哲学
** 科学哲学
*** 生物学哲学
*** 经济哲学
*** 化学哲学
*** 地理哲学
*** 数学哲学
*** 物理哲学 
**** 量子力学与哲学
* 区域哲学
* 解释学
* 神秘学

=== 宗教研究 ===
* 亚伯拉罕诸教
** 基督教
*** 基督教神学
**** 圣经神学
**** 解释学
**** 神学主体
***** 基督论
***** 圣灵学
**** 救赎论
**** 法律和福音
**** 教会学
**** 末世论
**** 罪论
**** 自然神学
** 伊斯兰教
*** 伊斯兰教史
*** 古兰经
*** 圣训
** 犹太教
*** 犹太教史
*** 犹太哲学
*** 塔木德
*** 哈拉卡
*** 米德拉什
* 印度宗教
** 佛教
** 印度教
** 耆那教
** 锡克教
* 东亚宗教
** 中国民间宗教
** 儒家
** 神道
** 道教
* 其他宗教
** 古埃及宗教
** 诺斯底主义
** 西方神秘传统* （西方秘教）
** 新兴宗教
** 祆教
* 无神论与宗教人文主义
* 宗教比较
* 神话与民俗学
* 说教术
* 符号学
** 旗帜学

== 30px职业与应用科学 ==

=== 农业/农学 ===
* 农学
* 林学
* 土壤学
* 昆虫学
* 植物病理学
* 农业经济学
* 农产运销学
* 水产学
* 园艺学
* 植物学
* 动物学

=== 工商管理 ===
*会计学
* 商业学、职业道德
* 金融学
*技术经济及管理
*农林经济管理
*林业经济管理 
*公共管理 
*行政管理
*社会医学与卫生事业管理
*教育经济与管理
*社会保障 
*土地资源管理 
* 劳资关系
** 国际相对劳动
** 劳动经济学额
** 劳动史
** 劳动统计学
* 信息系统
* 管理学
** 人力资源管理
** 财务管理
** 市场营销学
*旅游管理
** 导游
** 酒店管理    
* 制造业

=== 企业 ===
* 工商管理
* 企业分析
* 商业道德
* 商法
* E化企业
* 企业家精神
* 金融
* 劳资关系
** 集体谈判
** 人力资源
** 组织行为学
** 劳动经济学
** 劳工史
* 国际贸易
* 市场行销
* 采购
* 风险管理和保险
* 系统科学

=== 语言 ===
* 英语
* 法语
* 德语
* 俄语
* 意大利语
* 西班牙语
* 葡萄牙语
* 丹麦语
* 匈牙利语
* 挪威语
* 亚美尼亚语
* 瑞典语
* 芬兰语
* 中国语
* 日语
* 朝鲜语
* 越南语
* 泰语
* 斯瓦西里语
* 印度语
* 阿拉伯语
* 塞尔维亚语
* 荷兰语
* 冰岛语
* 希腊语
* 罗马尼亚语
* 拉丁语
* 梵语
* 世界语

=== 神学 ===
* 基督教历史
* 教会法
* 宣教场
** 教牧辅导
** 教牧神学

=== 建筑与设计 ===
* 建筑和相关设计
** 建筑
** 都市计划（城市设计）
** 室内设计
** 景观设计
* 纪念物保存
* 工业设计（产品设计）
** 人因工程学
** 游戏设计** 玩具与娱乐设计* 时装设计
* 视觉传达设计
** 平面设计
*** 字体设计
** 使用者介面设计
** 工程制图
* 建筑学
* 建筑设计
* 空间设计
* 包装设计
* 商业设计
* 广告设计
* 造型设计
* 多媒体

=== 教育 ===
* 批判性教学
* 课程设计
* 教育行政
* 教学领导
* 教育哲学
* 教育心理学
* 教育社会学
* 遥距教育
** 小学
** 中学
** 高等教育
** 掌握学习
** 职业教育
** 双语教学
** 军事教育
* 专业教育
** 语言教育
** 数学教育
** 音乐教育
** 艺术教育
** 道德教育
** 体育/教练
** 阅读教育
** 科学教育
** 性教育
** 特殊教育
** 幼儿教育

=== 工程学 ===
* 航空航天工程
**航空工程
**太空工程
* 农业工程
** 食品工程
** 水土保持
** 农业机械
* 建筑工程
* 生物工程学
** 生物材料工程
** 生物医学工程
* 化学工程
* 土木工程
** 土力工程
** 工程地质学
** 地震工程
** 运输工程
* 计算机工程
* 控制工程
* 生态工程
* 电机工程学
* 电子工程
** 微电子工程
** 广播工程
** 音讯工程
* 工程物理
* 环境工程
* 工业工程
* 材料科学
** 陶瓷工程
** 冶金工程
** 高分子材料工程
* 机械工程
** 动力机械工程
** 机械设计工程
** 机械材料工程
** 桥梁工程
** 制造工程
* 机电整合工程
* 矿业工程
* 纳米工程
* 核工程
* 海洋工程
** 轮机工程
** 船舶工程
* 光学工程
* 品质保证工程
* 石油工程
* 安全工程
* 软体工程
* 结构工程学
* 系统工程
* 通信工程
* 车辆工程
** 汽车工程
* 包装工程
* 品质保证
* 声学工程
* 仪表工程
* 战地工程
* 自动化

=== 家政学 ===
* 消费者教育
* 住房
* 室内设计
* 营养学
** 餐饮管理
** 酒店管理
* 纺织

=== 环境研究与林业 ===
* 环境资源管理
** 海岸管理
** 渔业管理
** 土地管理
** 自然资源管理
** 野生动物管理
* 环境政策
* 休闲生态学
* 林学
* 可持续发展
* 毒理学

=== 健康科学 ===
* 健康科学
** 细胞遗传学
** 血液学
** 细胞生物学
** 组织学
** 免疫学
** 微生物学
** 分子遗传学
** 寄生虫学
* 牙医学
** 牙科卫生员与流行病学
** 牙修复学
** 齿颚矫正学
** 口腔外科
** 牙周病学
* 中医学
** 针灸学
* 护理学
** 接生员
* 营养学与营养师
* 视光学
* 物理治疗
* 职能治疗
* 言语治疗
* 药学
* 心理学
** 临床心理学
** 健康心理学
** 辅导
* 兽医学
* 肌力与体能训练
** 有氧健身操
** 个人体能教练
* 公共卫生

==== 医学 ====
* 内科学
** 心脏内科
** 肝胆肠胃科
** 血液学
** 肿瘤科
** 麻醉学
** 皮肤科
** 内分泌学
** 流行病学
** 传染病
** 肾脏科
** 精神医学
** 复健科（康复科）
** 安宁缓和科
** 产科学
** 肿瘤学
** 儿科
** 神经内科
** 神经学
** 眼科学
*** 神经眼科学
* 外科学
** 普通外科
** 心脏外科
** 整形外科
** 血管外科
** 神经外科（有时简称脑外科）
** 泌尿外科
** 矫形外科（即骨外科）
** 小儿外科
** 移植外科
** 耳鼻喉科学
** 妇产科
** 口腔颌面外科
** 泌尿外科
*** 男科学
** 微创手术
* 司法科学
* 预防医学
* 病理学
* 影像诊断学
* 老人医学
* 妇科学
* 小儿科
* 基层医护
** 普通科医生
* 精神病学
* 影像诊断学
* 复健医学
* 胸腔医学
* 胸部心脏外科
* 整容
* 特级护理学

=== 新闻学，媒体研究和传播学 ===
* 新闻学
** 广播新闻学
** 新媒体
** 体育事件广播
* 媒体研究（大众媒体）
** 报纸
** 杂志
** 无线电
** 电视
*** 电视研究
** 网际网路
* 传播学
** 资讯理论
** 广告
** 广告学
** 市场行销
** 大众传播
** 翻译
** 政治宣传
** 公共关系
** 非言语交际

=== 法律 ===
* 法理
* 法律哲学
* 比较法
* 法律社会学
* 法律史
* 法律经济学
* 宪法
* 国内法
* 国际法
* 实体法
* 程序法
* 公法
* 私法
* 民法
* 刑法
** 刑事诉讼法
** 刑事司法
*** 警政学
*** 取证
* 土地法
** 会计法
** 公司法
** 合同法
** 环境法
** 劳动法
** 知识产权法
** 税法
** 侵权
* 行政法
** 行政诉讼法
* 教会法
* 欧陆法系
* 英美法系
* 中华法系
* 伊斯兰律法
* 法律教育

=== 图书馆与博物馆研究 ===
* 档案学
* 书目计量学
** 引文分析
* 资讯架构 
* 博物馆学 
** 艺术管理

=== 军事学 ===
* 军队学
* 空军学
* 军事活动
* 军事策划
* 军事比较系统
* 博弈论
* 联合作战研究
* 领导学
* 后勤
* 军事道德
* 军事史
* 军事情报
* 军事法
* 军事医学
* 海军学
** 海事工程
** 海上战术
** 海事建筑
** 武器系统
* 特别行动及低强度冲突
* 战略
* 战术

=== 公共行政 ===
* 矫正
* 保育生物学
* 刑事司法
* 灾害管理
* 政府事务
* 国际关系
* 公共行政
** 非营利组织管理
** 非政府组织管理
* 公共政策
** 国内政策
*** 卫生政策研究
*** 住宅政策
*** 劳工政策
*** 社会政策
*** 公园及娱乐事业管理
** 毒品禁制政策
** 能源政策
** 环境政策
** 财政政策
** 外交政策
** 移民政策
** 产业关系

=== 社会工作 ===
* 儿童福利
* 社群实践
** 社群组织
** 社会政策
** 社会法
** 社会行政
* 矫正
* 老人学
* 医疗社会工作
* 心理健康
* 校园辅导
* 社会团体工作
* 社会个案工作

=== 交通运输 ===
* 运输学
* 交通安全
* 信息图形
* 船运
** 港埠管理
* 作业研究（或属工商管理和应用数学）
* 公共运输
* 交通工程（或属工程类）
* 交通控制
* 物流管理（或属工商管理）
* 运输政策

== 参见 ==
* 学科
* 学术
* 跨学科
* 科系

== 外部连结 ==
*  Classification of Instructional Programs (CIP 2000)



统计式的语言模型是一个机率分布，给定一个长度为  的字词所组成的字串 ，派机率给字串：。
语言模型提供上下文来区分听起来相似的单词和短语。例如，短语“再给我两份葱，让我把记忆煎成饼”和“再给我两分钟，让我把记忆结成冰”听起来相似，但意思不同。
语言模型经常使用在许多自然语言处理方面的应用，如语音识别，机器翻译，词性标注，句法分析，手写体识别和资讯检索。由于字词与句子都是任意组合的长度，因此在训练过的语言模型中会出现未曾出现的字串(资料稀疏的问题)，也使得在语料库中估算字串的机率变得很困难，这也是要使用近似的平滑n-元语法(N-gram)模型之原因。
在语音辨识和在资料压缩的领域中，这种模式试图捕捉语言的特性，并预测在语音串列中的下一个字。
在语音识别中，声音与单词序列相匹配。当来自语言模型的证据与发音模型和声学模型相结合时，歧义更容易解决。
当用于资讯检索，语言模型是与文件有关的集合。以查询字「Q」作为输入，依据机率将文件作排序，而该机率代表该文件的语言模型所产生的语句之机率。

== 模型类型 ==

=== 单元语法（unigram） ===
一个单元模型可以看作是几个单状态有限自动机的组合。 它会分开上下文中不同术语的概率, 比如将  拆分为.
在这个模型中，每个单词的概率只取决于该单词在文档中的概率，所以我们只有一个状态有限自动机作为单位。自动机本身在模型的整个词汇表中有一个概率分布，总和为1。下面是一个文档的单元模型。
 
为特定查询(query)生成的概率计算如下
 
不同的文档有不同的语法模型，其中单词的命中率也不同。不同文档的概率分布用于为每个查询生成命中概率。可以根据概率对查询的文档进行排序。两个文档的单元模型示例:
在信息检索环境中，通常会对单语法语言模型进行平滑处理，以避免出现P(term)= 0的情况。一种常见的方法是为整个集合生成最大似然模型，并用每个文档的最大似然模型对集合模型进行线性插值来平滑化模型。

=== n-元语法 ===
在一个 n-元语法模型中，观测到序列  的概率  可以被近似为
 
此处我们引入马尔科夫假设，一个词的出现并不与这个句子前面的所有词关联，只与这个词前的 n 个词关联（n阶马尔科夫性质）。在已观测到 i-1 个词的情况中，观测到第i个词 wi 的概率，可以被近似为，观测到第i个词前面n个词（第 i-(n-1) 个词到第 i-1 个词）的情况下，观测到第i个词的概率。第 i 个词前 n 个词可以被称为 n-元。
条件概率可以从n-元语法模型频率计数中计算:
 
术语 二元语法(bigram) 和三元语法(trigram) 语言模型表示 n = 2 和 n = 3 的 n-元 。
典型地，n-元语法模型概率不是直接从频率计数中导出的，因为以这种方式导出的模型在面对任何之前没有明确看到的n-元时会有严重的问题。相反，某种形式的平滑是必要的，将一些总概率质量分配给看不见的单词或n-元。使用了各种方法，从简单的“加一”平滑(将计数1分配给看不见的n-元，作为一个无信息的先验)到更复杂的模型，例如Good-Turing discounting或 4=back-off 模型。

==== 例子 ====
在二元语法模型中 (n = 2) , I saw the red house 这个句子的概率可以被估计为
 
而在三元语法模型中，这个句子的概率估计为
 
注意前 n-1 个词的 n-元会用句首符号 

=== 指数型 ===
语言模型用特征函数编码了词和n-元的关系。 

其中  是4=分区函数,  是参数向量，  是特征函数。
在最简单的情况下，特征函数只是某个n-gram存在的指示器。使用先验的 a 或者使用一些正则化的手段是很有用的。
对数双线性模型是指数型语言模型的另一个例子。

==外部链接==
* LMSharp - 开源统计语言模型工具包，支持n-gram模型（Kneser-Ney平滑），以及反馈神经网络模型（recurrent neural network model）




文本挖掘有时也被称为文字探勘、文本数据挖掘等，大致相当于文字分析，一般指文本处理过程中产生高质量的信息。高质量的信息通常通过分类和预测来产生，如模式识别。文本挖掘通常涉及输入文本的处理过程（通常进行分析，同时加上一些衍生语言特征以及消除杂音，随后插入到数据库中） ，产生结构化数据，并最终评价和解释输出。'高品质'的文本挖掘通常是指某种组合的相关性，新颖性和趣味性。典型的文本挖掘方法包括文本分类，文本聚类，概念/实体挖掘，生产精确分类，观点分析，文档摘要和实体关系模型（即，学习已命名实体之间的关系） 。
文本分析包括了信息检索、词典分析来研究词语的频数分布、模式识别、标签\注释、信息抽取，数据挖掘技术包括链接和关联分析、可视化和预测分析。本质上，首要的任务是，通过自然语言处理（NLP）和分析方法，将文本转化为数据进行分析。

==文本挖掘与文本分析==

==历史==
劳工密集型的人工纯文字挖掘方法最早出现在20世纪80年代中期，但在过去的十年中，技术的进步已经使这一领域迅速取得进展。文本挖掘已经是信息检索、数据挖掘、机器学习、统计以及计算语言学等学科中的重要领域。由于目前的大多数信息（80％）是以文本的形式来保存，文本挖掘被认为具有较高的商业潜在价值。
多语种数据挖掘已经越来越多的引起人们的兴趣：能够根据自己的意愿从跨语种的文字来源中挖掘出有用的信息。

== 文本分析过程 == 

== 应用 ==

===安全应用===
许多文本挖掘的软件包是面对安全设备的。它们多数是出于国家安全的的目的，监控和分析类似于互联网新闻、博客等的在线纯文本。
对文本挖掘的研究还被包含在文本解密的领域中。

=== 生物医学应用 ===

=== 软件应用 ===

===在线媒体应用===

===营销应用===

===情感分析===

===学术应用===

===数位人文学与计算社会学===

== 软件和应用==

===开源软件和应用===
Weka工具 http://www.cs.waikato.ac.nz/ml/weka/

==知识产权法与文本挖掘==

==影响==

==注释==

==参考资料==
* Ronen Feldman and James Sanger, The Text Mining Handbook, Cambridge University Press, ISBN 9780521836579
* Kao Anne, Poteet, Steve R. (Editors), Natural Language Processing and Text Mining, Springer, ISBN 184628175X 
* Konchady Manu "Text Mining Application Programming (Programming Series)" by Manu Konchady, Charles River Media, ISBN 1584504609
* M. Ikonomakis, S. Kotsiantis, V. Tampakas, Text Classification Using Machine Learning Techniques, WSEAS Transactions on Computers, Issue 8, Volume 4, August 2005, pp. 966-974 (https://web.archive.org/web/20081203004649/http://www.math.upatras.gr/~esdlab/en/members/kotsiantis/Text%20Classification%20final%20journal.pdf)

== 更多链接==
* http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC
* http://projects.ldc.upenn.edu/ace/ ACE (LDC)
* https://web.archive.org/web/20060308054306/http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE (NIST)
* https://web.archive.org/web/20070928002315/http://www.arts-humanities.net/text_mining (Discussion group text mining)
*  Text Analysis Portal for Research (TAPoR)
* http://textanalytics.wikidot.com/ Text Analytics Wiki
*  Getting started in text mining
*  Pimiento A Text-Mining Application Framework written in Java.




统计机器翻译（Statistical Machine Translation，简写为SMT）是机器翻译的一种，也是目前非限定领域机器翻译中性能较佳的一种方法。统计机器翻译的基本思想是通过对大量的平行语料进行统计分析，构建统计翻译模型，进而使用此模型进行翻译。从早期基于词的机器翻译已经过渡到基于短语的翻译，并正在融合句法信息，以进一步提高翻译的精确性。
2016年前Google翻译的大部分语言对采用的都是统计机器翻译的方法。而Google亦在此本领域保持领先地位，在美国国家标准局组织的机器翻译评测中遥遥领先。但Google翻译在2016年11月开始使用神经机器翻译作为主要翻译系统，并开发了Google神经机器翻译系统。此外，常用的基于统计法机器翻译的系统还包括Bing翻译和百度翻译等。
统计机器翻译的首要任务是为语言的产生构造某种合理的统计模型，并在此统计模型基础上，定义要估计的模型参数，并设计参数估计算法。早期的基于词的统计机器翻译采用的是噪声信道模型，采用最大似然准则进行无监督训练，而近年来常用的基于短语的统计机器翻译则采用区分性训练方法，一般来说需要参考语料进行有监督训练。

== 历史 ==
早在1949年，瓦伦·韦弗就基于香农的信息论提出了统计机器翻译的基本思想。而最早提出可行的统计机器翻译模型的是IBM研究院的研究人员。他们在著名的文章《统计机器翻译的数学理论：参数估计》中提出了由简及繁的五种词到词的统计模型，分别被称为IBM Model 1到IBM Model 5。这五种模型均为噪声信道模型，而其中所提出的参数估计算法均基于最大似然估计。然而由于计算条件的限制和平行语料库的缺乏，尚无法实现基于大规模数据的计算。其后，由Stephan Vogel提出了基于隐马尔科夫模型的统计模型也受到重视，被认为可以较好的替代IBM Model 2.
在此文发表后6年，即1999年，约翰·霍普金斯大学夏季讨论班集中了一批研究人员实现了GIZA软件包，实现了IBM Model 1到IBM Model 5。Franz-Joseph Och在随后对GIZA进行了优化，加快了训练速度，特别是IBM Model 3到5的训练。同时他还提出了更加复杂的Model 6。Och发布的软件包被命名为GIZA++，直到现在，该软件包还是绝大部分机器翻译系统的基石。目前，针对大规模语料的训练，已有GIZA++的若干并行化版本存在。
基于词的统计机器翻译虽然开辟了统计机器翻译这条道路，其性能却由于建模单元过小而受到极大限制。同时，产生性（generative）模型使得模型适应性较差。因此，许多研究者开始转向基于短语的翻译方法。Franz-Josef Och再次凭借其出色的研究，推动了统计机器翻译技术的发展，他提出的基于最大熵模型的区分性训练方法使得统计机器翻译的性能极大提高并在此后数年间远远超过其他方法。更进一步的，Och又提出修改最大熵方法的优化准则，直接针对客观评价标准进行优化，从而产生了今天广泛采用的最小错误训练方法（Minimum Error Rate Training）。
另一件促进统计机器翻译进一步发展的重要发明是翻译结果自动评价方法的出现，这些方法为翻译结果提供了客观的评价标准，从而避免了人工评价的繁琐与昂贵。这其中最为重要的评价是BLEU评价指标。虽然许多研究者抱怨BLEU与人工评价相差甚远，并且对于一些小的错误极其敏感，绝大部分研究者仍然使用BLEU作为评价其研究结果的首要（如果不是唯一）的标准。 
于2012年提出的LEPOR评价指标在权威的计算语言学会统计机器翻译Workshop（ACL-WMT）的八个语言对语料上的实验得到了比BLEU与人的评价更高相关性的分数。
是目前维护较好的开源机器翻译软件，由爱丁堡大学研究人员组织开发。其发布使得以往繁琐复杂的处理简单化。

== 模型 ==

=== 噪声信道模型 ===
噪声信道模型假定，源语言中的句子（信宿）是由目标语言中的句子（信源）经过含有噪声的信道编码后得到的。那么，如果已知了信宿和信道的性质，我们可以得到信源产生信宿的概率，即。而寻找最佳的翻译结果也就等同于寻找：

利用贝叶斯公式，并考虑对给定，为常量，上式即等同于

由此，我们得到了两部分概率：
# ，指给定信源，观察到信号的概率。在此称为翻译模型。
# ，信源发生的概率。在此称为语言模型
可以这样理解翻译模型与语言模型，翻译模型是一种语言到另一种语言的词汇间的对应关系，而语言模型则体现了某种语言本身的性质。翻译模型保证翻译的意义，而语言模型保证翻译的流畅。从中国对翻译的传统要求“信达雅”三点上看，翻译模型体现了信与达，而雅则在语言模型中得到反映。
原则上任何语言模型均可以应用到上述公式中，因此以下讨论集中于翻译模型。在IBM提出的模型中，翻译概率被定义为：

词对齐示例
其中的被定义为隐含变量——词对齐（Word Alignment），所谓词对齐，简而言之就是知道源语言句子中某个词是由目标语言中哪个词翻译而来的。例如右图中，一个词可以被翻译为一个或多个词，甚至不被翻译。于是，获取翻译概率的问题转化为词对齐问题。IBM系列模型及HMM, Model 6都是词对齐的参数化模型。它们之间的区别在于模型参数的数量，类型各不相同。例如IBM Model 1，唯一的参数是词翻译概率，与词在句子中的位置无关。也就是说：

其中是词对齐中的一条连接，表示源语言中的第个词翻译到目标语言中的第个词。注意这里的翻译概率是词之间而非位置之间的。IBM Model 2的参数中增加了词在句子中的位置，公式为：

其中分别为源、目标语言的句子长度。
HMM模型将IBM Model 2中的绝对位置更改为相对位置，即相对上一个词连接的位置，而IBM Model 3,4,5及Model 6引入了“Fertility Model”，代表一个词翻译为若干词的概率。
在参数估计方面，一般采用最大似然准则进行无监督训练，对于大量的“平行语料”，亦即一些互为翻译的句子

由于并没有直接的符号化最优解，实践中采用EM算法。首先，通过现有模型，对每对句子估计全部可能的（或部分最可能的）词对齐的概率，统计所有参数值发生的加权频次，最后进行归一化。对于IBM Model 1,2，由于不需要Fertility Model，有简化公式可获得全部可能词对齐的统计量，而对于其他模型，遍历所有词对齐是NP难的。因此，只能采取折衷的办法。首先，定义Viterbi对齐为当前模型参数下，概率最大的词对齐：

在获取了Viterbi对齐后，可以只统计该对齐结果的相关统计量，亦可以根据该对齐，做少许修改后（即寻找“临近”的对齐）后再计算统计量。IBM 3,4,5及Model 6都是采用这种方法。
目前直接采用噪声信道模型进行完整机器翻译的系统并不多见，然而其副产品——词对齐却成为了各种统计机器翻译系统的基石。时至今日，大部分系统仍然首先使用GIZA++对大量的平行语料进行词对齐。由于所面对的平行语料越来越多，对速度的关注使得MGIZA++，PGIZA++等并行化实现得到应用。噪声信道模型和词对齐仍然是研究的热点，虽然对于印欧语系诸语言，GIZA++的对齐错误率已经很低，在阿拉伯语，中文等语言与印欧语系语言的对齐中错误率仍然很高。特别是中文，错误率常常达到30%以上。所谓九层之台，起于累土，缺乏精确的词对齐是中文机器翻译远远落后于其他语言的原因。虽然目前出现了一些区分性词对齐技术，无监督对齐仍然是其中的重要组成部分。

=== 判别式模型 ===
判别式模型噪声信道模型（产生式模型）不同，它不应用贝叶斯公式，而是直接对条件概率建模。

==== 特征函数 ====
在这个框架下，个特征函数

通过参数化公式

其中是每个特征函数的权重，也是模型所要估计的参数集，记为。基于这个模型，获取给定源语言句子，最佳翻译的决策准则为：

简而言之，就是找到使得特征函数最大的解。
原则上，任何特征函数都可以被置于此框架下，噪声信道模型中的翻译模型、语言模型都可以作为特征函数。并且，在产生式模型中无法使用的“反向翻译模型”，即也可以很容易的被引入这个框架中。目前基于短语的翻译系统中，最常用的特征函数包括
*短语翻译概率
*词翻译概率（短语中每个词的翻译概率）
*反向短语翻译概率
*反向词翻译概率
*语言模型
而一些基于句法的特征也在被加入。

==== 优化准则 ====
优化准则指的是给定训练语料，如何估计模型参数。一般来说，训练模型参数需要一系列已翻译的文本，每个源语言句子拥有个参考翻译。
早期，区分性训练被置于最大熵准则下，即：

这一准则简单快速且由于优化目标是凸的，收敛速度快。然而，一个极大的问题是，“信息熵”本身和翻译质量并无联系，优化信息熵以期获得较好的翻译结果在逻辑上较难说明。借助客观评价准则如BLEU，希望直接针对这些客观准则进行优化能够提升翻译性能。由此而产生最小化错误率训练算法。通过优化系统参数，使得翻译系统在客观评价准则上的得分越来越高，同时，不断改进客观评价准则，使得客观评价准则与主观评价准则越来越接近是目前统计机器翻译的两条主线。
使用这些客观评价准则作为优化目标，即：

的一个主要问题是，无法保证收敛性。并且由于无法得到误差函数（即客观评价准则）的导数，限制了可使用的优化方法。目前常用的方法多为改进的Powell法，一般来说训练时间颇长且无法针对大量数据进行训练。

==== 调序模型 ====
许多语言对的语序是有很大差别的。在前述词对齐模型中，包含有词调序模型，在区分性训练中也需要较好的调序模型。调序模型可以是基于位置，也就是描述两种语言每个句子不同位置的短语的调序概率，也可以是基于短语本身，例如Moses中的调序模型即是基于短语本身，描述在给定当前短语对条件下，其前后短语对是否互换位置。由于现实中的调序模型远非“互换位置”这么简单，而是牵涉句法知识，调序的效果仍然不佳。目前重定位问题还是机器翻译中亟待解决的问题。

=== 解码 ===
无论采用哪种模型，在进行实际翻译过程中，都需要进行解码。所谓解码，即是指给定模型参数和待翻译句子，搜索使概率最大（或代价最小）的翻译结果的过程。同许多序列标注问题，例如中文分词问题类似，解码搜索可以采用分支定界或启发式深度优先搜索（A*）方法。一般来说，搜索算法首先构造搜索网络，也就是将待翻译句子与可能的翻译结果融合为一个加权有限状态转换机（Weighted Finite State Transducer），而后在此网络上搜索最优路径。

== 基本流程 ==
统计机器翻译同大多数的机器学习方法相类似，有训练及解码两个阶段，其中训练阶段的目标是获得模型参数，而解码阶段的目标则是利用所估计的参数和给定的优化目标，获取待翻译语句的最佳翻译结果。对于基于短语的统计机器翻译来说，“训练”阶段较难界定，严格来说，只有最小错误率训练一个阶段可称为训练。但是一般来说，词对齐和短语抽取阶段也被归为训练阶段。

=== 语料获取及预处理 ===
语料预处理阶段，需要搜集或下载平行语料，所谓平行语料，指的是语料中每一行的两个句子互为翻译。目前网络上有大量可供下载的平行语料。搜寻适合目标领域（如医疗、新闻等）的语料是提高特定领域统计机器翻译系统性能的重要方法。
在获取语料后，需要进行一定得文本规范化处理，例如对英语进行词素切分，例如将's独立为一个词，将与词相连的符号隔离开等。而对中文则需要进行分词。同是，尽可能过滤一些包含错误编码的句子，过长的句子或长度不匹配（相差过大）的句子。
获取的语料可分为三部分，第一部分用于词对齐及短语抽取，第二部分用于最小错误率训练，第三部分则用于系统评价。第二第三部分的数据中，每个源语言句子最好能有多条参考翻译。

=== 词对齐 ===
首先，使用GIZA++对平行语料进行对齐。由于GIZA++是“单向”的词对齐，故而对齐应当进行两次，一次从源到目标，第二次从目标到源。一般来说，GIZA++需要依次进行IBM Model 1, HMM及IBM Model 3,4的对齐，因IBM Model 2对齐效果不佳，而IBM Model 5耗时过长且对性能没有较大贡献。根据平行语料的大小不同及所设置的迭代次数多少，训练时间可能很长。一个参考数据为，1千万句中文-英文平行语料（约3亿词）在Intel Xeon 2.4GHz服务器上运行时间约为6天。如果耗时过长可考虑使用MGIZA++和PGIZA++进行并行对齐（PGIZA++支持分布式对齐）。
其后，对两个方向的GIZA++对齐结果进行合并，供短语抽取之用。

=== 短语抽取 ===
短语抽取的基本准则为，两个短语之间有至少一个词对有连接，且没有任何词连接于短语外的词。Moses软件包包含短语抽取程序，抽取结果将占有大量的磁盘空间。建议若平行语料大小达到1千万句，短语最大长度大于等于7，至少应准备500GB的存储空间。

=== 短语特征准备 ===
在短语抽取完毕后，可进行短语特征提取，即计算短语翻译概率及短语的词翻译概率。该需要对抽取的所有短语进行两次排序，一般来说，中等规模（百万句语料）的系统亦需要进行外部排序，磁盘读写速度对处理时间影响极大。建议在高速磁盘上运行。参考运行时间及磁盘空间消耗：前述千万句语料库，限制短语长度7，外部排序运行于SCSI Raid 0+1磁盘阵列，运行时间3日11小时，峰值磁盘空间消耗813GB。

=== 语言模型训练 ===
语言模型训练请参考语言模型。在区分性训练框架下，允许使用多个语言模型，因此，使用由大语料训练得到的无限领域语言模型配合领域相关的语言模型能够得到最好的效果。

=== 最小化错误率训练 ===
最小化错误率训练通过在所准备的第二部分数据——优化集（Tuning Set）上优化特征权重，使得给定的优化准则最优化。一般常见的优化准则包括信息熵、BLEU、TER等。这一阶段需要使用解码器对优化集进行多次解码，每次解码产生N个得分最高的结果，并调整特征权重。当权重被调整时，N个结果的排序也会发生变化，而得分最高者，即解码结果，将被用于计算BLEU得分或TER。当得到一组新的权重，使得整个优化集的得分得到改进后，将重新进行下一轮解码。如此往复直至不能观察到新的改进。
根据选取的N值的不同，优化集的大小，模型大小及解码器速度，训练时间可能需要数小时或数日。

=== 解码及系统评价 ===
使用经最小化错误率训练得到的权重，即可进行解码。一般此时即可在测试集上进行系统性能评价。在客观评价基础上，有一些有条件的机构还常常进行主观评价。

== 难点及研究方向 ==
机器翻译金字塔
统计机器翻译的难点主要在于模型中所包含句法、语义成分较低，因而在处理句法差别较大的语言对，例如中文-英文时将遇到问题。有时翻译结果虽然“词词都对”却无法被人阅读。可以说目前主流（如Moses）统计机器翻译仍然处于机器翻译金字塔的底层。目前大量的研究集中于将句法知识引入框架中，例如使用依存文法限制翻译路径，等。
同时，统计机器翻译依赖巨大的语料库，随着语料库资源越来越丰富和算法的日趋复杂，处理这些语料需要越来越强大的计算能力。长期以来，Google在机器翻译领域的领先地位就得益于其强大的分布式计算能力。随着分布式计算的普及，将机器翻译相关技术并行化将是另一研究热点。
最后，机器翻译依赖客观评价准则，而客观评价准则最终要与主观评价准则挂钩。每年各类机器翻译相关的会议上都会有若干关于客观评价准则的研究发表，总的来说，评价翻译的优劣本身就是一个人工智能问题，其难度绝不在机器翻译之下。

== 参考文献 ==

== 外部链接 ==
*  Statistical Machine Translation—研究介绍、会议、语料及软件列表
*  GIZA++:开源词对齐软件
*  MGIZA++/PGIZA++: GIZA++的并行化版本
*  Moses:开源机器翻译软件
*  Annotated list of statistical natural language processing resources




感知器（英语：Perceptron）是Frank Rosenblatt在1957年就职于康奈尔航空实验室（Cornell Aeronautical Laboratory）时所发明的一种人工神经网路。它可以被视为一种最简单形式的前馈神经网路，是一种二元线性分类器。
Frank Rosenblatt给出了相应的感知机学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知机模型。
神经细胞结构示意图
感知机是生物神经细胞的简单抽象。神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为『是』，而未激动时为『否』。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。
在人工神经网络领域中，感知机也被指为单层的人工神经网络，以区别于较复杂的多层感知机（Multilayer Perceptron）。作为一种线性分类器，（单层）感知机可说是最简单的前向人工神经网络形式。尽管结构简单，感知机能够学习并解决相当复杂的问题。感知机主要的本质缺陷是它不能处理线性不可分问题。

==历史==
1943年，心理学家沃伦·麦卡洛克和数理逻辑学家沃尔特·皮茨在合作的《A logical calculus of the ideas immanent in nervous activity》论文中提出并给出了人工神经网络的概念及人工神经元的数学模型，从而开创了人工神经网络研究的时代。1949年，心理学家唐纳德·赫布在《The Organization of Behavior》论文中描述了神经元学习法则——赫布型学习。
人工神经网络更进一步被美国神经学家弗兰克·罗森布拉特所发展。他提出了可以模拟人类感知能力的机器，并称之为『感知机』。1957年，在Cornell航空实验室中，他成功在IBM 704机上完成了感知机的仿真。两年后，他又成功实现了能够识别一些英文字母、基于感知机的神经计算机——Mark1，并于1960年6月23日，展示与众。
为了『教导』感知机识别图像，弗兰克·罗森布拉特在Hebb学习法则的基础上，发展了一种迭代、试错、类似于人类学习过程的学习算法——感知机学习。除了能够识别出现较多次的字母，感知机也能对不同书写方式的字母图像进行概括和归纳。但是，由于本身的局限，感知机除了那些包含在训练集里的图像以外，不能对受干扰（半遮蔽、不同大小、平移、旋转）的字母图像进行可靠的识别。
首个有关感知机的成果，由弗兰克·罗森布拉特于1958年发表在《The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain》的文章里。1962年，他又出版了《Principles of Neurodynamics: Perceptrons and the theory of brain mechanisms》一书，向大众深入解释感知机的理论知识及背景假设。此书介绍了一些重要的概念及定理证明，例如感知机收敛定理。
虽然最初被认为有着良好的发展潜能，但感知机最终被证明不能处理诸多的模式识别问题。1969年，马文·明斯基和西摩尔·派普特在《Perceptrons》书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限，证明感知机不能解决简单的异或（XOR）等线性不可分问题，但弗兰克·罗森布拉特和马文·明斯基和西摩尔·派普特等人在当时已经了解到多层神经网络能够解决线性不可分的问题。
由于弗兰克·罗森布拉特等人没能够及时推广感知机学习算法到多层神经网络上，又由于《Perceptrons》在研究领域中的巨大影响，及人们对书中论点的误解，造成了人工神经领域发展的长年停滞及低潮，直到人们认识到多层感知机没有单层感知机固有的缺陷及反向传播算法在80年代的提出，才有所恢复。1987年，书中的错误得到了校正，并更名再版为《Perceptrons - Expanded Edition》。
近年，在Freund及Schapire（1998）使用核技巧改进感知机学习算法之后，愈来愈多的人对感知机学习算法产生兴趣。后来的研究表明除了二元分类，感知机也能应用在较复杂、被称为structured learning类型的任务上（Collins, 2002），又或使用在分布式计算环境中的大规模机器学习问题上（McDonald, Hall and Mann, 2011）。

==定义==
感知器使用特征向量来表示的前馈神经网络，它是一种二元分类器，把矩阵上的输入（实数值向量）映射到输出值上（一个二元的值）。

是实数的表示权重的向量，是点积。是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。
（0或1）用于对进行分类，看它是肯定的还是否定的，这属于二元分类问题。如果是负的，那么加权后的输入必须产生一个肯定的值并且大于，这样才能令分类神经元大于阈值0。从空间上看，偏置改变了决策边界的位置（虽然不是定向的）。
由于输入直接经过权重关系转换为输出，所以感知器可以被视为最简单形式的前馈式人工神经网络。

==结构==
300px
设有维输入的单个感知机（如右图示），至为维输入向量的各个分量，至为各个输入分量连接到感知机的权量（或称权值），为偏置，为激活函数（又曰激励函数或传递函数），为标量输出。输出的数学描述为：
其中，及为反对称的符号函数，其定义为：
从式(1)可知，偏置被引申为权量，而对应的输入值为。故，一感知机的输出行为是求得输入向量与权向量的内积后，经一个激活函数所得一个标量结果。
设输入向量与权向量的内积为零，可得出维的超平面。平面的法向量为，并经过维输入空间的原点。法向量指向的输入空间，其输出值为，而与法向量反向的输入空间，其输出值则为。故可知这个超平面定义了决策边界，并把输入空间划分为二。

==准则函数==
设一训练集为，其中表示输入，而是对应的目标输出。由于符号函数的不连续性，如果采用标准的均方误差，所得误差函数必然是不连续的，因而基于梯度的学习算法也就不能被使用。为此，Rosenblatt提出了感知机准则函数：
其中是被当前错误分类的的输入向量集合。当时，为，而当时，为。故，误差函数是一组正数的和，又或当训练集里所有输入都被正确分类时，等于零。

==学习算法==
学习算法对于所有的神经元都是一样的，因此下面所有东西都要独立的应用于每个神经元。我们首先定义一些变量：
*表示n维输入向量中的第j项
*表示权重向量的第j项
*表示神经元接受输入产生的输出
*是一个常数，符合（接受率）
更进一步，为了简便我们假定偏置量等于0。因为一个额外的维维，可以用的形式加到输入向量，这样我们就可以用代替偏置量。
感知器的学习通过对所有训练实例进行多次的迭代进行更新的方式来建模。令表示一个有个训练实例的训练集。
每次迭代权重向量以如下方式更新：
对于每个中的每个对，

注意这意味着，仅当针对给定训练实例产生的输出值与预期的输出值不同时，权重向量才会发生改变。
如果存在一个正的常数和权重向量，对所有的满足，训练集就被叫被做线性分隔的。Novikoff（1962）证明如果训练集是线性分隔的，那么感知器算法可以在有限次迭代后收敛，错误的数量由限定，其中为输入向量的最大平均值。
然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。

==参见==
*机器学习
*神经网络
*统计学习

==参考文献==
*Freund, Y. and Schapire, R. E. 1998. Large margin classification using the perceptron algorithm. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT' 98). ACM Press.
*Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179-191.
*Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386-408.
*Minsky M L and Papert S A 1969 Perceptrons (Cambridge, MA: MIT Press)
*Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615-622. Polytechnic Institute of Brooklyn.
*Widrow, B., Lehr, M.A., "30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation," Proc. IEEE, vol 78, no 9, pp. 1415-1442, (1990)。
*神经网络设计（Neural Network Design），美哈根等著，戴葵等译。机械工业出版社。ISBN 9787111075851

==外部连结==
*Chapter 3  Weighted networks - the perceptron and chapter 4  Perceptron learning of  Neural Networks - A Systematic Introduction by Raúl Rojas（ISBN 978-3540605058）
* History of perceptrons
* Mathematics of perceptrons
* Perceptron demo applet and an introduction by examples




人工智能的历史源远流长。在古代的神话传说中，技艺高超的工匠可以制作人造人，并为其赋予智能或意识。2004现代意义上的AI始于古典哲学家用机械符号处理的观点解释人类思考过程的尝试。20世纪40年代基于抽象数学推理的可编程数字计算机的发明使一批科学家开始严肃地探讨构造一个电子大脑的可能性。
1956年，在达特茅斯学院举行的一次会议上正式确立了人工智能的研究领域。会议的参加者在接下来的数十年间是AI研究的领军人物。他们中有许多人预言，经过一代人的努力，与人类具有同等智能水平的机器将会出现。同时，上千万美元被投入到AI研究中，以期实现这一目标。
研究人员发现自己大大低估了这一工程的难度，人工智慧史上共出现过好几次低潮。由于詹姆斯·莱特希尔爵士的批评和国会方面的压力，美国和英国政府于1973年停止向没有明确目标的人工智能研究项目拨款。七年之后受到日本政府研究规划的刺激，美国政府和企业再次在AI领域投入数十亿研究经费，但这些投资者在80年代末重新撤回了投资。AI研究领域诸如此类的高潮和低谷不断交替出现；至今仍有人对AI的前景作出异常乐观的预测。
尽管在政府官僚和风投资本家那里经历了大起大落，AI领域仍在取得进展。某些在20世纪70年代被认为不可能解决的问题今天已经获得了圆满解决并已成功应用在商业产品上。与第一代AI研究人员的乐观估计不同，具有与人类同等智能水平的机器至今仍未出现。图灵在1950年发表的一篇催生现代智能机器研究的著名论文中称，“我们只能看到眼前的一小段距离……但是，我们可以看到仍有许多工作要做”。

== 先驱 ==
McCorduck写道2004：“某种形式上的人工智能是一个遍布于西方知识分子历史的观点，是一个急需被实现的梦想，”先民对人工智能的追求表现在诸多神话，传说，故事，预言以及制作机器人偶（自动机）的实践之中。pp=5–35

=== 神话，幻想和预言中的AI ===
希腊神话中已经出现了机械人和人造人，如赫淮斯托斯的黄金机器人和皮格马利翁的伽拉忒亚。中世纪出现了使用巫术或炼金术将意识赋予无生命物质的传说，如贾比尔的Takwin，帕拉塞尔苏斯的何蒙库鲁兹和Judah Loew的魔像。19世纪的幻想小说中出现了人造人和会思考的机器之类题材，例如玛丽·雪莱的《弗兰肯斯坦》和卡雷尔·恰佩克的《罗素姆的万能机器人》。pp=17–25Samuel Butler的《机器中的达尔文（Darwin among the Machines）》一文（1863）探讨了机器通过自然选择进化出智能的可能性。1863至今人工智能仍然是科幻小说的重要元素。

=== 自动人偶 ===
加扎利的可编程自动人偶(1206年)
许多文明中都有创造自动人偶的杰出工匠，例如偃师（中国西周），希罗（希腊），加扎利2005和Wolfgang von Kempelen 等等。已知最古老的“机器人”是古埃及和古希腊的圣像，忠实的信徒认为工匠为这些神像赋予了思想，使它们具有智慧和激情。赫耳墨斯·特里斯墨吉斯忒斯（赫耳墨斯·特里斯墨吉斯忒斯）写道“当发现神的本性时，人就能够重现他”

=== 形式推理 ===
人工智能的基本假设是人类的思考过程可以机械化。对于机械化推理（即所谓“形式推理（formal reasoning）”）的研究已有很长历史。中国，印度和希腊哲学家均已在公元前的第一个千年里提出了形式推理的结构化方法。他们的想法为后世的哲学家所继承和发展，其中著名的有亚里士多德（对三段论逻辑进行了形式分析），欧几里得（其著作《几何原本》是形式推理的典范），花剌子密（代数学的先驱，“algorithm”一词由他的名字演变而来）以及一些欧洲经院哲学家，如奥卡姆的威廉和邓斯·司各脱。
马略卡哲学家拉蒙·柳利（1232-1315）开发了一些“逻辑机”，试图通过逻辑方法获取知识。 柳利的机器能够将基本的，无可否认的真理通过机械手段用简单的逻辑操作进行组合，以求生成所有可能的知识。Llull的工作对莱布尼兹产生了很大影响，后者进一步发展了他的思想。
莱布尼兹猜测人类的思想可以简化为机械计算
在17世纪中，莱布尼兹，托马斯·霍布斯和笛卡儿尝试将理性的思考系统化为代数学或几何学那样的体系。霍布斯在其著作《利维坦》中有一句名言：“推理就是计算（reason is nothing but reckoning）。” 莱布尼兹设想了一种用于推理的普适语言（他的通用表意文字），能将推理规约为计算，从而使“哲学家之间，就像会计师之间一样，不再需要争辩。他们只需拿出铅笔放在石板上，然后向对方说（如果想要的话，可以请一位朋友作为证人）：‘我们开始算吧。’” 这些哲学家已经开始明确提出形式符号系统的假设，而这一假设将成为AI研究的指导思想。
在20世纪，数理逻辑研究上的突破使得人工智能好像呼之欲出。这方面的基础著作包括布尔的《思维的定律》与弗雷格的《概念文字》。基于弗雷格的系统，罗素和怀特海在他们于1913年出版的巨著《数学原理》中对数学的基础给出了形式化描述。这一成就激励了希尔伯特，后者向20世纪20年代和30年代的数学家提出了一个基础性的难题：“能否将所有的数学推理形式化?” 这个问题的最终回答由哥德尔不完备定理，图灵机和Alonzo Church的λ演算给出。他们的答案令人震惊：首先，他们证明了数理逻辑的局限性；其次（这一点对AI更重要），他们的工作隐含了任何形式的数学推理都能在这些限制之下机械化的可能性。
在摩尔学校的电气工程的ENIAC计算机.
邱奇-图灵论题暗示，一台仅能处理0和1这样简单二元符号的机械设备能够模拟任意数学推理过程。这里最关键的灵感是图灵机：这一看似简单的理论构造抓住了抽象符号处理的本质。这一创造激发科学家们探讨让机器思考的可能。

=== 计算机科学 ===
计算机硬体历史
用于计算的机器古已有之；历史上许多数学家对其作出了改进。19世纪初，查尔斯·巴贝奇设计了一台可编程计算机（“分析机”），但未能建造出来。爱达·勒芙蕾丝预言，这台机器“将创作出无限复杂，无限宽广的精妙的科学乐章”。（她常被认为是第一个程序员，因为她留下的一些笔记完整地描述了使用这一机器计算伯努利数的方法。）
第一批现代计算机是二战期间建造的大型译码机（包括Z3，ENIAC和Colossus等）。后两个机器的理论基础是图灵和约翰·冯·诺伊曼提出和发展的学说。

== 人工智能的诞生：1943 - 1956 ==
IBM 702:第一代AI研究者使用的电脑.
在20世纪40年代和50年代，来自不同领域（数学，心理学，工程学，经济学和政治学）的一批科学家开始探讨制造人工大脑的可能性。1956年，人工智能被确立为一门学科。

=== 控制论与早期神经网络 ===
最初的人工智能研究是30年代末到50年代初的一系列科学进展交汇的产物。神经学研究发现大脑是由神经元组成的电子网络，其激励电平只存在“有”和“无”两种状态，不存在中间状态。维纳的控制论描述了电子网络的控制和稳定性。克劳德·香农提出的信息论则描述了数字信号（即高低电平代表的二进制信号）。图灵的计算理论证明数字信号足以描述任何形式的计算。这些密切相关的想法暗示了构建电子大脑的可能性。
这一阶段的工作包括一些机器人的研发，例如W. Grey Walter的“乌龟（turtles）”，还有“约翰霍普金斯兽”（Johns Hopkins Beast）。这些机器并未使用计算机，数字电路和符号推理；控制它们的是纯粹的模拟电路。
Walter Pitts和Warren McCulloch分析了理想化的人工神经元网络，并且指出了它们进行简单逻辑运算的机制。他们是最早描述所谓“神经网络”的学者。马文·明斯基是他们的学生，当时是一名24岁的研究生。1951年他与Dean Edmonds一道建造了第一台神经网络机，称为SNARC。在接下来的五十年中，明斯基是AI领域最重要的领导者和创新者之一。

=== 游戏AI ===
1951年，Christopher Strachey使用曼彻斯特大学的Ferranti Mark 1机器写出了一个西洋跳棋（checkers）程序；Dietrich Prinz则写出了一个国际象棋程序。亚瑟·山谬尔（Arthur Samuel）在五十年代中期和六十年代初开发的西洋棋程序的棋力已经可以挑战具有相当水平的业余爱好者。游戏AI一直被认为是评价AI进展的一种标准。

=== 图灵测试 ===
1950年，图灵发表了一篇划时代的论文，文中预言了创造出具有真正智能的机器的可能性。由于注意到“智能”这一概念难以确切定义，他提出了著名的图灵测试：如果一台机器能够与人类展开对话（通过电传设备）而不能被辨别出其机器身份，那么称这台机器具有智能。这一简化使得图灵能够令人信服地说明“思考的机器”是可能的。论文中还回答了对这一假说的各种常见质疑。图灵测试是人工智能哲学方面第一个严肃的提案。

=== 符号推理与“逻辑理论家”程序 ===
50年代中期，随着数字计算机的兴起，一些科学家直觉地感到可以进行数字操作的机器也应当可以进行符号操作，而符号操作可能是人类思维的本质。这是创造智能机器的一条新路。
1955年，艾伦·纽厄尔和后来荣获诺贝尔奖的赫伯特·西蒙在J. C. Shaw的协助下开发了“逻辑理论家（Logic Theorist）”。这个程序能够证明《数学原理》中前52个定理中的38个，其中某些证明比原著更加新颖和精巧。Simon认为他们已经“解决了神秘的心/身问题，解释了物质构成的系统如何获得心灵的性质。” （这一断言的哲学立场后来被John Searle称为“强人工智能”，即机器可以像人一样具有思想。）

=== 1956年达特茅斯会议：AI的诞生 ===
1956年达特矛斯会议的组织者是马文·明斯基，约翰·麦卡锡和另两位资深科学家克劳德·香农以及内森·罗彻斯特（Nathan Rochester），后者来自IBM。会议提出的断言之一是“学习或者智能的任何其他特性的每一个方面都应能被精确地加以描述，使得机器可以对其进行模拟。” 与会者包括雷·索罗门诺夫（Ray Solomonoff），奥利佛·塞尔弗里奇（Oliver Selfridge），Trenchard More，亚瑟·山谬尔（Arthur Samuel），艾伦·纽厄尔和赫伯特·西蒙，他们中的每一位都将在AI研究的第一个十年中作出重要贡献。会上纽厄尔和西蒙讨论了“逻辑理论家”，而麦卡锡则说服与会者接受“人工智能”一词作为本领域的名称。1956年达特矛斯会议上AI的名称和任务得以确定，同时出现了最初的成就和最早的一批研究者，因此这一事件被广泛承认为AI诞生的标志。

== 黄金年代：1956 - 1974 ==
达特茅斯会议之后的数年是大发现的时代。对许多人而言，这一阶段开发出的程序堪称神奇：计算机可以解决代数应用题，证明几何定理，学习和使用英语。当时大多数人几乎无法相信机器能够如此“智能”。 研究者们在私下的交流和公开发表的论文中表达出相当乐观的情绪，认为具有完全智能的机器将在二十年内出现。 DARPA（国防高等研究计划署）等政府机构向这一新兴领域投入了大笔资金。

=== 研究工作 ===
从50年代后期到60年代涌现了大批成功的AI程序和新的研究方向。下面列举其中最具影响的几个。

==== 搜索式推理 ====
许多AI程序使用相同的基本算法。为实现一个目标（例如赢得游戏或证明定理），它们一步步地前进，就像在迷宫中寻找出路一般；如果遇到了死胡同则进行回溯。这就是“搜索式推理”。
这一思想遇到的主要困难是，在很多问题中，“迷宫”里可能的线路总数是一个天文数字（所谓“指数爆炸”）。研究者使用启发式算法去掉那些不太可能导出正确答案的支路，从而缩小搜索范围。
艾伦·纽厄尔和赫伯特·西蒙试图通过其“通用解题器（General Problem Solver）”程序，将这一算法推广到一般情形。另一些基于搜索算法证明几何与代数问题的程序也给人们留下了深刻印象，例如赫伯特·吉宁特（Herbert Gelernter）的几何定理证明机（1958）和马文·李·闵斯基的学生James Slagle开发的SAINT（1961）。还有一些程序通过搜索目标和子目标作出决策，如斯坦福大学为控制机器人Shakey而开发的STRIPS系统。
一个语义网的例子

==== 自然语言 ====
AI研究的一个重要目标是使计算机能够通过自然语言（例如英语）进行交流。早期的一个成功范例是Daniel Bobrow的程序STUDENT，它能够解决高中程度的代数应用题。
如果用节点表示语义概念（例如“房子”，“门”），用节点间的连线表示语义关系（例如“有 -- 一个”），就可以构造出“语义网（semantic net）”。第一个使用语义网的AI程序由Ross Quillian开发； 而最为成功（也是最有争议）的一个则是Roger Schank的“概念关联（Conceptual Dependency）”。
Joseph Weizenbaum的ELIZA是第一个聊天机器人，可能也是最有趣的会说英语的程序。与ELIZA“聊天”的用户有时会误以为自己是在和人类，而不是和一个程序，交谈。但是实际上ELIZA根本不知道自己在说什么。它只是按固定套路作答，或者用符合语法的方式将问题复述一遍。

==== 微世界 ====
60年代后期，麻省理工大学AI实验室的马文·闵斯基和西摩尔·派普特建议AI研究者们专注于被称为“微世界”的简单场景。他们指出在成熟的学科中往往使用简化模型帮助基本原则的理解，例如物理学中的光滑平面和完美刚体。许多这类研究的场景是“积木世界”，其中包括一个平面，上面摆放着一些不同形状，尺寸和颜色的积木。
在这一指导思想下，杰拉德·杰伊·萨斯曼（研究组长），Adolfo Guzman，大卫·瓦尔兹（David Waltz，“约束传播（constraint propagation）”的提出者），特别是Patrick Winston等人在机器视觉领域作出了创造性贡献。同时，Minsky和Papert制作了一个会搭积木的机器臂，从而将“积木世界”变为现实。微世界程序的最高成就是Terry Winograd的SHRDLU，它能用普通的英语句子与人交流，还能作出决策并执行操作。

=== 乐观思潮 ===
第一代AI研究者们曾作出了如下预言:
*1958年，艾伦·纽厄尔和赫伯特·西蒙：“十年之内，数字计算机将成为国际象棋世界冠军。” “十年之内，数字计算机将发现并证明一个重要的数学定理。”
*1965年，赫伯特·西蒙：“二十年内，机器将能完成人能做到的一切工作。”
*1967年，马文·闵斯基：“一代之内……创造‘人工智能’的问题将获得实质上的解决。”
*1970年，马文·闵斯基：“在三到八年的时间里我们将得到一台具有人类平均智能的机器。”

=== 经费 ===
1963年6月，MIT从新建立的ARPA（即后来的DARPA，国防高等研究计划局）获得了二百二十万美元经费，用于资助MAC工程，其中包括Minsky和McCarthy五年前建立的AI研究组。此后ARPA每年提供三百万美元，直到七十年代为止。ARPA还对艾伦·纽厄尔和赫伯特·西蒙在卡内基梅隆大学的工作组以及斯坦福大学AI项目（由John McCarthy于1963年创建）进行类似的资助。另一个重要的AI实验室于1965年由Donald Michie在爱丁堡大学建立。在接下来的许多年间，这四个研究机构一直是AI学术界的研究（和经费）中心。
经费几乎是无条件地提供的：时任ARPA主任的J. C. R. Licklider相信他的组织应该“资助人，而不是项目”，并且允许研究者去做任何感兴趣的方向。这导致了MIT无约无束的研究氛围及其hacker文化的形成，但是好景不长。

== 第一次AI低谷：1974 - 1980 ==
到了70年代，AI开始遭遇批评，随之而来的还有资金上的困难。AI研究者们对其课题的难度未能作出正确判断：此前的过于乐观使人们期望过高，当承诺无法兑现时，对AI的资助就缩减或取消了。同时，由于马文·闵斯基对感知器的激烈批评，联结主义（即神经网络）销声匿迹了十年。70年代后期，尽管遭遇了公众的误解，AI在逻辑编程，常识推理等一些领域还是有所进展。

=== 问题 ===
70年代初，AI遭遇了瓶颈。即使是最杰出的AI程序也只能解决它们尝试解决的问题中最简单的一部分，也就是说所有的AI程序都只是“玩具”。AI研究者们遭遇了无法克服的基础性障碍。尽管某些局限后来被成功突破，但许多至今仍无法满意地解决。
*计算机的运算能力。当时的计算机有限的内存和处理速度不足以解决任何实际的AI问题。例如，罗斯·奎利恩（Ross Quillian）在自然语言方面的研究结果只能用一个含二十个单词的词汇表进行演示，因为内存只能容纳这么多。1976年，汉斯·莫拉维克指出，计算机离智能的要求还差上百万倍。他做了个类比：人工智能需要强大的计算能力，就像飞机需要大功率动力一样，低于一个门限时是无法实现的；但是随着能力的提升，问题逐渐会变得简单。
*计算复杂性和指数爆炸。1972年理查德·卡普根据史提芬·古克于1971年提出的Cook-Levin理论证明，许多问题只可能在指数时间内获解（即，计算时间与输入规模的幂成正比）。除了那些最简单的情况，这些问题的解决需要近乎无限长的时间。这就意味着AI中的许多玩具程序恐怕永远也不会发展为实用的系统。
*常识与推理。许多重要的AI应用，例如机器视觉和自然语言，都需要大量对世界的认识信息。程序应该知道它在看什么，或者在说些什么。这要求程序对这个世界具有儿童水平的认识。研究者们很快发现这个要求太高了：1970年没人能够做出如此巨大的数据库，也没人知道一个程序怎样才能学到如此丰富的信息。
*莫拉维克悖论。证明定理和解决几何问题对计算机而言相对容易，而一些看似简单的任务，如人脸识别或穿过屋子，实现起来却极端困难。这也是70年代中期机器视觉和机器人方面进展缓慢的原因。
*框架和资格问题。采取逻辑观点的AI研究者们（例如John McCarthy）发现，如果不对逻辑的结构进行调整，他们就无法对常见的涉及自动规划（planning or default reasoning）的推理进行表达。为解决这一问题，他们发展了新逻辑学（如非单调逻辑（non-monotonic logics）和模态逻辑（modal logics））。

=== 停止拨款 ===
由于缺乏进展，对AI提供资助的机构（如英国政府，DARPA和NRC）对无方向的AI研究逐渐停止了资助。早在1966年ALPAC（Automatic Language Processing Advisory Committee，自动语言处理顾问委员会）的报告中就有批评机器翻译进展的意味，预示了这一局面的来临。NRC（National Research Council，美国国家科学委员会）在拨款二千万美元后停止资助。1973年詹姆斯·莱特希尔针对英国AI研究状况的报告批评了AI在实现其“宏伟目标”上的完全失败，并导致了英国AI研究的低潮（该报告特别提到了指数爆炸问题，以此作为AI失败的一个原因）。DARPA则对CMU的语音理解研究项目深感失望，从而取消了每年三百万美元的资助。到了1974年已经很难再找到对AI项目的资助。
Hans Moravec将批评归咎于他的同行们不切实际的预言：“许多研究者落进了一张日益浮夸的网中”。还有一点，自从1969年Mansfield修正案通过后，DARPA被迫只资助“具有明确任务方向的研究，而不是无方向的基础研究”。60年代那种对自由探索的资助一去不复返；此后资金只提供给目标明确的特定项目，比如自动坦克，或者战役管理系统。

=== 来自大学的批评 ===
一些哲学家强烈反对AI研究者的主张。其中最早的一个是John Lucas，他认为哥德尔不完备定理已经证明形式系统（例如计算机程序）不可能判断某些陈述的真理性，但是人类可以。修伯特·德雷福斯（Hubert Dreyfus）讽刺六十年代AI界那些未实现的预言，并且批评AI的基础假设，认为人类推理实际上仅涉及少量“符号处理”，而大多是具体的，直觉的，下意识的“窍门（know how）”。 约翰·希尔勒于1980年提出“中文房间”实验，试图证明程序并不“理解”它所使用的符号，即所谓的“意向性（intentionality）”问题。希尔勒认为，如果符号对于机器而言没有意义，那么就不能认为机器是在“思考”。
AI研究者们并不太把这些批评当回事，因为它们似乎有些离题，而计算复杂性和“让程序具有常识”等问题则显得更加紧迫和严重。对于实际的计算机程序而言，“常识”和“意向性”的区别并不明显。马文·闵斯基提到德雷福斯和希尔勒时说，“他们误解了，所以应该忽略”。在MIT任教的德雷福斯遭到了AI阵营的冷遇：他后来说，AI研究者们“生怕被人看到在和我一起吃午饭”。 ELIZA程序的作者约瑟夫·维森鲍姆感到他的同事们对待德雷福斯的态度不太专业，而且有些孩子气。虽然他直言不讳地反对德雷福斯的论点，但他“清楚地表明了他们待人的方式不对”。
约瑟夫·维森鲍姆后来开始思考AI相关的伦理问题，起因是Kenneth Colby开发了一个模仿医师的聊天机器人DOCTOR，并用它当作真正的医疗工具。二人发生争执；虽然Colby认为约瑟夫·维森鲍姆对他的程序没有贡献，但这于事无补。1976年约瑟夫·维森鲍姆出版著作《计算机的力量与人类的推理》，书中表示人工智能的滥用可能损害人类生命的价值。

=== 感知器与联结主义遭到冷落 ===
感知器是神经网络的一种形式，由Frank Rosenblatt于1958年提出。与多数AI研究者一样，他对这一发明的潜力非常乐观，预言说“感知器最终将能够学习，作出决策和翻译语言”。整个六十年代里这一方向的研究工作都很活跃。
1969年Minsky和Papert出版了著作《感知器》，书中暗示感知器具有严重局限，而Frank Rosenblatt的预言过于夸张。这本书的影响是破坏性的：联结主义的研究因此停滞了十年。后来新一代研究者使这一领域获得重生，并使其成为人工智能中的重要部分；遗憾的是Rosenblatt没能看到这些，他在《感知器》问世后不久即因游船事故去世。

=== “简约派（the neats）”：逻辑，Prolog语言和专家系统 ===
早在1958年，John McCarthy就提出了名为“纳谏者（Advice Taker）”的一个程序构想，将逻辑学引入了AI研究界。1963年，J. Alan Robinson发现了在计算机上实现推理的简单方法：归结（resolution）与合一（unification）算法。然而，根据60年代末McCarthy和他的学生们的工作，对这一想法的直接实现具有极高的计算复杂度：即使是证明很简单的定理也需要天文数字的步骤。70年代Robert Kowalsky在Edinburgh大学的工作则更具成效：法国学者Alain Colmerauer和Phillipe Roussel在他的合作下开发出成功的逻辑编程语言Prolog。Prolog使用一组逻辑(与"规则"和"Production_system_(computer_science)"密切相关的"霍恩子句")，并允许进行可处理的计算。规则持续带来影响，为爱德华·费根鲍姆(Edward Feigenbaum)的专家系统以及艾伦·纽厄尔和赫伯特·西蒙的工作奠定基础，使其完成了Soar及认知统一理论。
Dreyfus等人针对逻辑方法的批评观点认为，人类在解决问题时并没有使用逻辑运算。心理学家Peter Wason，Eleanor Rosch，阿摩司·特沃斯基，Daniel Kahneman等人的实验证明了这一点。McCarthy则回应说，人类怎么思考是无关紧要的：真正想要的是解题机器，而不是模仿人类进行思考的机器。

=== “芜杂派（the scruffies）”：框架和脚本 ===
对McCarthy的做法持批评意见的还有他在MIT的同行们。马文·闵斯基，Seymour Papert和Roger Schank等试图让机器像人一样思考，使之能够解决“理解故事”和“目标识别”一类问题。为了使用“椅子”，“饭店”之类最基本的概念，他们需要让机器像人一样作出一些非逻辑的假设。不幸的是，这些不精确的概念难以用逻辑进行表达。Gerald Sussman注意到，“使用精确的语言描述本质上不精确的概念，并不能使它们变得精确起来”。Schank用“芜杂（scruffy）”一词描述他们这一“反逻辑”的方法，与McCarthy，Kowalski，Feigenbaum，Newell和Simon等人的“简约（neat）”方案相对。
在1975年的一篇开创性论文中，Minsky注意到与他共事的“芜杂派”研究者在使用同一类型的工具，即用一个框架囊括所有相关的常识性假设。例如，当我们使用“鸟”这一概念时，脑中会立即浮现出一系列相关事实，如会飞，吃虫子，等等。我们知道这些假设并不一定正确，使用这些事实的推理也未必符合逻辑，但是这一系列假设组成的结构正是我们所想和所说的一部分。他把这个结构称为“框架（frames）”。Schank使用了“框架”的一个变种，他称之为“脚本（scripts）”，基于这一想法他使程序能够回答关于一篇英语短文的提问。 多年之后的面向对象编程采纳了AI“框架”研究中的“继承（inheritance）”概念。

== 繁荣：1980 - 1987 ==
在80年代，一类名为“专家系统”的AI程序开始为全世界的公司所采纳，而“知识处理”成为了主流AI研究的焦点。日本政府在同一年代积极投资AI以促进其第五代计算机工程。80年代早期另一个令人振奋的事件是John Hopfield和David Rumelhart使联结主义重获新生。AI再一次获得了成功。

=== 专家系统获得赏识 ===
专家系统是一种程序，能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。最早的示例由Edward Feigenbaum和他的学生们开发。1965年起设计的Dendral能够根据分光计读数分辨混合物。1972年设计的MYCIN能够诊断血液传染病。它们展示了这一方法的威力。
专家系统仅限于一个很小的知识领域，从而避免了常识问题；其简单的设计又使它能够较为容易地编程实现或修改。总之，实践证明了这类程序的实用性。直到现在AI才开始变得实用起来。
1980年CMU为DEC（Digital Equipment Corporation，数字设备公司）设计了一个名为XCON的专家系统，这是一个巨大的成功。在1986年之前，它每年为公司省下四千万美元。全世界的公司都开始研发和应用专家系统，到1985年它们已在AI上投入十亿美元以上，大部分用于公司内设的AI部门。为之提供支持的产业应运而生，其中包括Symbolics，Lisp Machines等硬件公司和IntelliCorp，Aion等软件公司。

=== 知识革命 ===
专家系统的能力来自于它们存储的专业知识。这是70年代以来AI研究的一个新方向。  Pamela McCorduck在书中写道，“不情愿的AI研究者们开始怀疑，因为它违背了科学研究中对最简化的追求。智能可能需要建立在对分门别类的大量知识的多种处理方法之上。” “70年代的教训是智能行为与知识处理关系非常密切。有时还需要在特定任务领域非常细致的知识。” 知识库系统和知识工程成为了80年代AI研究的主要方向。
第一个试图解决常识问题的程序Cyc也在80年代出现，其方法是建立一个容纳一个普通人知道的所有常识的巨型数据库。发起和领导这一项目的Douglas Lenat认为别无捷径，让机器理解人类概念的唯一方法是一个一个地教会它们。这一工程几十年也没有完成。

=== 重获拨款：第五代工程 ===
1981年，日本经济产业省拨款八亿五千万美元支持第五代计算机项目。其目标是造出能够与人对话，翻译语言，解释图像，并且像人一样推理的机器。令“芜杂派”不满的是，他们选用Prolog作为该项目的主要编程语言。
其他国家纷纷作出响应。英国开始了耗资三亿五千万英镑的Alvey工程。美国一个企业协会组织了MCC（Microelectronics and Computer Technology Corporation，微电子与计算机技术集团），向AI和信息技术的大规模项目提供资助。 DARPA也行动起来，组织了战略计算促进会（Strategic Computing Initiative），其1988年向AI的投资是1984年的三倍。
一个四节点的Hopfield网络.

=== 联结主义的重生 ===
1982年，物理学家John Hopfield证明一种新型的神经网络（现被称为“Hopfield网络”）能够用一种全新的方式学习和处理信息。大约在同时（早于Paul Werbos），David Rumelhart推广了反向传播算法，一种神经网络训练方法。这些发现使1970年以来一直遭人遗弃的联结主义重获新生。
1986年由Rumelhart和心理学家James McClelland主编的两卷本论文集“分布式并行处理”问世，这一新领域从此得到了统一和促进。90年代神经网络获得了商业上的成功，它们被应用于光字符识别和语音识别软件。

== 第二次AI低谷：1987 - 1993 ==
80年代中商业机构对AI的追捧与冷落符合经济泡沫的经典模式，泡沫的破裂也在政府机构和投资者对AI的观察之中。尽管遇到各种批评，这一领域仍在不断前进。来自机器人学这一相关研究领域的Rodney Brooks和Hans Moravec提出了一种全新的人工智能方案。

=== 人工智慧的低谷 ===
“AI之冬”一词由经历过1974年经费削减的研究者们创造出来。他们注意到了对专家系统的狂热追捧，预计不久后人们将转向失望。事实被他们不幸言中：从80年代末到90年代初，AI遭遇了一系列财政问题。
变天的最早征兆是1987年AI硬件市场需求的突然下跌。Apple和IBM生产的台式机性能不断提升，到1987年时其性能已经超过了Symbolics和其他厂家生产的昂贵的Lisp机。老产品失去了存在的理由：一夜之间这个价值五亿美元的产业土崩瓦解。
XCON等最初大获成功的专家系统维护费用居高不下。它们难以升级，难以使用，脆弱（当输入异常时会出现莫名其妙的错误），成了以前已经暴露的各种各样的问题（例如资格问题（））的牺牲品。专家系统的实用性仅仅局限于某些特定情景。
到了80年代晚期，战略计算促进会大幅削减对AI的资助。DARPA的新任领导认为AI并非“下一个浪潮”，拨款将倾向于那些看起来更容易出成果的项目。
直到1991年，“第五代工程”并没有实现，事实上其中一些目标，比如“与人展开交谈”，直到2010年也没有实现。 与其他AI项目一样，期望比真正可能实现的要高得多。

=== 躯体的重要性：Nouvelle AI与嵌入式推理 ===
80年代后期，一些研究者根据机器人学的成就提出了一种全新的人工智能方案。 他们相信，为了获得真正的智能，机器必须具有躯体 - 它需要感知，移动，生存，与这个世界交互。他们认为这些感知运动技能对于常识推理等高层次技能是至关重要的，而抽象推理不过是人类最不重要，也最无趣的技能（参见Moravec悖论）。他们号召「自底向上」地创造智能，这一主张复兴了从60年代就沉寂下来的控制论。
另一位先驱是在理论神经科学上造诣深厚的David Marr，他于70年代来到MIT指导视觉研究组的工作。他排斥所有符号化方法（不论是McCarthy的逻辑学还是Minsky的框架），认为实现AI需要自底向上地理解视觉的物理机制，而符号处理应在此之后进行。
在发表于1990年的论文「大象不玩象棋（Elephants Don't Play Chess）」中，机器人研究者Rodney Brooks提出了「物理符号系统假设」，认为符号是可有可无的，因为「这个世界就是描述它自己最好的模型。它总是最新的。它总是包括了需要研究的所有细节。诀窍在于正确地，足够频繁地感知它。」 在80年代和90年代也有许多认知科学家反对基于符号处理的智能模型，认为身体是推理的必要条件，这一理论被称为「具身的心灵/理性/ 认知（embodied mind/reason/cognition）」论题。

== AI：1993 - 2011 ==
现已年过半百的AI终于实现了它最初的一些目标。它已被成功地用在技术产业中，不过有时是在幕后。这些成就有的归功于计算机性能的提升，有的则是在高尚的科学责任感驱使下对特定的课题不断追求而获得的。不过，至少在商业领域里AI的声誉已经不如往昔了。“实现人类水平的智能”这一最初的梦想曾在60年代令全世界的想象力为之着迷，其失败的原因至今仍众说纷纭。各种因素的合力将AI拆分为各自为战的几个子领域，有时候它们甚至会用新名词来掩饰“人工智能”这块被玷污的金字招牌。AI比以往的任何时候都更加谨慎，却也更加成功。

=== 里程碑和摩尔定律 ===
1997年5月11日，深蓝成为战胜国际象棋世界冠军卡斯帕罗夫的第一个计算机系统。2005年，Stanford开发的一台机器人在一条沙漠小径上成功地自动行驶了131英里，赢得了DARPA挑战大赛头奖。2009年，蓝脑计划声称已经成功地模拟了部分鼠脑。2011年，IBM 沃森参加《危险边缘》节目，在最后一集打败了人类选手。2016年3月，AlphaGo击败李世乭，成为第一个不让子而击败职业围棋棋士的电脑围棋程式。2017年5月，AlphaGo在中国乌镇围棋峰会的三局比赛中击败当时世界排名第一的中国棋手柯洁。
这些成就的取得并不是因为范式上的革命。它们仍然是工程技术的复杂应用，但是计算机性能已经今非昔比了。事实上，深蓝计算机比克里斯多福·斯特雷奇（Christopher Strachey）在1951年用来下棋的Ferranti Mark 1快一千万倍。这种剧烈增长可以用摩尔定律描述：计算速度和内存容量每两年翻一番。计算性能上的基础性障碍已被逐渐克服。

=== 智能代理 ===
90年代，被称为“智能代理”的新范式被广泛接受。尽管早期研究者提出了模块化的分治策略， 但是直到Judea Pearl，Alan Newell等人将一些概念从决策理论和经济学中引入AI之后现代智能代理范式才逐渐形成。当经济学中的“理性代理（rational agent）”与计算机科学中的“对象”或“模块”相结合，“智能代理”范式就完善了。
智能代理是一个系统，它感知周围环境，然后采取措施使成功的几率最大化。最简单的智能代理是解决特定问题的程序。已知的最复杂的智能代理是理性的，会思考的人类。智能代理范式将AI研究定义为“对智能代理的学习”。这是对早期一些定义的推广：它超越了研究人类智能的范畴，涵盖了对所有种类的智能的研究。
这一范式让研究者们通过学习孤立的问题找到可证的并且有用的解答。它为AI各领域乃至经济学，控制论等使用抽象代理概念的领域提供了描述问题和共享解答的一种通用语言。人们希望能找到一种完整的Agent_architecture（像Newell的Soar那样），允许研究者们应用交互的智能代理建立起通用的智能系统。

=== “简约派”的胜利 ===
越来越多的AI研究者们开始开发和使用复杂的数学工具。人们广泛地认识到，许多AI需要解决的问题已经成为数学，经济学和运筹学领域的研究课题。数学语言的共享不仅使AI可以与其他学科展开更高层次的合作，而且使研究结果更易于评估和证明。AI已成为一门更严格的科学分支。 Russell和Norvig（2003）将这些变化视为一场“革命”和“简约派的胜利”。
Judea Pearl发表于1988年的名著将概率论和决策理论引入AI。现已投入应用的新工具包括贝叶斯网络，隐马尔可夫模型，信息论，随机模型和经典优化理论。针对神经网络和进化算法等“计算智能”范式的精确数学描述也被发展出来。

=== 幕后的AI ===
AI研究者们开发的算法开始变为较大的系统的一部分。AI曾经解决了大量的难题，这些解决方案在产业界起到了重要作用。应用了AI技术的有数据挖掘，工业机器人，物流，语音识别，银行业软件，医疗诊断和Google搜索引擎等。
AI领域并未从这些成就之中获得多少益处。AI的许多伟大创新仅被看作计算机科学工具箱中的一件工具。Nick Bostrom解释说，“很多AI的前沿成就已被应用在一般的程序中，不过通常没有被称为AI。这是因为，一旦变得足够有用和普遍，它就不再被称为AI了。”
90年代的许多AI研究者故意用其他一些名字称呼他们的工作，例如信息学，知识系统，认知系统或计算智能。部分原因是他们认为他们的领域与AI存在根本的不同，不过新名字也有利于获取经费。至少在商业领域，导致AI之冬的那些未能兑现的承诺仍然困扰着AI研究，正如New York Times在2005年的一篇报道所说：“计算机科学家和软件工程师们避免使用人工智能一词，因为怕被认为是在说梦话。”

=== HAL 9000在哪里? ===
1968年亚瑟·克拉克和史丹利·库柏力克创作的《“2001太空漫游”》中设想2001年将会出现达到或超过人类智能的机器。他们创造的这一名为HAL-9000的角色是以科学事实为依据的：当时许多顶极AI研究者相信到2001年这样的机器会出现。
“那么问题是，为什么在2001年我们并未拥有HAL呢?” 马文·闵斯基问道。 Minsky认为，问题的答案是绝大多数研究者醉心于钻研神经网络和遗传算法之类商业应用，而忽略了常识推理等核心问题。另一方面，约翰·麦卡锡则归咎于资格问题（）。雷蒙德·库茨魏尔相信问题在于计算机性能，根据摩尔定律，他预测具有人类智能水平的机器将在2029年出现。杰夫·霍金认为神经网络研究忽略了人类大脑皮质的关键特性，而简单的模型只能用于解决简单的问题。还有许多别的解释，每一个都对应着一个正在进行的研究计划。

== 深度学习，大数据和人工智能：2011至今 ==
进入21世纪，得益于大数据和计算机技术的快速发展，许多先进的机器学习技术成功应用于经济社会中的许多问题。麦肯锡全球研究院在一份题为《大数据：创新、竞争和生产力的下一个前沿领域》的报告中估计，到2009年，美国经济所有行业中具有1000名以上员工的公司都至少平均拥有一个200兆兆字节的存储数据。
到2016年，AI相关产品、硬件、软件等的市场规模已经超过80亿美元，纽约时报评价道AI已经到达了一个热潮。大数据应用也开始逐渐渗透到其他领域，例如生态学模型训练、经济领域中的各种应用、医学研究中的疾病预测及新药研发等。深度学习（特别是深度卷积神经网络和循环网络）更是极大地推动了图像和视频处理、文本分析、语音识别等问题的研究进程。

=== 深度学习 ===
深度学习是机器学习的一个分支，它通过一个有着很多层处理单元的深层网络对数据中的高级抽象进行建模。根据全局逼近原理（Universal approximation theorem），对于神经网络而言，如果要拟合任意连续函数，深度性并不是必须的，即使一个单层的网络，只要拥有足够多的非线性激活单元，也可以达到拟合目的。但是，目前深度神经网络得到了更多的关注，这主要是源于其结构层次性，能够快速建模更加复杂的情况，同时避免浅层网络可能遭遇的诸多缺点。
然而，深度学习也有自身的缺点。以循环神经网络为例，一个最常见的问题是梯度消失问题（沿着时间序列反向传播过程中，梯度逐渐减小到0附近，造成学习停滞）。为了解决这些问题，很多针对性的模型被提出来，例如LSTM（长短期记忆网络，早在1997年就已经提出，最近随着RNN的大火，又重新进入大众视野）、GRU（门控循环神经单元）等等。
现在，最先进的神经网络结构在某些领域已经能够达到甚至超过人类平均准确率，例如在计算机视觉领域，特别是一些具体的任务上，比如MNIST数据集（一个手写数字识别数据集）、交通信号灯识别等。再如游戏领域，Google的deepmind团队研发的AlaphaGo，在问题搜索复杂度极高的围棋上，已经打遍天下无敌手。

==注释==

==参考文献==
*  oclc = 46890682 .
* Citation | first=Bruce G. | last=Buchanan | title=A (Very) Brief History of Artificial Intelligence | magazine=AI Magazine | date=Winter 2005 | year=2005 | pages=53−60 | url=http://www.aaai.org/AITopics/assets/PDF/AIMag26-04-016.pdf | accessdate=2007-08-30 | deadurl=yes | archiveurl=https://web.archive.org/web/20070926023314/http://www.aaai.org/AITopics/assets/PDF/AIMag26-04-016.pdf | archivedate=2007-09-26 .
* Citation | first = Rodney | last = Brooks | title = Elephants Don't Play Chess | journal = Robotics and Autonomous Systems | volume=6 | year =1990 | pages = 3−15 | author-link=Rodney Brooks | url=http://people.csail.mit.edu/brooks/papers/elephants.pdf | accessdate=2007-08-30 | doi = 10.1016/S0921-8890(05)80025-9.
*  newspaper = the Press, Christchurch, New Zealand .
* .
*  accessdate=8 October 2008.
*  location=Dordrecht .
* Citation | first = Brad | last = Darrach | date=20 November 1970  
magazine=Life Magazine | pages = 58−68 .
* Citation | first = J. | last = Doyle | year = 1983 | title = What is rational psychology? Toward a modern mental philosophy | magazine = AI Magazine | volume= 4 | issue =3 |pages = 50−53 .
* Citation | last=Dreyfus | first=Hubert | year =1965 | title = Alchemy and AI  
publisher = RAND Corporation Memo | author-link = Hubert Dreyfus .
* Citation | last=Dreyfus | first=Hubert | year =1972 | title = What Computers Can't Do  
oclc=5056816 .
* Citation | last=The Economist | date=7 June 2007 | magazine=The Economist | title=Are You Talking to Me? | url=http://www.economist.com/science/tq/displaystory.cfm?story_id=9249338 | accessdate=16 October 2008.
* Citation | first = Edward A. | last = Feigenbaum | first2=Pamela |last2=McCorduck |title = The Fifth Generation: Artificial Intelligence and Japan's Computer Challenge to the World | publisher = Michael Joseph | year = 1983| author-link = Edward Feigenbaum | isbn = 0-7181-2401-4 .
*  doi = 10.1177/0008125619864925 .
* Citation | last=Hawkins | first=Jeff | author-link=Jeff Hawkins | last2=Blakeslee | first2=Sandra | year=2004 | title=On Intelligence | publisher=Owl Books | location=New York, NY | isbn=0-8050-7853-3 | oclc=61273290 .
*  oclc=48871099 .
*  archivedate=2009-12-29 
* .
* .
*  accessdate= 30 August 2007.
* bibcode = 1982Sci...217.1237K .
* Citation | first = Ray | last = Kurzweil | title = The Singularity is Near | year = 2005 | publisher = Viking Press | author-link = Ray Kurzweil | isbn=0-14-303788-9 | oclc = 71826177 .
*  isbn = 0-226-46804-6.
*  oclc=19981533 .
*  isbn = 0-7864-0778-6 .
*  archivedate=2008年10月8日 
*  archivedate = 2007年8月26日 
*  archivedate=2008年9月30日 
*  oclc=52197627.
*  issue = 4
*  With notes upon the Memoir by the Translator
*  dead-url = yes 
*  oclc = 245755104 
*  oclc = 246584055
* publication-place= New York
* .
*  accessdate=2007-01-10
* .
* .
*  publication-place=San Mateo, California.
* .
*  isbn = 0-19-510270-3.
* date=July 1959.
* .
* .
*  publication-place = New York .
* .
* .
* .
* .
* Citation | first = Joseph | last = Weizenbaum  
title = Computer Power and Human Reason | publisher = W.H. Freeman & Company  
oclc = 10952283 .
.




自动标引（Automatic Indexing）包括关键词自动提取（又称自动抽词标引）与自动赋词标引两种类型。关键词自动提取是一种识别有意义且具有代表性片段或词汇的自动化技术。关键词自动提取在文本挖掘域被称为关键词抽取（Keyword Extraction），在计算语言学领域通常着眼于术语自动识别（Automatic Term Recognition），在信息检索领域，就是指自动标引。自动标引属于文本信息抽取的范畴。文本信息抽取是从文本数据中抽取人们关注的特定的信息。 

==作用==
由于关键词是表达文件主题意义的最小单位，因此大部分对非结构化文件的自动处理，如自动标引、自动文摘、自动分类、自动聚类、相关反馈、自动过滤、事件检测与跟踪、知识挖掘、信息可视化、概念检索、检索提示、关联知识分析、自动问答等，都必须先进行关键词提取的动作，再进行其他的处理。可以说，关键词提取是所有文件自动处理的基础与核心技术。目前大多文档都不具有关键词,同时手工标引费力费时且主观性较强, 因此关键词自动标引是一项值得研究的技术。 

==发展过程==
自动标引研究可以分为三个阶段：
从Luhn于1957年开始进行自动标引后开始，到目前为止，自动标引研究经历了50年的发展历程。一直到20世纪90年代初，关于关键词自动提取的研究一直就没有停止过。 20世纪90年代初到90年代末，自动标引研究渐渐冷却，原因主要包括：全文索引逐渐被人采用，并且基本上能满足用户需要；传统的自动标引方法的效率到了极限；网络兴起之初的冲击与信息需求环境的改变。20世纪90年代末一直到现在，关键词自动提取的研究逐渐升温，尤其是最近几年，关键词自动提取研究进行的如火如荼，产生该现象的主要原因为：全文索引的功能越来越难以满足实际需求，用户需要更加精确的结果；另外互联网的很多服务，例如自动摘要，文档分类与聚类，文本分析，主题检索等都要依赖于关键词自动提取的结果，只有这样才能有希望从根本上提高信息服务质量。 

==代表方法==
根据见诸于报道的自动标引研究情况，结合自动标引研究领域的影响程度和自动标引方法的创新程度，归纳出1957~2007年五十年时间里比较有代表性的自动标引方法。 
*1957年，Luhn开始自动标引研究，首次将计算机技术引入文献标引领域，开创了以词频为特征的统计标引方法，其理论基础是Zipf定律，该方法具有一定的客观性和合理性，并且简单易行，在自动标引中占有重要地位 
*1958年，Luhn提出基于绝对频率加权法的自动标引方法 ； P.B.Baxendale提出从论题句和介词短语中自动提取关键词 
*1959年，Edmundson与Oswald提出基于相对频率加权法的自动标引方法 
*1960年，Maron & Kuhns提出基于相关概率的赋词标引方法 
*1969年，H.P.Edmundson提出了一些新的加权方法，如提示词(预示词)加权法、题名加权法、位置加权法，并探讨了不同加权法的最优组合问题 
*1970年，Lois L. Earl利用句法分析等语言学方法与词频统计方法相结合的方法来提取关键词 
*1973年，Salton等提出基于词区分值的自动标引方法 
*1975年，Salton等将VSM模型用于自动标引中 
*1983年，Dillon等提出一种基于概念的自动标引方法，研制了FASIT系统 
*1985年，Devadason提出基于深层结构标引方法 
*1990年，Deerwester & Dumais等提出潜在语义分析标引法 
*1993年，Silva & Milidiu提出基于相信函数模型的赋词标引方法 
*1995年，Cohen提出N-Gram分析法的自动标引方法 
*1997年，简立峰提出基于PAT树的关键词提取方法 
*1999年，Frank等人提出基于朴素贝叶斯(Naive Bayes，NB)的关键词提取方法 ；Turney 利用遗传算法和C4.5决策树算法等机器学习方法进行关键短语提取的研究 
*2001年，Anjewierden & Kabel提出基于本体的自动标引方法 
*2003年，Tomokiyo & Hurst提出了基于语言模型的关键词提取方法；Hulth利用Bagging算法进行了基于集成学习的关键词抽取 
*2004年，李素建提出基于最大熵模型的关键词提取方法
*2006年，张阔提出基于支持向量机自动标引模型 
*2007年，Ercan, G. & Cicekli, I提出基于词汇链的自动标引方法 

==典型应用==
# 汉语自动标引加权方法试验研究  	1994年 史继红、赖茂生	北京大学信息管理系           
# 自动标引“匹配标引法”原理  	1994年 袁庆华	         总后档案馆	
# 语义矢量空间模式 (SVSM)及其试验评价——自然语言处理与文献自动标引   1996年 Geoffrey Z. Liu	美国加利福尼亚州圣何塞州立大学图书情报学院	  
# 文书档案主题自动标引系统的设计与实践 1996年	兰生柱、尹秀兰等	解放军档案馆	           
# 统计方法结合受限自然语言理解技术用模糊方法抽取关键词 1998年	何新贵、彭甫阳	北京系统工程研究所	  
# 主题转译标引技术 1998年	陈光华	       台湾大学图书馆和信息科学系	  
# 对规范文本篇章结构 1998年	单永明	         山西大学计算机系	           
# 科技文献主题词的自动标引法 1998年	石国华	         杭州大学	                    
# 中文科技文献题内自动抽词标引系统 1998年	邵艳秋、刘挺等	黑龙江交通高等专科学校计算中心、哈尔滨工业大学计算机系	  
# 针对生物学文献 1999年	王永成、韩客松等	上海交通大学	           
# 单汉字标引技术 1999年	胡盈盈	         南京大学	                    
# 基于《中国分类主题词表》的WWW科技信息资源自动标引设计方案  1999年	肖明	      北京师范大学信息技术与管理学系  
# 网络环境下档案主题自动标引的实现方法 1999年	熊志云	       湖北大学人文学院档案系         
# 公安文献全文著录、机助标引及检索系统（PWDBC） 2000年		       江苏公安专科学校	           
# 计算机模糊检索在图书自动标引中的应用 2000年	许玲	       曲阜师范大学图书馆	          
# 基于中国档案主题词表的自动标引控制研究 2002年	王兰成等	     南京政治学院上海分院信息管理系  
# 基于多词表的自动标引技术研究——新华社新闻稿自动标引的实验 2002年	查贵庭、侯汉清	南京农业大学信息管理系	 
# 字面相似聚类法辅助构造词族表、分面类表和自动标引 2002年	张琪玉	    南京政治学院上海分院信息管理系	  
# 网页自动标引方案的优选及标引性能的测评 2002年	仲云云、侯汉清等	南京农业大学信息管理系

==参考文献==

==外部链接==
# http://sites.google.com/site/zhangczhomepage/keyword-extraction
category:图书资讯科学
category:计算语言学
category:数据挖掘




在机器学习中，单纯贝氏分类器是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单概率分类器。
单纯贝氏自20世纪50年代已广泛研究。在20世纪60年代初就以另外一个名称引入到文本信息检索界中，488 并仍然是文本分类的一种热门（基准）方法，文本分类是以词频为特征判断文件所属类别或其他（如垃圾邮件、合法性、体育或政治等等）的问题。通过适当的预处理，它可以与这个领域更先进的方法（包括支持向量机）相竞争。 它在自动医疗诊断中也有应用。
单纯贝氏分类器是高度可扩展的，因此需要数量与学习问题中的变量（特征/预测器）成线性关系的参数。最大似然训练可以通过评估一个封闭形式的表达式来完成，718 只需花费线性时间，而不需要其他很多类型的分类器所使用的费时的迭代逼近。
在统计学和计算机科学文献中，单纯贝氏模型有各种名称，包括简单贝叶斯和独立贝叶斯。 所有这些名称都参考了贝叶斯定理在该分类器的决策规则中的使用，但单纯贝氏不（一定）用到贝叶斯方法； 《Russell和Norvig》提到“『单纯贝氏』有时被称为贝叶斯分类器，这个马虎的使用促使真正的贝叶斯论者称之为傻瓜贝叶斯模型。”482

==简介==
单纯贝氏是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有单纯贝氏分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而单纯贝氏分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。
对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，单纯贝氏模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，单纯贝氏模型也能奏效。
尽管是带着这些朴素思想和过于简单化的假设，但单纯贝氏分类器在很多复杂的现实情形中仍能够取得相当好的效果。2004年，一篇分析贝叶斯分类器问题的文章揭示了单纯贝氏分类器取得看上去不可思议的分类效果的若干理论上的原因。 尽管如此，2006年有一篇文章详细比较了各种分类方法，发现更新的方法（如决策树和随机森林）的性能超过了贝叶斯分类器。
单纯贝氏分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量的方法，而不需要确定整个协方差矩阵。

== 单纯贝氏概率模型 ==
理论上，概率模型分类器是一个条件概率模型。

独立的类别变量有若干类别，条件依赖于若干特征变量
,,...,。但问题在于如果特征数量较大或者每个特征能取大量值时，基于概率模型列出概率表变得不现实。所以我们修改这个模型使之变得可行。
贝叶斯定理有以下式子：

用朴素的语言可以表达为：

实际中，我们只关心分式中的分子部分，因为分母不依赖于而且特征的值是给定的，于是分母可以认为是一个常数。这样分子就等价于联合分布模型。

重复使用链式法则，可将该式写成条件概率的形式，如下所示：

:
:
:
:
:
现在“朴素”的条件独立假设开始发挥作用:假设每个特征对于其他特征,是条件独立的。这就意味着

对于，所以联合分布模型可以表达为

这意味着上述假设下，类变量的条件分布可以表达为：

其中(证据因子)是一个只依赖与等的缩放因子，当特征变量的值已知时是一个常数。
由于分解成所谓的类先验概率和独立概率分布，上述概率模型的可掌控性得到很大的提高。如果这是一个分类问题，且每个可以表达为个参数，于是相应的单纯贝氏模型有(k − 1) + n r k个参数。实际应用中，通常取（二分类问题）， (伯努利分布作为特征），因此模型的参数个数为，其中是二值分类特征的个数。

===从概率模型中构造分类器===
讨论至此为止我们导出了独立分布特征模型，也就是单纯贝氏概率模型。单纯贝氏分类器包括了这种模型和相应的决策规则。一个普通的规则就是选出最有可能的那个：这就是大家熟知的最大后验概率（MAP）决策准则。相应的分类器便是如下定义的公式：


== 参数估计 ==
所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计。类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。 

=== 高斯单纯贝氏 ===
如果要处理的是连续数据一种通常的假设是这些连续数值为高斯分布。
例如，假设训练集中有一个连续属性，。我们首先对数据根据类别分类，然后计算每个类别中的均值和方差。令 表示为在c类上的均值，令为 在c类上的方差。在给定类中某个值的概率，，可以通过将表示为均值为方差为正态分布计算出来。如下，
处理连续数值问题的另一种常用的技术是通过离散化连续数值的方法。通常，当训练样本数量较少或者是精确的分布已知时，通过概率分布的方法是一种更好的选择。在大量样本的情形下离散化的方法表现更优，因为大量的样本可以学习到数据的分布。由于单纯贝氏是一种典型的用到大量样本的方法（越大计算量的模型可以产生越高的分类精确度），所以单纯贝氏方法都用到离散化方法，而不是概率分布估计的方法。

== 样本修正 ==
如果一个给定的类和特征值在训练集中没有一起出现过，那么基于频率的估计下该概率将为0。这将是一个问题。因为与其他概率相乘时将会把其他概率的信息统统去除。所以常常要求要对每个小类样本的概率估计进行修正，以保证不会出现有为0的概率出现。

== 讨论 ==
尽管实际上独立假设常常是不准确的，但单纯贝氏分类器的若干特性让其在实践中能够取得令人惊奇的效果。特别地，各类条件特征之间的解耦意味着每个特征的分布都可以独立地被当做一维分布来估计。这样减轻了由于维数灾带来的阻碍,当样本的特征个数增加时就不需要使样本规模呈指数增长。然而单纯贝氏在大多数情况下不能对类概率做出非常准确的估计，但在许多应用中这一点并不要求。例如，单纯贝氏分类器中，依据最大后验概率决策规则只要正确类的后验概率比其他类要高就可以得到正确的分类。所以不管概率估计轻度的甚至是严重的不精确都不影响正确的分类结果。在这种方式下，分类器可以有足够的鲁棒性去忽略单纯贝氏概率模型上存在的缺陷。

==实例==

===性别分类===
问题描述:通过一些测量的特征，包括身高、体重、脚的尺寸，判定一个人是男性还是女性。

====训练====
训练数据如下：
假设训练集样本的特征满足高斯分布，得到下表：
我们认为两种类别是等概率的，也就是P(male)= P(female) = 0.5。在没有做辨识的情况下就做这样的假设并不是一个好的点子。但我们通过数据集中两类样本出现的频率来确定P(C)，我们得到的结果也是一样的。

====测试====
以下给出一个待分类是男性还是女性的样本。
我们希望得到的是男性还是女性哪类的后验概率大。男性的后验概率通过下面式子来求取

女性的后验概率通过下面式子来求取

证据因子（通常是常数）用来对各类的后验概率之和进行归一化.

证据因子是一个常数（在正态分布中通常是正数），所以可以忽略。接下来我们来判定这样样本的性别。

,其中，是训练集样本的正态分布参数. 注意，这里的值大于1也是允许的 – 这里是概率密度而不是概率，因为身高是一个连续的变量.








由于女性后验概率的分子比较大，所以我们预计这个样本是女性。

===文本分类===
这是一个用单纯贝氏分类做的一个文本分类问题的例子。考虑一个基于内容的文本分类问题，例如判断邮件是否为垃圾邮件。想像文本可以分成若干的类别，首先文本可以被一些单词集标注，而这个单词集是独立分布的，在给定的C类文本中第i个单词出现的概率可以表示为：

（通过这种处理，我们进一步简化了工作，假设每个单词是在文中是随机分布的-也就是单词不依赖于文本的长度，与其他词出现在文中的位置，或者其他文本内容。）
所以，对于一个给定类别C，文本D包含所有单词的概率是:

我们要回答的问题是「文档D属于类C的概率是多少？」换而言之是多少？
现在定义


通过贝叶斯定理将上述概率处理成似然度的形式

假设现在只有两个相互独立的类别，S和¬S（垃圾邮件和非垃圾邮件），这里每个元素（邮件）要么是垃圾邮件，要么就不是。


用上述贝叶斯的结果，可以写成


两者相除:

整理得:

这样概率比p(S | D) / p(¬S | D)可以表达为似然比。实际的概率p(S | D)可以很容易通过log (p(S | D) / p(¬S | D))计算出来，基于p(S | D) + p(¬S | D) = 1。
结合上面所讨论的概率比，可以得到：

(这种对数似然比的技术在统计中是一种常用的技术。在这种两个独立的分类情况下（如这个垃圾邮件的例子），把对数似然比转化为S曲线的形式)。
最后文本可以分类，当或者时判定为垃圾邮件，否则为正常邮件。

==参见==
* AODE
* 贝叶斯垃圾邮件过滤
* 贝叶斯网络
* 随机森林
* 线性分类器
* 提升方法
* 模糊逻辑
* 逻辑回归
* Class membership probabilities
* 神经网络
* 预测分析
* 感知机
* 支持向量机
* 贝叶斯定理
* 有监督学习
* 分类器
* 最大似然估计
* 贝叶斯概率
* boosted trees

==参考文献==

==延伸阅读==
* cite journal |last1=Domingos |first1=Pedro |first2=Michael |last2=Pazzani |year=1997 |title=On the optimality of the simple Bayesian classifier under zero-one loss |journal=Machine Learning |volume=29 |pages=103–137 |url=http://citeseer.ist.psu.edu/domingos97optimality.html
* doi=10.1007/s10994-005-4258-6 
* url=http://eprints.fri.uni-lj.si/154/01/PKDD_camera_mozina.pdf
* cite journal |last1=Maron |first1=M. E. |year=1961 |title=Automatic Indexing: An Experimental Inquiry |journal=JACM |volume=8 |issue=3 |pages=404–417 |doi=10.1145/321075.321084
* pages=8-30

==外部链接==
*  Book Chapter: Naive Bayes text classification, Introduction to Information Retrieval
*  Naive Bayes for Text Classification with Unbalanced Classes
*  Benchmark results of Naive Bayes implementations
*  Hierarchical Naive Bayes Classifiers for uncertain data (an extension of the Naive Bayes classifier).
软件
* Naive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout,  Mallet, NLTK, Orange, scikit-learn and Weka.
* IMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier.
*  Winnow content recommendation Open source Naive Bayes text classifier works with very small training and unbalanced training sets. High performance, C, any Unix.
* An interactive Microsoft Excel spreadsheet  Naive Bayes implementation using VBA (requires enabled macros) with viewable source code.
*  jBNC - Bayesian Network Classifier Toolbox
*  Statistical Pattern Recognition Toolbox for Matlab.
*  ifile - the first freely available (Naive) Bayesian mail/spam filter
*  NClassifier - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J.
*  Classifier4J - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier.




多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为反向传播算法的监督学习方法常被用来训练MLP。  MLP是感知器的推广，克服了感知器不能对线性不可分数据进行识别的弱点。

== 理论 ==

=== 激活函数 ===
若每个神经元的激活函数都是线性函数，那么，任意层数的MLP都可被约简成一个等价的单层感知器。
实际上，MLP本身可以使用任何形式的激活函数，譬如阶梯函数逻辑Sigmoid函数，但为了使用反向传播算法进行有效学习，激活函数必须限制为可微函数。由于具有良好可微性，很多S函数，尤其是双曲正切函数（Hyperbolic tangent）及逻辑函数，被采用为激活函数。
在深度学习的最新发展中，线性整流(ReLU)更频繁地被用来克服与S函数相关的数值问题。
两个历史上常见的激活函数都是 S函数，形式是
和 。
第一个是个双曲正切函数，定义域为 -1 到 1；第二个是个逻辑函数，形状很相似但是定义域为 0 到 1。令 yi 为第 i 个节点（神经元）的输出，而 vi 是输入连接的加权和。也有其他的激活函数，例如线性整流函数，径向基函数（用于径向基函数网络，另一种监督神经网络模型）。

=== 层 ===
MLP由三层或更多层非线性激活节点组成(一个输入层和一个具有一个或多个隐藏层的输出层)。由于多层互连是完全连接的，所以一层中的每个节点都以一定的权重 wij 连接到下一层的每个节点。

=== 学习 ===
MLP 在感知器中进行学习，通过每次处理数据后改变连接权重，降低输出与预测结果的误差量。这是有监督学习的一个例子，通过反向传播来实现，反向传播是线性感知器中最小均方算法的推广。
我们可以将输出节点 j 的第 n 个数据点的误差表示为 ，其中 d 是目标值，y 是由感知器预测的值。调整节点权重的方式是，尝试通过修正节点权重最小化输出的整体误差
 .
使用梯度下降，每个权重的修正量为
 
其中 yi 是前一个神经元的输出，η是学习率。η需要精心挑选，保证权重可以快速收敛而不发生震荡。
式中的导数取决于局部场 vj。场是变化的。很容易证明输出节点的导数可以简化为
 
其中  是激活函数的导数。是不变的。对于隐藏节点的权重变化，分析更加困难，但是可以看出相关的导数是
 .
代表输出层的第k个节点的权重变化会影响这个导数。因此，为了改变隐藏层权重，输出层权重根据激活函数的导数而改变，因此该算法代表激活函数的反向传播。

== 术语 ==
术语“多层感知器”不是指具有多层的单感知器，每一层由多个感知器组成。另一种说法是是“多层感知器网络”。此外，MLP的“感知器”不是最严格意义上的感知器。真正的感知器在形式上是人工神经元的一个特例，它使用一个阈值激活函数，如阶跃函数。MLP感知器可以使用任意激活函数。一个真正的感知器执行二进制分类(或者这个或者那个)，一个MLP神经元可以自由地执行分类或者回归，这取决于它的激活函数。
后来应用术语“多层感知器”时，没有考虑节点/层的性质，节点/层可以由任意定义的人工神经元组成，而不是具体的感知器。这种解释避免了将“感知器”的定义放宽到一般意义上的人工神经元。

==应用==
常被MLP用来进行学习的反向传播算法，在模式识别的领域中算是标准监督学习算法，并在计算神经学及并行分布式处理领域中，持续成为被研究的课题。MLP已被证明是一种通用的函数近似方法，可以被用来拟合复杂的函数，或解决分类问题。
MLP在80年代的时候曾是相当流行的机器学习方法，拥有广泛的应用场景，譬如语音识别、图像识别、机器翻译等等，但自90年代以来，MLP遇到来自更为简单的支持向量机的强劲竞争。近来，由于深度学习的成功，MLP又重新得到了关注。

== 文献 ==




反向传播（Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。
反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的delta rule的推广，可以用链式法则对每层迭代计算梯度。反向传播要求artificial neuron（或“节点”）的激励函数可微。

==动机==
任何监督式学习算法的目标是找到一个能把一组输入最好地映射到其正确的输出的函数。例如一个简单的分类任务，其中输入是动物的图像，正确的输出将是动物的名称。一些输入和输出模式可以很容易地通过单层神经网络（如感知器）学习。但是这些单层的感知机只能学习一些比较简单的模式，例如那些非linearly separable模式。例如，人可以通过识别动物的图像的某些特征进行分类，例如肢的数目，皮肤的纹理（无论是毛皮，羽毛，鳞片等），该动物的体型，以及种种其他特征。但是，单层神经网络必须仅仅使用图像中的像素的强度来学习一个输出一个标签函数。因为它被限制为仅具有一个层，所以没有办法从输入中学习到任何抽象特征。多层的网络克服了这一限制，因为它可以创建内部表示，并在每一层学习不同的特征。 第一层可能负责从图像的单个像素的输入学习线条的走向。第二层可能就会结合第一层所学并学习识别简单形状（如圆形）。每升高一层就学习越来越多的抽象特征，如上文提到的用来图像分类。每一层都是从它下方的层中找到模式，就是这种能力创建了独立于为多层网络提供能量的外界输入的内部表达形式。
反向传播算法的发展的目标和动机是找到一种训练的多层神经网络的方法，于是它可以学习合适的内部表达来让它学习任意的输入到输出的映射。

==概括==
反向传播算法（BP 算法）主要由两个阶段组成：激励传播与权重更新。

===第1阶段：激励传播===
每次迭代中的传播环节包含两步：
# （前向传播阶段）将训练输入送入网络以获得激励响应；
# （反向传播阶段）将激励响应同训练输入对应的目标输出求差，从而获得输出层和隐藏层的响应误差。

===第2阶段：权重更新===
对于每个突触上的权重，按照以下步骤进行更新：
# 将输入激励和响应误差相乘，从而获得权重的梯度；
# 将这个梯度乘上一个比例并取反后加到权重上。
这个比例（百分比）将会影响到训练过程的速度和效果，因此成为「训练因子」。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。
第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。

==算法==
三层网络算法（只有一个隐藏层）：
初始化网络权值（通常是小的随机值）
do
forEach 训练样本 ex
prediction = neural-net-output(network, ex)  // 正向传递
actual = teacher-output(ex)
计算输出单元的误差 (prediction - actual)
计算   对于所有隐藏层到输出层的权值                           // 反向传递
计算   对于所有输入层到隐藏层的权值                           // 继续反向传递
更新网络权值 // 输入层不会被误差估计改变
until 所有样本正确分类或满足其他停止标准
return 该网络
这个算法的名称意味着误差会从输出结点反向传播到输入结点。严格地讲，反向传播算法对网络的可修改权值计算了网络误差的梯度。 这个梯度会在简单stochastic gradient descent中经常用来求最小化误差的权重。通常“反向传播”这个词使用更一般的含义，用来指涵盖了计算梯度以及在随机梯度下降法中使用的整个过程。在适用反向传播算法的网络中，它通常可以快速收敛到令人满意的极小值。
BP网络都是多层感知机（通常都会有一个输入层、一个隐藏层及一个输出层）。为了使隐藏层能够适合所有有用的函数，多层网络必须具有用于多个层的非线性激活函数：仅用线性激活函数的多层网络会与相当于单层线性网络。常用的非线性激活函数有逻辑函数、柔性最大函数和高斯函数。
用反向传播算法求梯度已被重新发现多次，是更加一般的自动微分技术在反向积累模式的特例。
它也与Gauss–Newton algorithm密切相关，也是继续研究neural backpropagation的一部分。

==直观理解==

=== 学习作为一个优化问题 ===
在给出反向传播算法的数学推导之前，培养关于神经元的真实输出与特定的训练情况的正确输出间的直观感受是很有帮助的。考虑一个有两个输入单元、一个输出单元、没有隐藏单元的简单神经网络。每个神经元都使用输入的加权和作为线性输出。具有2个输入单元和1个输出单元的一个简单的神经网络
最初在训练之前，会随机分配权重。之后神经元根据训练实例进行学习，在此情况下包含元组 (, , ) 的集合，其中  与  是网络的输入， 为正确输出（在给定相同的输入时网络最终应当产生的输出）。网络在给定  和  时，会计算一个输出 ，很可能与  不同（因为权重最初是随机的）。衡量期望输出  与实际输出  之间的差异的一个常见方法是采用平方误差测度：
,
其中  为差异或误差。
举例来讲，考虑单一训练实例的网络：，输入  与  均为1，正确输出  为 0。现在若将实际输出  画在x轴，误差  画在  轴，得出的是一条抛物线。抛物线的极小值对应输出 ，最小化了误差 。对于单一训练实例，极小值还会接触到  轴，这意味着误差为零，网络可以产生与期望输出  完全匹配的输出 。因此，把输入映射到输出的问题就化为了一个找到一个能产生最小误差的函数的最佳化问题。单一训练实例的线性神经元的误差曲面。
然而，一个神经元的输出取决于其所有输入的加权总和：
,
其中  和  是从输入单元到输出单元相连的权重。因此，误差取决于输入到该神经元的权重，也是网络要学习最终需要改变的。若每个权重都画在一个水平的轴上，而误差画在垂直轴上，得出的就是一个抛物面（若一个神经元有  个权重，则误差曲面的维度就会是 ，因而就是二维抛物线的  维等价）。
两个输入误差的神经元的误差曲面
反向传播算法的目的是找到一组能最大限度地减小误差的权重。寻找抛物线或任意维度中的任何函数的极大值的方法有若干种。其中一种方法是通过求解方程组，但这依赖于网络是一个线性系统，而目标也需要可以训练多层非线性网络（因为多层线性网络与单层网络等价）。在反向传播中使用的方法是梯度下降法。

=== 运用类比理解梯度下降法 ===
梯度下降法背后的直观感受可以用假设情境进行说明。一个被卡在山上的人正在试图下山（即试图找到极小值）。大雾使得能见度非常低。因此，下山的道路是看不见的，所以他必须利用局部信息来找到极小值。他可以使用梯度下降法，该方法涉及到察看在他当前位置山的陡峭程度，然后沿着负陡度（即下坡）最大的方向前进。如果他要找到山顶（即极大值）的话，他需要沿着正陡度（即上坡）最大的方向前进。使用此方法，他会最终找到下山的路。不过，要假设山的陡度不能通过简单地观察得到，而需要复杂的工具测量，而这个工具此人恰好有。需要相当长的一段时间用仪器测量山的陡峭度，因此如果他想在日落之前下山，就需要最小化仪器的使用率。问题就在于怎样选取他测量山的陡峭度的频率才不致偏离路线。
在这个类比中，此人代表反向传播算法，而下山路径表示能使误差最小化的权重集合。山的陡度表示误差曲面在该点的斜率。他要前行的方向对应于误差曲面在该点的梯度。用来测量陡峭度的工具是微分（误差曲面的斜率可以通过对平方误差函数在该点求导数计算出来）。他在两次测量之间前行的距离（与测量频率成正比）是算法的学习速率。参见限制一节中对此类型“爬山”算法的限制的讨论。

==推导==
由于反向传播使用梯度下降法，需要计算平方误差函数对网络权重的导数。假设对于一个输出神经元， 平方误差函数为：
 ，
其中
 为平方误差，
 为训练样本的目标输出，
 为输出神经元的实际输出。
加入系数  是为了抵消微分出来的指数。之后，该表达式会乘以一个任意的学习速率，因此在这里乘上一个常系数是没有关系的。
对每个神经元 ，它的输出  定义为
.
通向一个神经元的输入  是之前神经元的输出  的加权和。若该神经元处于输入层后的第一层，输入层的输出  就是网络的输入 。该神经元的输入数量是 。变量  表示神经元  与  之间的权重。
激活函数  一般是非线性可微函数。常用作激活函数的是逻辑函数：
 
其导数的形式很好：
 

===求误差的导数===
计算误差对权重  的偏导数是两次使用链式法则得到的：
 
在右边的最后一项中，只有加权和  取决于 ，因此
 .
神经元  的输出对其输入的导数就是激活函数的偏导数（这里假定使用逻辑函数）：
 
这就是为什么反向传播需要的激活函数是可微的。
如果神经元在输出层中，因为此时  以及
 
所以第一项可以直接算出。
但如果  是网络中任一内层，求  关于  的导数就不太简单了。
考虑  为接受来自神经元  的输入的所有神经元  的输入的函数，
 
并关于  取全微分，可以得到该导数的一个递归表达式：
 
因此，若已知所有关于下一层（更接近输出神经元的一层）的输出  的导数，则可以计算  的导数。
把它们放在一起：

其中

要使用梯度下降法更新 ，必须选择一个学习速率 。要加在原本的权重上的权重的变化，等于学习速率与梯度的乘积，乘以 ：
 
之所以要乘以  是因为要更新误差函数极小值而不是极大值的方向。
对于单层网络，这个表达式变为Delta Rule。
要想更好地理解反向传播是如何起作用的，下面是一个例子来说明它： The Back Propagation Algorithm，12页。

==学习模式==
有可供选择的三种学习模式(Mode)：在线，批量和随机。在在线和随机学习，每次传播后被立即做一个权重的更新。在批量学习模式，权重更新前有许多传播发生。在线学习被用于提供新的型态(pattern)的连续流的动态环境。随机学习和批量学习都使用静态型态(pattern)的一个训练集合。随机学习以一个随机的顺序经过数据集合，以减少陷入局部极小值的机会。随机学习也比批量学习更快，因为权重在每次传播后被立即更新。然而批量学习将产生一个更加稳定下降到一个局部最小值，因为每次更新都是基于所有型态被进行的。

== 限制 ==
* 结果可能会收敛到极值。如果只有一个极小值，梯度下降的“爬山”策略一定可以起作用。然而，往往是误差曲面有许多局部最小值和最大值。如果梯度下降的起始点恰好介于局部最大值和局部最小值之间，则沿着梯度下降最大的方向会到达局部最小值。梯度下降法可以找到局部最小值，而不是全局最小值。
* 从反向传播学习获得的收敛很慢。
* 在反向传播学习的收敛性不能保证。
** 然而，收敛到全局最小值据说使用自适应终止条件得到保证。
* 反向传播学习不需要输入向量的标准化（normalization）；然而，标准化可提高性能。

== 历史 ==
Vapnik引用（Bryson, A.E.; W.F. Denham; S.E. Dreyfus. Optimal programming problems with inequality constraints. I: Necessary conditions for extremal solutions. AIAA J. 1, 11 (1963) 2544-2550）在他的书《支持向量机》中首次发表反向传播算法。在1969年Arthur E. Bryson和何毓琦将其描述为多级动态系统优化方法。  直到1974年以后在神经网络的背景下应用，并由Paul Werbos、David E. Rumelhart、杰弗里·辛顿和Ronald J. Williams的著作，它才获得认可，并引发了一场人工神经网络的研究领域的“文艺复兴”。在21世纪初人们对其失去兴趣，但在2010年后又拥有了兴趣，如今可以通过GPU等大型现代运算器件用于训练更大的网络。例如在2013年，顶级语音识别器现在使用反向传播算法训练神经网络。

==注释==

==参见==
* 人工神经网络
* 生物神经网络
* Catastrophic interference
* 表征学习
* AdaBoost
* 过适

==参考文献==

== 外部连结 ==
* A Gentle Introduction to Backpropagation - An intuitive tutorial by Shashi Sathyanarayana The article contains pseudocode ("Training Wheels for Training Neural Networks") for implementing the algorithm.
* Neural Network Back-Propagation for Programmers (a tutorial)
* Backpropagation for mathematicians
* Chapter 7  The backpropagation algorithm of  Neural Networks - A Systematic Introduction by Raúl Rojas (ISBN 978-3540605058)
* Implementation of BackPropagation in C++
* Implementation of BackPropagation in C#
* Implementation of BackPropagation in Java
* Another Implementation of BackPropagation in Java
* Implementation of BackPropagation in Ruby
* Implementation of BackPropagation in Python
* Implementation of BackPropagation in PHP
* Quick explanation of the backpropagation algorithm
* Graphical explanation of the backpropagation algorithm
* Concise explanation of the backpropagation algorithm using math notation by Anand Venkataraman
* Backpropagation neural network tutorial at the Wikiversity




深度学习（deep learning）是机器学习的分支，是一种以人工神经网路为架构，对资料进行表征学习的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。
表征学习的目标是寻求更好的表示方法并建立更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地建立在类似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉动神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。
至今已有数种深度学习框架，如深度神经网络、卷积神经网络和Deep belief network和循环神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并取得了极好的效果。
另外，「深度学习」已成为时髦术语，或者说是人工神经网络的品牌重塑。

== 简介 ==
深度学习框架，尤其是基于人工神经网络的框架可以追溯到1980年福岛邦彦提出的新认知机，而人工神经网络的历史更为久远。1989年，扬·勒丘恩（Yann LeCun）等人开始将1974年提出的标准反向传播算法应用于深度神经网络，这一网络被用于手写邮政编码识别。尽管算法可以成功执行，但计算代价非常巨大，神经网路的训练时间达到了3天，因而无法投入实际使用。许多因素导致了这一缓慢的训练过程，其中一种是由于尔根·施密德胡伯的学生赛普·霍克赖特于1991年提出的梯度消失问题。
最早的进行一般自然杂乱图像中自然物体识别的深度学习网络是翁巨扬（Juyang Weng）等在1991和1992发表的生长网（Cresceptron）。它也是第一个提出了后来很多实验广泛采用的一个方法：现在称为最大汇集（max-pooling)以用于处理大物体的变形等问题。生长网不仅直接从杂乱自然场景中学习老师指定的一般物体，还用网络反向分析的方法把图像内被识别了的物体从背景图像中分割出来。
2007年前后，杰弗里·辛顿和鲁斯兰·萨拉赫丁诺夫（Ruslan Salakhutdinov）提出了一种在前馈神经网络中进行有效训练的算法。这一算法将网络中的每一层视为无监督的受限玻尔兹曼机，再使用有监督的反向传播算法进行调优。在此之前的1992年，在更为普遍的情形下，施密德胡伯也曾在循环神经网络上提出一种类似的训练方法，并在实验中证明这一训练方法能够有效提高有监督学习的执行速度.
自深度学习出现以来，它已成为很多领域，尤其是在计算机视觉和语音识别中，成为各种领先系统的一部分。在通用的用于检验的数据集，例如语音识别中的TIMIT和图像识别中的ImageNet, Cifar10上的实验证明，深度学习能够提高识别的精度。与此同时，神经网络也受到了其他更加简单归类模型的挑战，支持向量机等模型在20世纪90年代到21世纪初成为过流行的机器学习算法。
硬件的进步也是深度学习重新获得关注的重要因素。高性能图形处理器的出现极大地提高了数值和矩阵运算的速度，使得机器学习算法的运行时间得到了显著的缩短。
由于脑科学方面的大量研究已表明人脑网络不是一个级联的结构，深度学习网络在2001年后正逐渐被更有潜力的基于脑模型的网络所替代。

== 基本概念 ==
深度学习的基础是机器学习中的分散表示（distributed representation）。分散表示假定观测值是由不同因子相互作用生成。在此基础上，深度学习进一步假定这一相互作用的过程可分为多个层次，代表对观测值的多层抽象。不同的层数和层的规模可用于不同程度的抽象。
深度学习运用了这分层次抽象的思想，更高层次的概念从低层次的概念学习得到。这一分层结构常常使用贪婪算法逐层构建而成，并从中选取有助于机器学习的更有效的特征.
不少深度学习算法都以无监督学习的形式出现，因而这些算法能被应用于其他算法无法企及的无标签数据，这一类数据比有标签数据更丰富，也更容易获得。这一点也为深度学习赢得了重要的优势。

== 人工神经网络下的深度学习 ==
一部分最成功的深度学习方法涉及到对人工神经网络的运用。人工神经网络受到了1959年由诺贝尔奖得主大卫·休伯尔（David H. Hubel）和托斯坦·威泽尔（Torsten Wiesel）提出的理论启发。休伯尔和威泽尔发现，在大脑的初级视觉皮层中存在两种细胞：简单细胞和复杂细胞，这两种细胞承担不同层次的视觉感知功能。受此启发，许多神经网络模型也被设计为不同节点之间的分层模型。
福岛邦彦提出的新认知机引入了使用无监督学习训练的卷积神经网络。扬·勒丘恩将有监督的反向传播算法应用于这一架构。事实上，从反向传播算法自20世纪70年代提出以来，不少研究者都曾试图将其应用于训练有监督的深度神经网络，但最初的尝试大都失败。赛普·霍克赖特在其博士论文中将失败的原因归结为梯度消失，这一现象同时在深度前馈神经网络和循环神经网络中出现，后者的训练过程类似深度网络。在分层训练的过程中，本应用于修正模型参数的误差随着层数的增加指数递减，这导致了模型训练的效率低下。
为了解决这一问题，研究者们提出了一些不同的方法。于尔根·施密德胡伯于1992年提出多层级网络，利用无监督学习训练深度神经网络的每一层，再使用反向传播算法进行调优。在这一模型中，神经网络中的每一层都代表观测变量的一种压缩表示，这一表示也被传递到下一层网络。
另一种方法是赛普·霍克赖特和于尔根·施密德胡伯提出的长短期记忆神经网络，LSTM）。2009年，在ICDAR 2009举办的连笔手写识别竞赛中，在没有任何先验知识的情况下，深度多维长短期记忆神经网络取得了其中三场比赛的胜利。
斯文·贝克提出了在训练时只依赖梯度符号的神经抽象金字塔模型，用以解决图像重建和人脸定位的问题。
其他方法同样采用了无监督预训练来构建神经网络，用以发现有效的特征，此后再采用有监督的反向传播以区分有标签数据。杰弗里·辛顿等人于2006年提出的深度模型提出了使用多层隐变量学习高层表示的方法。这一方法使用斯摩棱斯基于1986年提出的受限玻尔兹曼机对每一个包含高层特征的层进行建模。模型保证了数据的对数似然下界随着层数的提升而递增。当足够多的层数被学习完毕，这一深层结构成为一个生成模型，可以通过自上而下的采样重构整个数据集。辛顿声称这一模型在高维结构化数据上能够有效地提取特征。
吴恩达和杰夫·迪恩领导的谷歌大脑团队创建了一个仅通过YouTube视频学习高层概念（例如猫）的神经网络
。
其他方法依赖了现代电子计算机的强大计算能力，尤其是GPU。2010年，在于尔根·施密德胡伯位于瑞士人工智能实验室IDSIA的研究组中，丹·奇雷尚（Dan Ciresan）和他的同事展示了利用GPU直接执行反向传播算法而忽视梯度消失问题的存在。这一方法在扬·勒丘恩等人给出的手写识别MNIST数据集上战胜了已有的其他方法。
截止2011年，前馈神经网络深度学习中最新的方法是交替使用卷积层（convolutional layers）和最大值池化层（max-pooling layers）并加入单纯的分类层作为顶端。训练过程也无需引入无监督的预训练。从2011年起，这一方法的GPU实现多次赢得了各类模式识别竞赛的胜利，包括IJCNN 2011交通标志识别竞赛和其他比赛。
这些深度学习算法也是最先在某些识别任务上达到和人类表现具备同等竞争力的算法。

== 深度学习结构 ==
深度神经网络是一种具备至少一个隐层的神经网络。与浅层神经网络类似，深度神经网络也能够为复杂非线性系统提供建模，但多出的层次为模型提供了更高的抽象层次，因而提高了模型的能力。深度神经网络通常都是前馈神经网络，但也有语言建模等方面的研究将其拓展到循环神经网络。卷积深度神经网络（Convolutional Neural Networks, CNN）在计算机视觉领域得到了成功的应用。此后，卷积神经网络也作为听觉模型被使用在自动语音识别领域，较以往的方法获得了更优的结果。

=== 深度神经网络 ===
深度神经网络（Deep Neural Networks, DNN）是一种判别模型，可以使用反向传播算法进行训练。权重更新可以使用下式进行Stochastic gradient descent求解：

其中，为学习率，为代价函数。这一函数的选择与学习的类型（例如监督学习、无监督学习、增强学习）以及激活函数相关。例如，为了在一个多分类问题上进行监督学习，通常的选择是使用ReLU作为激活函数，而使用交叉熵作为代价函数。Softmax函数定义为，其中代表类别的概率，而和分别代表对单元和的输入。交叉熵定义为，其中代表输出单元的目标概率，代表应用了激活函数后对单元的概率输出。

=== 深度神经网络的问题 ===
与其他神经网络模型类似，如果仅仅是简单地训练，深度神经网络可能会存在很多问题。常见的两类问题是过拟合和过长的运算时间。
深度神经网络很容易产生过拟合现象，因为增加的抽象层使得模型能够对训练数据中较为罕见的依赖关系进行建模。对此，权重递减（正规化）或者稀疏（-正规化）等方法可以利用在训练过程中以减小过拟合现象。另一种较晚用于深度神经网络训练的正规化方法是丢弃法（"dropout" regularization），即在训练中随机丢弃一部分隐层单元来避免对较为罕见的依赖进行建模。
反向传播算法和梯度下降法由于其实现简单，与其他方法相比能够收敛到更好的局部最优值而成为神经网络训练的通行方法。但是，这些方法的计算代价很高，尤其是在训练深度神经网络时，因为深度神经网络的规模（即层数和每层的节点数）、学习率、初始权重等众多参数都需要考虑。扫描所有参数由于时间代价的原因并不可行，因而小批量训练（mini-batching），即将多个训练样本组合进行训练而不是每次只使用一个样本进行训练，被用于加速模型训练。而最显著地速度提升来自GPU，因为矩阵和向量计算非常适合使用GPU实现。但使用大规模集群进行深度神经网络训练仍然存在困难，因而深度神经网络在训练并行化方面仍有提升的空间。

=== 深度置信网络 ===
一个包含完全连接可见层和隐层的受限玻尔兹曼机（RBM）。注意到可见层单元和隐层单元内部彼此不相连。
深度置信网络（deep belief networks，DBN）是一种包含多层隐单元的概率生成模型，可被视为多层简单学习模型组合而成的复合模型。
深度置信网络可以作为深度神经网络的预训练部分，并为网络提供初始权重，再使用反向传播或者其它判定算法作为调优的手段。这在训练数据较为缺乏时很有价值，因为不恰当的初始化权重会显著影响最终模型的性能，而预训练获得的权重在权值空间中比随机权重更接近最优的权重。这不仅提升了模型的性能，也加快了调优阶段的收敛速度。
深度置信网络中的每一层都是典型的受限玻尔兹曼机（restricted Boltzmann machine，RBM），可以使用高效的无监督逐层训练方法进行训练。受限玻尔兹曼机是一种无向的基于能量的生成模型，包含一个输入层和一个隐层。图中对的边仅在输入层和隐层之间存在，而输入层节点内部和隐层节点内部则不存在边。单层RBM的训练方法最初由杰弗里·辛顿在训练“专家乘积”中提出，被称为对比分歧（contrast divergence, CD）。对比分歧提供了一种对最大似然的近似，被理想地用于学习受限玻尔兹曼机的权重。当单层RBM被训练完毕后，另一层RBM可被堆叠在已经训练完成的RBM上，形成一个多层模型。每次堆叠时，原有的多层网络输入层被初始化为训练样本，权重为先前训练得到的权重，该网络的输出作为新增RBM的输入，新的RBM重复先前的单层训练过程，整个过程可以持续进行，直到达到某个期望中的终止条件。
尽管对比分歧对最大似然的近似十分粗略（对比分歧并不在任何函数的梯度方向上），但经验结果证实该方法是训练深度结构的一种有效的方法。

=== 卷积神经网络 ===
卷积神经网络（convolutional neural networks，CNN）由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，使之成为一种颇具吸引力的深度学习结构。

=== 卷积深度置信网络 ===
卷积深度置信网络（convolutional deep belief networks，CDBN）是深度学习领域较新的分支。在结构上，卷积深度置信网络与卷积神经网络在结构上相似。因此，与卷积神经网络类似，卷积深度置信网络也具备利用图像二维结构的能力，与此同时，卷积深度信念网络也拥有深度置信网络的预训练优势。卷积深度置信网络提供了一种能被用于信号和图像处理任务的通用结构，也能够使用类似深度置信网络的训练方法进行训练。

=== 结果 ===

==== 语音识别 ====
下表中的结果展示了深度学习在通行的TIMIT数据集上的结果。TIMIT包含630人的语音数据，这些人持八种常见的美式英语口音，每人阅读10句话。这一数据在深度学习发展之初常被用于验证深度学习结构。TIMIT数据集较小，使得研究者可以在其上实验不同的模型配置。

==== 图像分类 ====
图像分类领域中一个公认的评判数据集是MNIST数据集。MNIST由手写阿拉伯数字组成，包含60,000个训练样本和10,000个测试样本。与TIMIT类似，它的数据规模较小，因而能够很容易地在不同的模型配置下测试。Yann LeCun的网站给出了多种方法得到的实验结果。截至2012年，最好的判别结果由Ciresan等人在当年给出，这一结果的错误率达到了0.23%。

== 深度学习与神经科学 ==
计算机领域中的深度学习与20世纪90年代由认知神经科学研究者提出的大脑发育理论（尤其是皮层发育理论）密切相关。对这一理论最容易理解的是杰弗里·艾尔曼于1996年出版的专著《对天赋的再思考》（Rethinking Innateness）（参见斯拉格和约翰逊以及奎兹和赛杰诺维斯基的表述）。由于这些理论给出了实际的神经计算模型，因而它们是纯计算驱动的深度学习模型的技术先驱。这些理论指出，大脑中的神经元组成了不同的层次，这些层次相互连接，形成一个过滤体系。在这些层次中，每层神经元在其所处的环境中获取一部分信息，经过处理后向更深的层级传递。这与后来的单纯与计算相关的深度神经网络模型相似。这一过程的结果是一个与环境相协调的自组织的堆栈式的转换器。正如1995年在《纽约时报》上刊登的那样，“……婴儿的大脑似乎受到所谓‘营养因素’的影响而进行着自我组织……大脑的不同区域依次相连，不同层次的脑组织依照一定的先后顺序发育成熟，直至整个大脑发育成熟。”
深度结构在人类认知演化和发展中的重要性也在认知神经学家的关注之中。发育时间的改变被认为是人类和其他灵长类动物之间智力发展差异的一个方面。在灵长类中，人类的大脑在出生后的很长时间都具备可塑性，但其他灵长类动物的大脑则在出生时就几乎完全定型。因而，人类在大脑发育最具可塑性的阶段能够接触到更加复杂的外部场景，这可能帮助人类的大脑进行调节以适应快速变化的环境，而不是像其他动物的大脑那样更多地受到遗传结构的限制。这样的发育时间差异也在大脑皮层的发育时间和大脑早期自组织中从刺激环境中获取信息的改变得到体现。当然，伴随着这一可塑性的是更长的儿童期，在此期间人需要依靠抚养者和社会群体的支持和训练。因而这一理论也揭示了人类演化中文化和意识共同进化的现象。

== 公众视野中的深度学习 ==
深度学习常常被看作是通向真正人工智能的重要一步，因而许多机构对深度学习的实际应用抱有浓厚的兴趣。2013年12月，Facebook宣布雇用杨立昆为其新建的人工智能实验室的主管，这一实验室将在加州、伦敦和纽约设立分支机构，帮助Facebook研究利用深度学习算法进行类似自动标记照片中用户姓名这样的任务。
2013年3月，杰弗里·辛顿和他的两位研究生亚历克斯·克里泽夫斯基和伊利娅·苏特斯科娃被谷歌公司雇用，以提升现有的机器学习产品并协助处理谷歌日益增长的数据。谷歌同时并购了辛顿创办的公司DNNresearch。
2016年3月，以深度学习开发的围棋程式AlphaGo首度在比赛中击败人类顶尖选手，形成广泛的讨论。

== 批评 ==
对深度学习的主要批评是许多方法缺乏理论支撑。大多数深度结构仅仅是梯度下降的某些变式。尽管梯度下降法已经被充分地研究，但理论涉及的其他算法，例如对比分歧算法，并没有获得充分的研究，其收敛性等问题仍不明确。深度学习方法常常被视为黑盒，大多数的结论确认都由经验而非理论来确定。
也有学者认为，深度学习应当被视为通向真正人工智能的一条途径，而不是一种包罗万象的解决方案。尽管深度学习的能力很强，但和真正的人工智能相比，仍然缺乏诸多重要的能力。理论心理学家加里·马库斯指出：

== 参见 ==
* 图模型
* 人工智能的应用
* 杰弗里·辛顿
* 人工智能项目列表
深度学习库
* Torch
* TensorFlow
* Theano
*  PaddlePaddle
* Deeplearning4j
*  Caffe
*  roNNie
* Keras
*  Mxnet

== 参考资料 ==

== 外部链接 ==
* 来自蒙特利尔大学的深度学习信息  
* 杰弗里·辛顿的主页 
* 深度学习视频教程 
* 燕乐存的主页 
* 麻省理工大学生物和计算学习中心 (CBCL) 
* 斯坦福大学提供的无监督特征学习和深度学习教程 
* 谷歌DistBelief框架 
* Theano深度学习工具包（使用Python） 
* Deeplearning4j开源深度学习工具包（使用Java） 
* NIPS 2013会议（介绍深度学习相关资料） 




循环神经网络（Recurrent neural network：RNN）是神经网络的一种。单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。
时间循环神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别是最早成功利用RNN的研究结果。

== 历史 ==
递归神经网络是基于大卫·鲁梅尔哈特1986年的工作。1982年，约翰·霍普菲尔德发现了Hopfield神经网络——一种特殊的RNN。1993年，一个神经历史压缩器系统解决了一个“非常深度学习”的任务，这个任务在RNN展开之后有1000多个后续层。

=== LSTM ===
Hochreiter和Schmidhuber于1997年发现了长短期记忆(LSTM)网络，并在多个应用领域创造了精确度记录。
大约在2007年，LSTM开始革新语音识别领域，在某些语音应用中胜过传统模型。2009年，一个由 4=CTC 训练的LSTM网络赢得了多项连笔手写识别竞赛，成为第一个赢得模式识别竞赛的RNN。2014年，百度在不使用任何传统语音处理方法的情况下，使用经过CTC训练的RNNs打破了Switchboard Hub5'00 语音识别基准。
LSTM还改进了大词汇量语音识别和文本到语音合成并在谷歌安卓系统中使用。据报道，2015年，谷歌语音识别通过接受过CTC训练的LSTM(谷歌语音搜索使用的)实现了49%的引用量的大幅提升。
LSTM打破了改进机器翻译、语言建模和多语言处理的记录。 LSTM 结合卷积神经网络改进了图像自动标注 。

==循环神经网络==

===编码器===
循环神经网络将输入序列编码为一个固定长度的隐藏状态，这里有（用自然语言处理作为例子）：
*   是输入序列，比如编码为数字的一系列词语，整个序列就是完整的句子。
* 是随时间更新的隐藏状态。当新的词语输入到方程中，之前的状态就转换为和当前输入相关的，距离当前时间越长，越早输入的序列，在更新后的状态中所占权重越小，从而表现出时间相关性。
其中，计算隐藏状态的方程是一个非线性方程，可以是简单的Logistic方程（tanh），也可以是复杂的LSTM单元（Long Short-Term Memory）。  而有了隐藏状态序列，就可以对下一个出现的词语进行预测：
* ，其中是第t个位置上的输出，它的概率基于之前输出的所有词语。
* 以上概率可以通过隐藏状态来计算：，是所有隐藏状态的编码，总含了所有隐藏状态，比如可以是简单的最终隐藏状态，也可以是非线性方程的输出。因为隐藏状态t就编码了第t个输入前全部的输入信息，也迭代式地隐含了之前的全部输出信息，所以这个概率计算方法是合理的。
这里的非线性方程可以是一个复杂的前馈神经网络，也可以是简单的非线性方程（但有可能因此无法适应复杂的条件而得不到任何有用结果）。给出的概率可以用监督学习的方法优化内部参数来给出翻译，也可以训练后用来给可能的备选词语，用计算其第j个备选词出现在下一位置的概率，给它们排序。排序后用于其它翻译系统，可以提升翻译质量。

===解码器===
更复杂的情况下循环神经网络还可以结合编码器作为解码器（Decoder），用于将编码后（Encoded）的信息解码为人类可识别的信息。也就是上述例子中的过程，当中非线性模型就是作为输出的循环神经网络。只是在解码过程中，隐藏状态因为是解码器的参数，所以为了发挥时间序列的特性，需要对继续进行迭代：
* ，是解码器传递给编码器的参数，是解码器中状态的summary。是解码器的隐藏状态。是第t个输出。
* 当输入仍为，输出是，最大化条件概率后就是最好的翻译结果。

===双向读取===
用两个循环神经网络双向读取一个序列可以使人工智能获得“注意力”。简单的做法是将一个句子分别从两个方向编码为两个隐藏状态，然后将两个拼接在一起作为隐藏状态。 这种方法能提高模型表现的原因之一可能是因为不同方向的读取在输入和输出之间创造了更多短期依赖关系，从而被RNN中的LSTM单元（及其变体）捕捉，例如在实验中发现颠倒输入序列的顺序（但不改变输出的顺序）可以意外达到提高表现的效果。

==结构递归神经网络==
结构递归（Recursive）神经网络是一类用结构递归的方式构建的网络，比如说递归自编码机（Recursive Autoencoder），在自然语言处理的神经网络分析方法中用于解析语句。 

== 架构 ==
RNN 有很多不同的变种

=== 完全循环 ===
基本的 RNN 是由4=人工神经元组织成的连续的层的网络。给定层中的每个节点都通过4=有向(单向)连接连接到下一个连续层中的每个其他节点。每个节点(神经元)都有一个时变的实值激活。每个连接(突触)都有一个可修改的实值4=权重。节点要么是输入节点(从网络外部接收数据)，要么是输出节点(产生结果)，要么是隐藏节点(在从输入到输出的过程中修改数据)。
对于离散时间设置中的监督学习，实值输入向量序列到达输入节点，一次一个向量。在任何给定的时间步长，每个非输入单元将其当前激活(结果)计算为与其连接的所有单元的激活的加权和的非线性函数。可以在特定的时间步长为某些输出单元提供主管给定的目标激活。例如，如果输入序列是对应于口语数字的语音信号，则在序列末尾的最终目标输出可以是对该数字进行分类的标签。
在强化学习环境中，没有教师提供目标信号。相反，适应度函数或奖励函数偶尔用于评估RNN的性能，它通过影响输出单元来影响其输入流，输出单元和一个可以影响环境的执行器相连。这可以被用来玩一个游戏，在这个游戏中，进度是用赢得的点数来衡量的。
每个序列产生一个误差，作为所有目标信号与网络计算的相应激活的偏差之和。对于大量序列的训练集，总误差是所有单个序列误差的总和。

=== Jordan网络Elman 网络和 Jordan 网络 ===
Elman 网络
Elman网络是一个三层网络(在图中水平排列为x、y和z)，添加了一组上下文单元(在图中为u)。中间(隐藏)层连接到这些权重为1的上下文单元。在每个时间步，输入被向前反馈，并且学习规则被应用。固定的反向连接在上下文单元中保存隐藏单元的先前值的副本(因为它们在应用学习规则之前在连接上传播)。因此，网络可以保持某种状态，允许它执行诸如序列预测之类的任务，这些任务超出了标准多层感知器的能力。
Jordan网络类似于Elman网络。上下文单元是从输出层而不是隐藏层馈送的。Jordan网络中的上下文单元也称为状态层。他们与自己有着经常性的联系。
Elman和Jordan网络也被称为“简单循环网络”。
 Elman 网络
 
 Jordan 网络
 
变量和函数
* : 输入向量
* : 隐藏层向量
* : 输出向量
* ,  和 : 参数矩阵和参数向量
*  和 : 激活函数

== 参考 ==

== 外部连结 ==
*  A Critical Review of Recurrent Neural Networks for Sequence Learning




卷积神经网络（Convolutional  Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。
卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构。

==定义==
“卷积神经网络”表示在网络采用称为卷积的数学运算。卷积是一种特殊的线性操作。卷积网络是一种特殊的神经网络，它们在至少一个层中使用卷积代替一般矩阵乘法

==概览==

==发展==

==结构==

===卷积层===
卷积层是一组平行的特征图（feature map），它通过在输入图像上滑动不同的卷积核并执行一定的运算而组成。此外，在每一个滑动的位置上，卷积核与输入图像之间会执行一个元素对应乘积并求和的运算以将感受野内的信息投影到特征图中的一个元素。这一滑动的过程可称为步幅 Z_s，步幅 Z_s 是控制输出特征图尺寸的一个因素。卷积核的尺寸要比输入图像小得多，且重叠或平行地作用于输入图像中，一张特征图中的所有元素都是通过一个卷积核计算得出的，也即一张特征图共享了相同的权重和偏置项。

===线性整流层===
线性整流层（Rectified Linear Units layer, ReLU layer）使用线性整流（Rectified Linear Units, ReLU）作为这一层神经的激励函数（Activation function）。它可以增强判定函数和整个神经网络的非线性特性，而本身并不会改变卷积层。
事实上，其他的一些函数也可以用于增强网络的非线性特性，如双曲正切函数 , ，或者Sigmoid函数。相比其它函数来说，ReLU函数更受青睐，这是因为它可以将神经网络的训练速度提升数倍，而并不会对模型的泛化准确度造成显著影响。

===池化层===
步幅为2，池化窗口为的最大池化层
池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种非线性形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。
直觉上，这种机制能够有效地原因在于，一个特征的精确位置远不及它相对于其他特征的粗略位置重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的网络结构中的卷积层之间都会周期性地插入池化层。池化操作提供了另一种形式的平移不变性。因为卷积核是一种特征发现器，我们通过卷积层可以很容易地发现图像中的各种边缘。但是卷积层发现的特征往往过于精确，我们即使高速连拍拍摄一个物体，照片中的物体的边缘像素位置也不大可能完全一致，通过池化层我们可以降低卷积层对边缘的敏感性。
池化层每次在一个池化窗口（depth slice）上计算输出，然后根据步幅移动池化窗口。下图是目前最常用的池化层，步幅为2，池化窗口为的二维最大池化层。每隔2个元素从图像划分出的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。
除了最大池化之外，池化层也可以使用其他池化函数，例如“平均池化”甚至“L2-范数池化”等。过去，平均池化的使用曾经较为广泛，但是最近由于最大池化在实践中的表现更好，平均池化已经不太常用。
由于池化层过快地减少了数据的大小，目前文献中的趋势是使用较小的池化滤镜，甚至不再使用池化层。
RoI池化(Region of Interest)是最大池化的变体，其中输出大小是固定的，输入矩形是一个参数。
池化层是基于 Fast-RCNN 架构的卷积神经网络的一个重要组成部分。

===完全连接层===
最后，在经过几个卷积和最大池化层之后，神经网络中的高级推理通过完全连接层来完成。就和常规的非卷积人工神经网络中一样，完全连接层中的神经元与前一层中的所有激活都有联系。因此，它们的激活可以作为仿射变换来计算，也就是先乘以一个矩阵然后加上一个偏差(bias)偏移量(向量加上一个固定的或者学习来的偏差量)。

===损失函数层===
损失函数层（loss layer）用于决定训练过程如何来“惩罚”网络的预测结果和真实结果之间的差异，它通常是网络的最后一层。各种不同的损失函数适用于不同类型的任务。例如，Softmax交叉熵损失函数常常被用于在K个类别中选出一个，而Sigmoid交叉熵损失函数常常用于多个独立的二分类问题。欧几里德损失函数常常用于标签取值范围为任意实数的问题。

==应用==

===影像辨识===
卷积神经网络通常在影像辨识系统中使用。

===视讯分析===
相比影像辨识问题，视讯分析要难许多。CNN也常被用于这类问题。

===自然语言处理===
卷积神经网络也常被用于自然语言处理。 CNN的模型被证明可以有效的处理各种自然语言处理的问题，如语义分析、搜索结果提取、句子建模 、分类、预测、和其他传统的NLP任务
等。

===药物发现===
卷积神经网路已在药物发现中使用。卷积神经网络被用来预测的分子与蛋白质之间的相互作用，以此来寻找靶向位点，寻找出更可能安全和有效的潜在治疗方法。

===围棋===
卷积神经网络在计算机围棋领域也被使用。2016年3月，AlphaGo对战李世乭的比赛，展示了深度学习在围棋领域的重大突破。

==微调（fine-tuning）==
卷积神经网络（例如Alexnet、VGG网络）在网络的最后通常为softmax分类器。微调一般用来调整softmax分类器的分类数。例如原网络可以分类出2种图像，需要增加1个新的分类从而使网络可以分类出3种图像。微调（fine-tuning）可以留用之前训练的大多数参数，从而达到快速训练收敛的效果。例如保留各个卷积层，只重构卷积层后的全连接层与softmax层即可。

==经典模型==
*LeNet
*AlexNet
*VGG
*GoogLeNet
*ResNet

==可用包==
*  roNNie: 是一个简易入门级框架,使用Tensorflow 计算层.可于python下载 pip3 ronnie
* Caffe: Caffe包含了CNN使用最广泛的库。它由伯克利视觉和学习中心（BVLC）研发，拥有比一般实现更好的结构和更快的速度。同时支持CPU和GPU计算，底层由C++实现，并封装了Python和MATLAB的接口。
* Torch7（www.torch.ch）
* OverFeat
* Cuda-convnet
* MatConvnet
* Theano：用Python实现的神经网络包
* TensorFlow
* Paddlepaddle( www.paddlepaddle.org)
* Keras

==参考==




在机器学习中，特征学习或表征学习是学习一个特征的技术的集合：将原始数据转换成为能够被机器学习来有效开发的一种形式。它避免了手动提取特征的麻烦，允许计算机学习使用特征的同时，也学习如何提取特征：学习如何学习。
机器学习任务，例如分类问题，通常都要求输入在数学上或者在计算上都非常便于处理，在这样的前提下，特征学习就应运而生了。然而，在我们现实世界中的数据例如图片，视频，以及传感器的测量值都非常的复杂，冗余并且多变。那么，如何有效的提取出特征并且将其表达出来就显得非常重要。传统的手动提取特征需要大量的人力并且依赖于非常专业的知识。同时，还不便于推广。这就要求特征学习技术的整体设计非常有效，自动化，并且易于推广。
特征学习可以被分为两类：监督的和无监督的，类似于机器学习。
* 在监督特征学习中，被标记过的数据被当做特征用来学习。例如神经网络，多层感知器，(监督)字典学习。
* 在无监督特征学习中，未被标记过的数据被当做特征用来学习。例如(无监督)字典学习，独立成分分析，自动编码，矩阵分解 ，各种聚类分析及其变形。

== 监督特征学习 ==
监督特征学习就是从被标记的数据中学习特征。大致有一下几种方法。

=== 监督字典学习 ===
总体来说，字典学习是为了从输入数据获得一组的表征元素，使每一个数据点可以（近似的）通过对表征元素加权求和来重构。字典中的元素和权值可以通过最小化表征误差来得到。通过L1正则化可以让权值变得稀疏（例，每一个数据点的表征只有几个非零的权值）。
监督字典学习利用输入数据的结构和给定的标签（输出）来优化字典。例如，2009年Mairal等人提出的一种监督字典学习方案被应用在了分类问题上。这个方案的优化目标包括最小化分类误差，表征误差，权值的1范数（L1正则化）和分类器参数的2范数。
有监督的字典学习可以被视为一个三层神经网络（一层隐含层），第一层（输入层）到第二层（隐含层）是表征学习，第二层到第三层（输出）是分类器的参数回归。

=== 神经网络 ===
神经网络是通过多层由内部相连的节点组成的网络的一个学习算法。它的命名是受到神经系统的启发，它的每一个节点就像神经系统里的神经元，而每一条边就像一条突触。神经网络里面的每一条边都有对应的权值，而整个网络则定义运算法则将输入数据转换成为输出。神经网络的网络函数通过权值来刻画输入层跟输出层之间的关系。通过适当的调整网络函数，可以尽量最小化损耗的同时解决各种各样的机器学习任务。

== 无监督特征学习 ==

=== κ-平均算法 ===

=== 主要成分分析 ===

=== 独立成分分析 ===

=== 局部线性嵌入算法 ===

=== 无监督字典学习 ===

== 另见 ==
* 特征检测
* 向量量化
* 深度学习

== 参考文献 ==




right
整流线性单位函数（Rectified Linear Unit, ReLU）,又称修正线性单元, 是一种人工神经网络中常用的激励函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。
比较常用的线性整流函数有斜坡函数 ，以及带泄露整流函数 (Leaky ReLU)，其中  为神经元(Neuron)的输入。线性整流被认为有一定的生物学原理，并且由于在实践中通常有着比其他常用激励函数（譬如逻辑函数）更好的效果，而被如今的深度神经网络广泛使用于诸如图像识别等计算机视觉人工智能领域。

== 定义 ==
通常意义下，线性整流函数指代数学中的斜坡函数，即

而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换 之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量 ，使用线性整流激活函数的神经元会输出

至下一层神经元或作为整个神经网络的输出（取决现神经元在网络结构中所处位置）。

== 变种 ==
线性整流函数在基于斜坡函数的基础上有其他同样被广泛应用于深度学习的变种，譬如带泄露线性整流(Leaky ReLU)， 带泄露随机线性整流(Randomized Leaky ReLU)，以及噪声线性整流(Noisy ReLU).

=== 带泄露线性整流 ===
在输入值  为负的时候，带泄露线性整流函数（Leaky ReLU）的梯度为一个常数 ，而不是0。在输入值为正的时候，带泄露线性整流函数和普通斜坡函数保持一致。换言之，

在深度学习中，如果设定  为一个可通过反向传播算法（Backpropagation）学习的变量，那么带泄露线性整流又被称为参数线性整流（Parametric ReLU）。

=== 带泄露随机线性整流 ===
带泄露随机线性整流（Randomized Leaky ReLU, RReLU）最早是在Kaggle全美数据科学大赛（NDSB）中被首先提出并使用的。相比于普通带泄露线性整流函数，带泄露随机线性整流在负输入值段的函数梯度  是一个取自连续性均匀分布  概率模型的随机变量，即

其中  且 。

=== 噪声线性整流 ===
噪声线性整流（Noisy ReLU）是修正线性单元在考虑高斯噪声的基础上进行改进的变种激活函数。对于神经元的输入值 ，噪声线性整流加上了一定程度的正态分布的不确定性，即

其中随机变量 。目前，噪声线性整流函数在受限玻尔兹曼机（Restricted Boltzmann Machine）在计算机图形学的应用中取得了比较好的成果。

== 优势 ==
相比于传统的神经网络激活函数，诸如逻辑函数（Logistic sigmoid）和tanh等双曲函数，线性整流函数有着以下几方面的优势：
*仿生物学原理：相关大脑方面的研究表明生物神经元的信息编码通常是比较分散及稀疏的。通常情况下，大脑中在同一时间大概只有1%-4%的神经元处于活跃状态。使用线性修正以及正则化（regularization）可以对机器神经网络中神经元的活跃度（即输出为正值）进行调试；相比之下，逻辑函数在输入为0时达到 ，即已经是半饱和的稳定状态，不够符合实际生物学对模拟神经网络的期望。不过需要指出的是，一般情况下，在一个使用修正线性单元（即线性整流）的神经网络中大概有50%的神经元处于激活态。
*更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题
*简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降

== 参考资料 ==

== 外部链接 ==
*  Quora: What is special about rectifier neural units used in NN learning?  




极限学习机（英文：Extreme Learning Machines，缩写ELM），又名超限学习机，为人工智能机器学习领域中的一种人工神经网络模型，是一种求解单隐层前馈神经网络的学习算法。

==特点==
传统的前馈神经网络（如BP神经网络）需要人为设置大量的网络训练参数，此算法却只需要设定网络的结构，而不需设置其他参数，因此具有简单易用的特点。其输入层到隐藏层的权值是一次随机确定的，算法执行过程中不需要再调整，而隐藏层到输出层的权值只需解一个线性方程组来确定，因此可以提升计算速度。

==开发==
极限学习机的名称来自新加坡南洋理工大学黄广斌教授所建立的模型。黄教授指出，此算法的泛化性能良好，且其学习速度比运用反向传播算法训练的速度要快上1000倍。

== 算法 ==
极限学习机中最简单的原理如下：

其中W1是输入向量到隐藏节点层的权重矩阵，σ是激活函数，W2是隐藏节点层到输出向量的权重矩阵。算法按下列步骤进行：
# 用随机产生的高斯噪声给矩阵W1的每个元素赋值；
# 用最小二乘法估计使期望输出Y与实际输出误差最小的输出权重矩阵W2，数学上能够证明计算隐藏节点层输出矩阵的广义逆 ⋅+ 即可：
#:

==参见==
* 机器学习
* 前馈神经网络
* 人工神经网络

== 参考资料 ==

==外部链接==
*  南洋理工学院极限学习机网站，附源代码、未决难题、ELM会议、教程、参考资料等
*  ELM的R程序包
* ELM开源程序（Python）
* ELM开源程序（C++）
* ELM开源程序（Matlab）



计算机语言学中，词义消歧是一个 自然语言处理和本体论的开放问题。歧义与消歧是自然语言理解中最核心的问题，在词义、句义、篇章含义层次都会出现语言根据上下文语义不同的现象，消歧即指根据上下文确定对象语义的过程。词义消歧即在词语层次上的语义消歧。语义消歧/词义消歧 是自然语言处理任务的一个核心与难点，影响了几乎所有任务的性能，比如搜索引擎、意见挖掘、文本理解与产生、推理等。
在语言学长期发展的过程中，语言本身积累了许多一词多义的用法。语言的产生是多方面共同作用的结果。语言的使用是不断变化的，一个词在发展中有许多具体的意思，现在通用的还有一些意思。不同地区可能对一个词有不同 的用法，不同的行业对一个词也会不同，甚至不同群体、不同个人、不同语气都会有自己的特殊的解读意思。语义消歧是一种语言理解的方式，一方面我们要理解通用词语一词多义的含义及应用，另一方面，还要考虑到具体场景，运用相关知识库、语料训练来增加一词多义的性能。
迄今为止，丰富多样的技术已经被研究，以词典为基础的方法，使用知识库与知识图谱技术的，监督学习的，无监督的，半监督的，基于词或者词向量的。基于各种资源的、半监督的、同时基于词与词向量的应该是发展的方向。

== 困难 ==

=== 词典 ===
基于词典的语义消歧依赖于词典对语义的区分。粗粒度的一词多义指区分较大的语义，比如水，可能表示自然水，也能指水货；细粒度的一词多义指能区分较小不同的语义。如果词典缺少某一层次/某一些语义的描述，以词典作为词语义的完全描述就会导致问题。这个特点对WSD(词义消歧)与EL(entity linking)同样适用。解决这个问题的办法是，对描述较少的语义聚集自动增量增加聚类。
英文里常用的字典包括WordNet, Roget'Thesaurus, BabelNet. 任意语言都可以把常用的字典、词典、网络百科、专业知识库/数据库 作为消歧的词典文件。

=== 词性标注 ===
词性标注与词义消歧是相互关联的两个问题，在人的系统他们同时能到满足。但是目前系统一般并不能让2者公用参数，同时输出。语义理解，包括分词、词性标注、词义消歧、句法解析、语义解析 并不是前馈的，是相互依赖的存在反馈的。
词性标注与语义消歧都要依赖上下文来标注，但是词性标注比语义消歧要简单以及成功。原因主要是词性标注的标注集合是确定的，而语义消歧并没有，并且量级要大的多；词性标注的上下文依赖比语义消歧要短。

=== judge依赖 ===
有时候人也不能很地判断一个词属于哪个意思。对于粗粒度的区分肯定比细粒度的高。所以一般选择粗粒度的任务，因为需要使用人的判断作为黄金标准。

=== 语用学 ===
许多研究者认为要做到词义消歧，需要理解语用学、一些常识。语言学本身就是与知识紧密结合的，肯定需要语言相关的常识帮助解析，就像实体消歧需要实体的相关的知识一样。

=== 不同任务使用词义消歧的区别 ===
不同的任务具体词义消歧会不同。比如翻译，不必须显式地输出词义消歧中间结果，他需要最后的句子的同义即可

=== 多义的定义 ===
人们一般能在粗粒度的定义上获得一致的看法，当他到更细的粒度，则很难统一。并且即便同个语义，在不同的环境里，也许还会有不同，因为语言表达有无限的可能性，导致语义在细粒度可能会迁移。

== 参考资料 ==



概率的潜在语义分析（PLSA），也称为概率潜在语义索引（PLSI，尤其是在信息检索领域），是用于分析双模和共现数据的统计方法。 实际上，人们可以根据对某些隐变量的亲和性来推导出观测变量的低维表示，就像PLSA是从潜在语义分析中演化而来。
与源于线性代数并缩小发生表（通常通过奇异值分解）的标准潜在语义分析所不同的是，概率潜在语义分析基于从潜类模型导出的混合分解。

== 模型 ==
可观测的变量，主题 是一个潜变量。
考虑到以单词和文档的共现 形式进行的观察，PLSA将每次共现的概率建模为条件独立的多项分布的混合：
 
其中'c'是单词的主题。值得注意的是，模型的主题数量是一个超参数，必须提前设置而不是从数据中估计。第一个公式是对称式，其中 和 都是以类似的方式从潜变量 生成（基于条件概率 和 ）；而第二个公式是不对称的 ，对于每个文档 根据 有条件地从文档中选择潜在类 ，然后根据 从该类生成一个单词。虽然在这个例子中我们使用单词和文档建模，但是任何离散变量的共现也可以用完全相同的方式建模。
因此，模型参数的数量等于 ，参数数量随文档数量呈线性增长。此外，尽管PLSA是基于文档集的生成模型，但它并不是新文档的生成模型。
模型的参数使用最大期望算法（EM算法）学习得到。

== 应用 ==
PLSA可以通过Fisher核函数用于判别设置。
PLSA在信息检索和过滤、自然语言处理、文本机器学习及其他相关领域都有应用。
根据报告，概率潜在语义分析中使用的方面模型存在严重的过拟合问题。

== 扩展 ==
* 分层扩展：
** 不对称：MASHA（Multinomial ASymmetric Hierarchical Analysis，多项式非对称分层分析）
** 对称：HPLSA（Hierarchical Probabilistic Latent Semantic Analysis，分层概率潜在语义分析）
* 生成模型：已经开发了以下模型来解决经常被批评的PLSA缺点——它不是新文档的正确生成模型。
** 潜在狄利克雷分配（LDA）——在每个文档-主题分布上添加狄利克雷先验
* 高阶数据：尽管在科学文献中很少讨论这一点，但PLSA可以自然地扩展到更高阶数据（三种模式或更高阶），它可以模拟三个或更多变量的共现。在上面的对称公式中，这仅需要为这些附加变量添加条件概率分布就可以实现。这是非负张量因子分解的概率类比。

== 历史 ==
这是潜类模型的一个特例（参见其中的参考文献），它与非负矩阵分解有关。当前的术语是由Thomas Hofmann在1999年创造的。

== 参见 ==
* 向量空间模型

== 参考文献 ==

== 外部链接 ==
*  概率潜在语义分析（PDF）
*  C#编写的PLSA的DEMO




AlexNet是一个卷积神经网络，由亚历克斯·克里泽夫斯基（Alex Krizhevsky）设计，与伊尔亚‧苏茨克维（Ilya Sutskever）和克里泽夫斯基的博士导师杰弗里·辛顿共同发表，而辛顿最初抵制他的学生的想法。
AlexNet参加了2012年9月30日举行的ImageNet大规模视觉识别挑战赛，达到最低的15.3%的Top-5错误率，比第二名低10.8个百分点。原论文的主要结论是，模型的深度对于提高性能至关重要，AlexNet的计算成本很高，但因在训练过程中使用了图形处理器（GPU）而使得计算具有可行性  。

== 背景 ==
AlexNet并不是卷积神经网络（CNN）第一次利用快速GPU实现而赢得图像识别竞赛。K. Chellapilla等人（2006）在GPU上的CNN比同等的CPU实现速度快4倍。Dan Ciresan等人（2011）的深层CNN在IDSIA上已经快了60倍，并在2011年8月取得了超过人类的表现。从2011年5月15日到2012年9月10日，他们的CNN赢得了不少于四场图像竞赛。他们还极大提高了文献中多个图像数据库的最佳性能。
根据AlexNet的论文，其与Ciresan的早期网络“有些相似”。两者最初都用CUDA编写，可在GPU支持下运行。实际上，两者都是杨立昆等人（1989）介绍的CNN设计的变体，他将反向传播算法应用于福岛邦彦（福岛 邦彦）最初提出的CNN架构“neocognitron”的一个变种。后来J. Weng提出的最大池化方法修改了该架构。

== 网络设计 ==
AlexNet包含八层。前五层是卷积层，之后一些层是最大池化层，最后三层是全连接层。它使用了非饱和的ReLU激活函数，显示出比tanh和sigmoid更好的训练性能。

== 影响 ==
AlexNet被认为是计算机视觉领域最有影响力的论文之一，它刺激了更多使用卷积神经网络和GPU来加速深度学习的论文的出现。截至2020年，AlexNet论文已被引用超过54,000次。

== 亚历克斯·克里泽夫斯基  ==
亚历克斯·克里泽夫斯基（出生于乌克兰，在加拿大长大）是一名计算机科学家，以在人工神经网络和深度学习方面的工作而著称。在通过AlexNet赢得ImageNet 2012挑战赛后不久，他和同事将他们的创业公司DNN研究公司（DNN Research Inc.）卖给了Google。克里泽夫斯基对这项工作失去兴趣后，于2017年9月离开了Google。在Dessa公司，克里泽夫斯基将为新的深度学习技术提供建议和帮助。研究人员经常引用他的许多有关机器学习和计算机视觉的论文。

==参考资料==



